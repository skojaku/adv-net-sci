<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Sadamori Kojaku">
<meta name="dcterms.date" content="2025-07-27">

<title>Characteristics of Random Walks</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-0348920b7671f696dc9078d39bff215e.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-e6dc204ec8b52f55243daf2cac742210.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "|"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../assets/custom.css">
</head>

<body class="nav-sidebar docked nav-fixed fullcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../logo.jpg" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Advanced Topics in Network Science</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-course" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Course</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-course">    
        <li>
    <a class="dropdown-item" href="../course/welcome.html">
 <span class="dropdown-text">Welcome</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../course/about.html">
 <span class="dropdown-text">About</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../course/discord.html">
 <span class="dropdown-text">Discord</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../course/minidora-usage.html">
 <span class="dropdown-text">Minidora</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../course/setup.html">
 <span class="dropdown-text">Setup</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-intro" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Intro</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-intro">    
        <li>
    <a class="dropdown-item" href="../intro/why-networks.html">
 <span class="dropdown-text">Why Networks?</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-foundations" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Foundations</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-foundations">    
        <li class="dropdown-header">─── M01: Euler Path ───</li>
        <li>
    <a class="dropdown-item" href="../m01-euler_tour/01-concepts.html">
 <span class="dropdown-text">Concepts</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m01-euler_tour/02-coding.html">
 <span class="dropdown-text">Coding</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m01-euler_tour/03-exercises.html">
 <span class="dropdown-text">Exercises</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m01-euler_tour/04-advanced.html">
 <span class="dropdown-text">Advanced</span></a>
  </li>  
        <li class="dropdown-header">─── M02: Small World ───</li>
        <li>
    <a class="dropdown-item" href="../m02-small-world/01-concepts.html">
 <span class="dropdown-text">Concepts</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m02-small-world/02-coding.html">
 <span class="dropdown-text">Coding</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m02-small-world/03-exercises.html">
 <span class="dropdown-text">Exercises</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m02-small-world/04-appendix.html">
 <span class="dropdown-text">Appendix</span></a>
  </li>  
        <li class="dropdown-header">─── M03: Robustness ───</li>
        <li>
    <a class="dropdown-item" href="../m03-robustness/01-concepts.html">
 <span class="dropdown-text">Concepts</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m03-robustness/02-coding.html">
 <span class="dropdown-text">Coding</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m03-robustness/03-exercises.html">
 <span class="dropdown-text">Exercises</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m03-robustness/04-appendix.html">
 <span class="dropdown-text">Appendix</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-core-topics" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Core Topics</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-core-topics">    
        <li class="dropdown-header">─── M04: Friendship Paradox ───</li>
        <li>
    <a class="dropdown-item" href="../m04-node-degree/01-concepts.html">
 <span class="dropdown-text">Concepts</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m04-node-degree/02-coding.html">
 <span class="dropdown-text">Coding</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m04-node-degree/03-exercises.html">
 <span class="dropdown-text">Exercises</span></a>
  </li>  
        <li class="dropdown-header">─── M05: Clustering ───</li>
        <li>
    <a class="dropdown-item" href="../m05-clustering/01-concepts.html">
 <span class="dropdown-text">Concepts</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m05-clustering/02-coding.html">
 <span class="dropdown-text">Coding</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m05-clustering/03-exercises.html">
 <span class="dropdown-text">Exercises</span></a>
  </li>  
        <li class="dropdown-header">─── M06: Centrality ───</li>
        <li>
    <a class="dropdown-item" href="../m06-centrality/01-concepts.html">
 <span class="dropdown-text">Concepts</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m06-centrality/02-coding.html">
 <span class="dropdown-text">Coding</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m06-centrality/03-exercises.html">
 <span class="dropdown-text">Exercises</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-advanced-topics" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Advanced Topics</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-advanced-topics">    
        <li class="dropdown-header">─── M07: Random Walks ───</li>
        <li>
    <a class="dropdown-item" href="../m07-random-walks/01-concepts.html">
 <span class="dropdown-text">Concepts</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m07-random-walks/02-coding.html">
 <span class="dropdown-text">Coding</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m07-random-walks/03-exercises.html">
 <span class="dropdown-text">Exercises</span></a>
  </li>  
        <li class="dropdown-header">─── M08: Embedding ───</li>
        <li>
    <a class="dropdown-item" href="../m08-embedding/01-concepts.html">
 <span class="dropdown-text">Concepts</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m08-embedding/02-coding.html">
 <span class="dropdown-text">Coding</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m08-embedding/03-exercises.html">
 <span class="dropdown-text">Exercises</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m08-embedding/04-appendix.html">
 <span class="dropdown-text">Appendix</span></a>
  </li>  
        <li class="dropdown-header">─── M09: Graph Neural Networks ───</li>
        <li>
    <a class="dropdown-item" href="../m09-graph-neural-networks/01-concepts.html">
 <span class="dropdown-text">Concepts</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m09-graph-neural-networks/02-coding.html">
 <span class="dropdown-text">Coding</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m09-graph-neural-networks/03-exercises.html">
 <span class="dropdown-text">Exercises</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m09-graph-neural-networks/04-appendix.html">
 <span class="dropdown-text">Appendix</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">Characteristics of Random Walks</li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false">
 <span class="menu-text">Course Information</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course/welcome.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course/about.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About us</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course/discord.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Discord</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course/minidora-usage.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Using Minidora</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course/how-to-submit-assignment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">How to submit assignment</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">Introduction</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../intro/why-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Networks</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false">
 <span class="menu-text">M01: Euler Path</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m01-euler_tour/01-concepts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">A Stroll, Seven Bridges, and a Mathematical Revolution</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m01-euler_tour/02-coding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Coding Networks in Python</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m01-euler_tour/03-exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exercises</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m01-euler_tour/04-advanced.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Advanced: Sparse Matrices for Large-Scale Networks</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false">
 <span class="menu-text">M02: Small World</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m02-small-world/01-concepts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Core Concepts</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m02-small-world/02-coding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Efficient Network Representation and Computing Paths</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m02-small-world/03-exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exercises and Assignments</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m02-small-world/04-appendix.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Appendix - Brief Introduction to igraph</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false">
 <span class="menu-text">M03: Robustness</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m03-robustness/01-concepts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Core Concepts</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m03-robustness/02-coding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Coding - Network Robustness Analysis</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m03-robustness/03-exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exercises and Assignments</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m03-robustness/04-appendix.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exercises and Assignments</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false">
 <span class="menu-text">M04: Friendship Paradox</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-node-degree/01-concepts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Core Concepts</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-node-degree/02-coding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Visualizing Degree Distributions in Python</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-node-degree/03-exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exercises</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false">
 <span class="menu-text">M05: Clustering</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m05-clustering/01-concepts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Core Concepts</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m05-clustering/02-coding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Clustering Algorithms and Implementation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m05-clustering/03-exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exercises and Assignments</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="false">
 <span class="menu-text">M06: Centrality</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m06-centrality/01-concepts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Advanced Topics in Network Science</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m06-centrality/02-coding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Advanced Topics in Network Science</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m06-centrality/03-exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Advanced Topics in Network Science</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false">
 <span class="menu-text">M07: Random Walks</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m07-random-walks/01-concepts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Advanced Topics in Network Science</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m07-random-walks/02-coding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Advanced Topics in Network Science</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m07-random-walks/03-exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Advanced Topics in Network Science</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="false">
 <span class="menu-text">M08: Embedding</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m08-embedding/01-concepts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Advanced Topics in Network Science</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m08-embedding/02-coding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Advanced Topics in Network Science</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m08-embedding/03-exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Advanced Topics in Network Science</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m08-embedding/04-appendix.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Advanced Topics in Network Science</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="false">
 <span class="menu-text">M09: Graph Neural Networks</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-11" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m09-graph-neural-networks/01-concepts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Advanced Topics in Network Science</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m09-graph-neural-networks/02-coding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Advanced Topics in Network Science</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m09-graph-neural-networks/03-exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Advanced Topics in Network Science</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m09-graph-neural-networks/04-appendix.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Advanced Topics in Network Science</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Characteristics of Random Walks</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Sadamori Kojaku </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">July 27, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="stationary-state" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="stationary-state"><span class="header-section-number">1</span> Stationary State</h2>
<p>Let’s dive into the math behind random walks in a way that’s easy to understand.</p>
<p>Imagine you’re at node <span class="math inline">i</span> at time <span class="math inline">t</span>. You randomly move to a neighboring node <span class="math inline">j</span>. The probability of this move, called the transition probability <span class="math inline">p_{ij}</span>, is:</p>
<p><span class="math display">
p_{ij} = \frac{A_{ij}}{k_i},
</span></p>
<p>Here, <span class="math inline">A_{ij}</span> is an element of the adjacency matrix, and <span class="math inline">k_i</span> is the degree of node <span class="math inline">i</span>. For a network with <span class="math inline">N</span> nodes, we can represent all transition probabilities in a transition probability matrix <span class="math inline">P</span>:</p>
<p><span class="math display">
\mathbf{P} = \begin{pmatrix}
p_{11} &amp; p_{12} &amp; \cdots &amp; p_{1N} \\
p_{21} &amp; p_{22} &amp; \cdots &amp; p_{2N} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
p_{N1} &amp; p_{N2} &amp; \cdots &amp; p_{NN}
\end{pmatrix}
</span></p>
<p>This matrix <span class="math inline">P</span> encapsulates the entire random walk process. We can use it to calculate the probability of visiting each node after any number of steps. For instance:</p>
<ul>
<li>After one step: <span class="math inline">P_{ij} = p_{ij}</span></li>
<li>After two steps: <span class="math inline">\left(\mathbf{P}^{2}\right)_{ij} = \sum_{k} P_{ik} P_{kj}</span></li>
<li>After <span class="math inline">T</span> steps: <span class="math inline">\left(\mathbf{P}^{T}\right)_{ij}</span></li>
</ul>
<pre class="{note}"><code>Let's explore why $\mathbf{P}^2$ represents the transition probabilities after two steps.

First, recall that $\mathbf{P}_{ij}$ is the probability of moving from node $i$ to node $j$ in one step. Now, consider a two-step walk from $i$ to $j$. We can express this as:

$$(\mathbf{P}^2)_{ij} = \sum_k \mathbf{P}_{ik} \mathbf{P}_{kj}$$

This equation encapsulates a key idea: to go from $i$ to $j$ in two steps, we must pass through some intermediate node $k$. Let's break this down step by step:

1. The probability of the first step ($i$ to $k$) is $\mathbf{P}_{ik}$.
2. The probability of the second step ($k$ to $j$) is $\mathbf{P}_{kj}$.
3. The probability of this specific path ($i$ → $k$ → $j$) is the product $\mathbf{P}_{ik} \mathbf{P}_{kj}$.
4. We sum over all possible intermediate nodes $k$ to get the total probability.

Likewise, for three steps, we have:

$$(\mathbf{P}^3)_{ij} = \sum_k \left( \mathbf{P}\right)^2_{ik} \mathbf{P}_{kj}$$

where:
1. The probability of going from $i$ to $k$ in two steps is $\left( \mathbf{P}\right)^2_{ik}$.
2. The probability of going from $k$ to $j$ in one step is $\mathbf{P}_{kj}$.
3. The probability of this specific path ($i$ →...→$k$ → $j$) is the product $\left( \mathbf{P}\right)^2_{ik} \mathbf{P}_{kj}$.
4. We sum over all possible intermediate nodes $k$ to get the total probability.

And we can extend this reasoning for any number of steps $t$.

In summary, for any number of steps $t$, $\left( \mathbf{P}^t \right)_{ij}$ gives the probability of being at node $j$ after $t$ steps, starting from node $i$.
</code></pre>
<p>As <span class="math inline">T</span> becomes very large, the probability distribution of being at each node, <span class="math inline">\mathbf{x}(t)</span>, approaches a constant value:</p>
<p><span class="math display">
\mathbf{x}(t+1) =\mathbf{x}(t) \mathbf{P}
</span></p>
<p>This is an eigenvector equation. The solution, given by the Perron-Frobenius theorem, is called the stationary distribution:</p>
<p><span class="math display">
\mathbf{x}(\infty) = \mathbb{\pi}, \; \mathbf{\pi} = [\pi_1, \ldots, \pi_N]
</span></p>
<p>For undirected networks, this stationary distribution always exists and is proportional to the degree of each node:</p>
<p><span class="math display">
\pi_j = \frac{k_j}{\sum_{\ell} k_\ell} \propto k_j
</span></p>
<p>This means the probability of being at node <span class="math inline">j</span> in the long run is proportional to the degree of node <span class="math inline">j</span>. The normalization ensures that the sum of all probabilities is 1, i.e., <span class="math inline">\sum_{j=1}^N \pi_j = 1</span>.</p>
</section>
<section id="experiment" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="experiment"><span class="header-section-number">2</span> Experiment</h2>
<p>Let us demonstrate the above math by using a small network using Python. Let us consider a small network of 5 nodes, which looks like this:</p>
<div id="72657981" class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> igraph <span class="im">as</span> ig</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>edge_list <span class="op">=</span> []</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>):</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(i<span class="op">+</span><span class="dv">1</span>, <span class="dv">5</span>):</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>        edge_list.append((i, j))</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>        edge_list.append((i<span class="op">+</span><span class="dv">5</span>, j<span class="op">+</span><span class="dv">5</span>))</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>edge_list.append((<span class="dv">0</span>, <span class="dv">6</span>))</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> ig.Graph(edge_list)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>ig.plot(g, vertex_size<span class="op">=</span><span class="dv">20</span>, vertex_label<span class="op">=</span>np.arange(g.vcount()))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The transition probability matrix <span class="math inline">P</span> is given by:</p>
<div id="54af21c5" class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy.sparse <span class="im">as</span> sparse</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> g.get_adjacency_sparse()</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>deg <span class="op">=</span> np.array(A.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>)).flatten()</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>Dinv <span class="op">=</span> sparse.diags(<span class="dv">1</span><span class="op">/</span>deg)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>P <span class="op">=</span> Dinv <span class="op">@</span> A</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>P.toarray()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let us compute the stationary distribution by using the power method.</p>
<div id="0fd6f245" class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.zeros(g.vcount())</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>x[<span class="dv">1</span>] <span class="op">=</span> <span class="dv">1</span> <span class="co"># Start from node 1</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>xt <span class="op">=</span> []</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(T):</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> x.reshape(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>) <span class="op">@</span> P</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    xt.append(x)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>xt <span class="op">=</span> np.vstack(xt) <span class="co"># Stack the results vertically</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">7</span>,<span class="dv">5</span>))</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>palette <span class="op">=</span> sns.color_palette().as_hex()</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(g.vcount()):</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>    sns.lineplot(x<span class="op">=</span><span class="bu">range</span>(T), y<span class="op">=</span>xt[:, i], label<span class="op">=</span><span class="ss">f"Node </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>, ax<span class="op">=</span>ax, color<span class="op">=</span>palette[i])</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"Time"</span>)</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"Probability"</span>)</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">"Stationary distribution of a random walk"</span>)</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>ax.legend(bbox_to_anchor<span class="op">=</span>(<span class="fl">1.05</span>, <span class="dv">1</span>), loc<span class="op">=</span><span class="st">'upper left'</span>)</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We see that the distributions of the walker converges, and there are three characteristic features in the convergence: 1. The distribution of the walker occilates with a decying amplitude and eventually converges. 2. Nodes of the same degree converge to the same stationary probability. 3. Nodes with higher degree converge to the higher stationary probability.</p>
<p>To validate the last two observation, let us compare the stationary distribution of a random walker with the expected stationary distribution, which is proportional to the degree of the nodes.</p>
<div id="79e91a4f" class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>n_edges <span class="op">=</span> np.<span class="bu">sum</span>(deg) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>expected_stationary_dist <span class="op">=</span> deg <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> n_edges)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>pd.DataFrame({</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Expected stationary distribution"</span>: expected_stationary_dist,</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Stationary distribution of a random walk"</span>: xt[<span class="op">-</span><span class="dv">1</span>].flatten()</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>}).style.<span class="bu">format</span>(<span class="st">"</span><span class="sc">{:.4f}</span><span class="st">"</span>).set_caption(<span class="st">"Comparison of Expected and Observed Stationary Distributions"</span>).background_gradient(cmap<span class="op">=</span><span class="st">'cividis'</span>, axis <span class="op">=</span> <span class="va">None</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="time-to-reach-the-stationary-state" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="time-to-reach-the-stationary-state"><span class="header-section-number">3</span> Time to reach the stationary state</h2>
<p>Let’s explore how quickly a random walker reaches its stationary state. The convergence speed is influenced by two main factors: edge density and community structure. In sparse networks, the walker needs more steps to explore the entire network. Additionally, the walker tends to remain within its starting community for some time.</p>
<p>The mixing time, denoted as <span class="math inline">t_{\text{mix}}</span>, is defined as the minimum number of steps required for a random walk to get close to the stationary distribution, regardless of the starting point, with the maximum error less than <span class="math inline">\epsilon</span>:</p>
<p><span class="math display">t_{\text{mix}} = \min\{t : \max_{{\bf x}(0)} \|{\bf x}(t) - {\bf \pi}\|_{1} \leq \epsilon\}</span></p>
<p>where <span class="math inline">\|{\bf x}(t) - {\bf \pi}\|_{1} = 2\max_{i} |x_i(t) - \pi_i|</span> represents the L1 distance between two probability distributions. The choice of <span class="math inline">\epsilon</span> is arbitrary.</p>
<p>We know that the distribution of a walker after <span class="math inline">t</span> steps is given by:</p>
<p><span class="math display">
\mathbf{x}(t) =  \mathbf{x}(0) \mathbf{P} ^{t}
</span></p>
<p>To find this distribution, we need to compute <span class="math inline">\mathbf{P}^t</span>. However, we face a challenge: <span class="math inline">\mathbf{P}</span> is not diagonalizable.</p>
<p>A diagonalizable matrix <span class="math inline">\mathbf{S}</span> can be written as <span class="math inline">\mathbf{S} = \mathbf{Q} \mathbf{\Lambda} \mathbf{Q}^{-1}</span>, where <span class="math inline">\mathbf{\Lambda}</span> is a diagonal matrix and <span class="math inline">\mathbf{Q}</span> is an orthogonal matrix. Visually, it looks like this:</p>
<p><img src="../figs/diagonalizable.jpg" class="img-fluid"></p>
<p>It is useful because we can then compute the power of the matrix as follows:</p>
<p><span class="math display">
\mathbf{S}^t = \mathbf{Q} \mathbf{\Lambda}^t \mathbf{Q}^{-1}
</span></p>
<p>And it is easy to find <span class="math inline">{\bf Q}</span> and <span class="math inline">{\bf \Lambda}</span> by using eigenvalue decomposition if <span class="math inline">{\bf S}</span> is symmetric and consists only of real values. Namely, the eigenvectors form <span class="math inline">{\cal Q}</span> and the eigenvalues form the diagonal matrix <span class="math inline">{\cal \Lambda}</span>.</p>
<pre class="{note}"><code>Let us demonstrate the above relation by calculating $\mathbf{S}^2$.
$$
\begin{align}
\mathbf{S}^2 &amp;= \mathbf{Q} \mathbf{\Lambda} \mathbf{Q}^{-1} \mathbf{Q} \mathbf{\Lambda} \mathbf{Q}^{-1} \\
&amp;= \mathbf{Q} \mathbf{\Lambda}^2 \mathbf{Q}^{-1}.
\end{align}
$$
(Note that $\mathbf{Q} \mathbf{Q}^{-1} = {\bf I}$.)

![](../figs/diagonalizable-squared.jpg)
</code></pre>
<p><span class="math inline">\mathbf{P}</span> is also diagonalizable but not symmetric like <span class="math inline">\mathbf{\overline A}</span> so that we cannot use the above relation directly. So we do a trick by rewriteing <span class="math inline">\mathbf{P}</span> as:</p>
<p><span class="math display">
\mathbf{P} = \mathbf{D}^{-1} \mathbf{A} = \mathbf{D}^{-\frac{1}{2}} \overline {\bf A} \mathbf{D}^{\frac{1}{2}}
</span></p>
<p>where <span class="math inline">\overline{\bf A} = \mathbf{D}^{-\frac{1}{2}} \mathbf{A} \mathbf{D}^{-\frac{1}{2}}</span> is the normalized adjacency matrix.</p>
<p>The advantage is that <span class="math inline">\overline{\bf A}</span> is diagonalizable: <span class="math inline">\overline{\bf A} = \mathbf{Q} \mathbf{\Lambda} \mathbf{Q}^\top</span>. Using this, we can compute <span class="math inline">\mathbf{P}^t</span>:</p>
<p><span class="math display">
\mathbf{P}^t = \mathbf{D}^{-\frac{1}{2}} \mathbf{Q} \mathbf{\Lambda}^t \mathbf{Q}^\top \mathbf{D}^{\frac{1}{2}} = \mathbf{Q}_L \mathbf{\Lambda}^t \mathbf{Q}_R ^\top
</span></p>
<p>where <span class="math inline">\mathbf{Q}_L = \mathbf{D}^{-\frac{1}{2}} \mathbf{Q}</span> and <span class="math inline">\mathbf{Q}_R = \mathbf{D}^{\frac{1}{2}} \mathbf{Q}</span>.</p>
<pre class="{note}"><code>Let us demonstrate the above relation by calculating $\mathbf{P}^2$.

$$
\begin{align}
\mathbf{P}^2 &amp;= \mathbf{D}^{-\frac{1}{2}} \overline{\bf A} \mathbf{D}^{\frac{1}{2}} \mathbf{D}^{-\frac{1}{2}} \overline{\bf A} \mathbf{D}^{\frac{1}{2}}\\
&amp;=  \mathbf{D}^{-\frac{1}{2}} \overline{\bf A} ^2 \mathbf{D}^{\frac{1}{2}}\\
&amp;= \mathbf{Q}_L \mathbf{\Lambda}^2 \mathbf{Q}_R ^\top
\end{align}</code></pre>
<p>The probability distribution after <span class="math inline">t</span> steps is then:</p>
<p><span class="math display">
\mathbf{x}(t) = \mathbf{x}(0) \mathbf{Q}_L \mathbf{\Lambda}^t \mathbf{Q}_R ^\top
</span></p>
<p>We can rewrite this in a more intuitive form:</p>
<p><span class="math display">
\begin{pmatrix}
x_1(t) \\
x_2(t) \\
\vdots \\
x_N(t)
\end{pmatrix}
=
\sum_{\ell=1}^N
\left[
\lambda_\ell^t
\begin{pmatrix}
q^{(L)}_{\ell 1} \\
q^{(L)}_{\ell 2} \\
\vdots \\
q^{(L)}_{\ell N}
\end{pmatrix}
\langle\mathbf{q}^{(R)}_{\ell},  \mathbf{x}(0) \rangle
\right]
</span></p>
<pre class="{note}"><code>Visualize the above equation by using the following figure.

![](../figs/diagonalizable-sum.jpg)
</code></pre>
<p>The term <span class="math inline">\lambda_\ell^t</span> represents the contribution of each eigenvalue to the stationary distribution over time. As <span class="math inline">t</span> increases, all terms decay exponentially except for the largest eigenvalue (<span class="math inline">\lambda_1 = 1</span>). This explains how the random walk converges to the stationary distribution:</p>
<p><span class="math display">
\pi_i = \lim_{t\to\infty} x_i(t) = \begin{pmatrix} q^{(L)}_{1 1} \\ q^{(L)}_{1 2} \\ \vdots \\ q^{(L)}_{1 N} \end{pmatrix} \langle\mathbf{q}^{(R)}_{1},  \mathbf{x}(0) \rangle
</span></p>
<p>The second largest eigenvalue primarily determines the convergence speed to the stationary distribution. A larger second eigenvalue leads to slower convergence. Thus, the mixing time is closely related to the second largest eigenvalue.</p>
<p>Levin-Peres-Wilmer theorem states that the mixing time is bounded by the relaxation time as</p>
<p><span class="math display">
t_{\text{mix}} &lt; \tau \log \left( \frac{1}{\epsilon \min_{i} \pi_i} \right), \quad \tau = \frac{1}{1-\lambda_2}
</span></p>
<p>where <span class="math inline">\lambda_2</span> is the second largest eigenvalue of the normalized adjacency matrix. The mixing time is known to be bounded by the relaxation time as</p>
<p>More commonly, it is expressed using the second smallest eigenvalue <span class="math inline">\mu</span> of the normalized laplacian matrix as</p>
<p><span class="math display">
t_{\text{mix}} \leq \frac{1}{\mu}
</span></p>
<p>where <span class="math inline">\mu = 1-\lambda_2</span>.</p>
<section id="compute-the-mixing-time" class="level3">
<h3 class="anchored" data-anchor-id="compute-the-mixing-time">Compute the mixing time</h3>
<p>Let us demonstrate the above math by using the network of two cliques.</p>
</section>
<section id="normalized-adjacency-matrix" class="level3">
<h3 class="anchored" data-anchor-id="normalized-adjacency-matrix">Normalized Adjacency Matrix</h3>
<p>First, let us construct the normalized adjacency matrix <span class="math inline">\overline{\bf A}</span> of the network.</p>
<div id="a6d7c618" class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>Dinv_sqrt <span class="op">=</span> sparse.diags(<span class="fl">1.0</span><span class="op">/</span>np.sqrt(deg))</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>A_norm <span class="op">=</span> Dinv_sqrt <span class="op">@</span> A <span class="op">@</span> Dinv_sqrt</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Next, let us compute the eigenvalues and eigenvectors of the normalized adjacency matrix.</p>
<div id="d9785482" class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>evals, evecs <span class="op">=</span> np.linalg.eigh(A_norm.toarray())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<pre class="{note}"><code>`evals` and `evecs` are sorted in descending order of the eigenvalues. `evecs[:, 0]` is the eigenvector corresponding to the largest eigenvalue, which is always 1.</code></pre>
<pre class="{warning}"><code>There is a similar function called `np.linalg.eig` which returns the eigenvalues and eigenvectors. It can be used for any matrices, while `np.linalg.eigh` is specifically for symmetric matrices. `np.linalg.eigh` is faster and more stable and therefore preferred if your matrix is symmetric. `np.linalg.eig` is more susceptible to numerical errors and therefore less stable.</code></pre>
<p>The eigenvalues and eigenvectors are shown below.</p>
<div id="b56d8577" class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>pd.DataFrame({</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Eigenvalue"</span>: evals</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>}).T.style.background_gradient(cmap<span class="op">=</span><span class="st">'cividis'</span>, axis <span class="op">=</span> <span class="dv">1</span>).set_caption(<span class="st">"Eigenvalues of the normalized adjacency matrix"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="ab8a6997" class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>pd.DataFrame({</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Eigenvector </span><span class="sc">%i</span><span class="st">"</span> <span class="op">%</span> i: evecs[:, i]</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>)</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>}).style.background_gradient(cmap<span class="op">=</span><span class="st">'cividis'</span>, axis <span class="op">=</span> <span class="va">None</span>).set_caption(<span class="st">"Eigenvectors of the normalized adjacency matrix"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Notice that the largest eigenvalue is 1, which is always true for a normalized adjacency matrix. The largest eigenvector (the leftmost one) is associated with the stationary distribution of the random walk.</p>
<pre class="{note}"><code>The sign of the eigenvector is indeterminate, which means we can choose the sign of the eigenvector arbitrarily. In fact, `np.linalg.eigh` returns the eigenvector whose sign can vary for a different run.</code></pre>
<p>We decompose <span class="math inline">\overline{\bf A}</span> as</p>
<p><span class="math display">\overline {\bf A} = {\bf Q}{\bf \Lambda}{\bf Q}^\top</span></p>
<p>where <span class="math inline">{\bf Q}</span> corresponds to <code>eigvecs</code> and <span class="math inline">{\bf \Lambda}</span> corresponds to <code>np.diag(evals)</code> (since <span class="math inline">{\bf \Lambda}</span> is a diagonal matrix). Let’s see if this is correct:</p>
<div id="7d1bd01b" class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(A_norm.toarray()).style.background_gradient(cmap<span class="op">=</span><span class="st">'cividis'</span>, axis <span class="op">=</span> <span class="va">None</span>).set_caption(<span class="st">"Normalized Adjacency Matrix"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="60755beb" class="cell">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>A_norm_reconstructed <span class="op">=</span> evecs <span class="op">@</span> np.diag(evals) <span class="op">@</span> evecs.T</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(A_norm_reconstructed).style.background_gradient(cmap<span class="op">=</span><span class="st">'cividis'</span>, axis <span class="op">=</span> <span class="va">None</span>).set_caption(<span class="st">"Reconstruction of the Normalized Adjacency Matrix"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Notice that the reconstruction is not perfect due to the numerical error, although overall the structure is correct.</p>
</section>
<section id="multi-step-transition-probability" class="level3">
<h3 class="anchored" data-anchor-id="multi-step-transition-probability">Multi-step Transition Probability</h3>
<p>Let us first conform whether we can compute the transition probability after <span class="math inline">t</span> steps by using the eigenvalues and eigenvectors.</p>
<div id="c9bc2060" class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>x_0 <span class="op">=</span> np.zeros(g.vcount())</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>x_0[<span class="dv">0</span>] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute x_t by using the eigenvalues and eigenvectors</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>Q_L <span class="op">=</span> np.diag(<span class="fl">1.0</span><span class="op">/</span>np.sqrt(deg)) <span class="op">@</span> evecs</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>Q_R <span class="op">=</span> np.diag(np.sqrt(deg)) <span class="op">@</span> evecs</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>x_t <span class="op">=</span> x_0 <span class="op">@</span> Q_L <span class="op">@</span> np.diag(evals<span class="op">**</span>t) <span class="op">@</span> Q_R.T</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute x_t by using the power iteration</span></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>x_t_power <span class="op">=</span> x_0.copy()</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(t):</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>    x_t_power <span class="op">=</span> x_t_power <span class="op">@</span> P</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>pd.DataFrame({</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Eigenvector"</span>: x_t.flatten(),</span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Power iteration"</span>: x_t_power.flatten()</span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>}).style.background_gradient(cmap<span class="op">=</span><span class="st">'cividis'</span>, axis <span class="op">=</span> <span class="va">None</span>).set_caption(<span class="st">"Comparison of Eigenvector and Power Iteration"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="relaxation-time-and-mixing-time" class="level3">
<h3 class="anchored" data-anchor-id="relaxation-time-and-mixing-time">Relaxation Time and Mixing Time</h3>
<p>Let us measure the relaxation time of the random walk.</p>
<div id="4d033218" class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>evals, evecs <span class="op">=</span> np.linalg.eigh(A_norm.toarray())</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>lambda_2 <span class="op">=</span> <span class="op">-</span>np.sort(<span class="op">-</span>evals)[<span class="dv">1</span>]</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>tau <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> lambda_2</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"The relaxation time of the random walk is </span><span class="sc">{</span>tau<span class="sc">:.4f}</span><span class="ss">."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>


<!-- -->

</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/skojaku\.github\.io\/adv-net-sci\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb20" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="fu"># Characteristics of Random Walks</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="fu">## Stationary State</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>Let's dive into the math behind random walks in a way that's easy to understand.</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>Imagine you're at node $i$ at time $t$. You randomly move to a neighboring node $j$. The probability of this move, called the transition probability $p_{ij}$, is:</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>p_{ij} = \frac{A_{ij}}{k_i},</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>Here, $A_{ij}$ is an element of the adjacency matrix, and $k_i$ is the degree of node $i$. For a network with $N$ nodes, we can represent all transition probabilities in a transition probability matrix $P$:</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>\mathbf{P} = \begin{pmatrix}</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>p_{11} &amp; p_{12} &amp; \cdots &amp; p_{1N} <span class="sc">\\</span></span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>p_{21} &amp; p_{22} &amp; \cdots &amp; p_{2N} <span class="sc">\\</span></span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>\vdots &amp; \vdots &amp; \ddots &amp; \vdots <span class="sc">\\</span></span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>p_{N1} &amp; p_{N2} &amp; \cdots &amp; p_{NN}</span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a>This matrix $P$ encapsulates the entire random walk process. We can use it to calculate the probability of visiting each node after any number of steps. For instance:</span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>After one step: $P_{ij} = p_{ij}$</span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>After two steps: $\left(\mathbf{P}^{2}\right)_{ij} = \sum_{k} P_{ik} P_{kj}$</span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>After $T$ steps: $\left(\mathbf{P}^{T}\right)_{ij}$</span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a><span class="in">```{note}</span></span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a><span class="in">Let's explore why $\mathbf{P}^2$ represents the transition probabilities after two steps.</span></span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true" tabindex="-1"></a><span class="in">First, recall that $\mathbf{P}_{ij}$ is the probability of moving from node $i$ to node $j$ in one step. Now, consider a two-step walk from $i$ to $j$. We can express this as:</span></span>
<span id="cb20-36"><a href="#cb20-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-37"><a href="#cb20-37" aria-hidden="true" tabindex="-1"></a><span class="in">$$(\mathbf{P}^2)_{ij} = \sum_k \mathbf{P}_{ik} \mathbf{P}_{kj}$$</span></span>
<span id="cb20-38"><a href="#cb20-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-39"><a href="#cb20-39" aria-hidden="true" tabindex="-1"></a><span class="in">This equation encapsulates a key idea: to go from $i$ to $j$ in two steps, we must pass through some intermediate node $k$. Let's break this down step by step:</span></span>
<span id="cb20-40"><a href="#cb20-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-41"><a href="#cb20-41" aria-hidden="true" tabindex="-1"></a><span class="in">1. The probability of the first step ($i$ to $k$) is $\mathbf{P}_{ik}$.</span></span>
<span id="cb20-42"><a href="#cb20-42" aria-hidden="true" tabindex="-1"></a><span class="in">2. The probability of the second step ($k$ to $j$) is $\mathbf{P}_{kj}$.</span></span>
<span id="cb20-43"><a href="#cb20-43" aria-hidden="true" tabindex="-1"></a><span class="in">3. The probability of this specific path ($i$ → $k$ → $j$) is the product $\mathbf{P}_{ik} \mathbf{P}_{kj}$.</span></span>
<span id="cb20-44"><a href="#cb20-44" aria-hidden="true" tabindex="-1"></a><span class="in">4. We sum over all possible intermediate nodes $k$ to get the total probability.</span></span>
<span id="cb20-45"><a href="#cb20-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-46"><a href="#cb20-46" aria-hidden="true" tabindex="-1"></a><span class="in">Likewise, for three steps, we have:</span></span>
<span id="cb20-47"><a href="#cb20-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-48"><a href="#cb20-48" aria-hidden="true" tabindex="-1"></a><span class="in">$$(\mathbf{P}^3)_{ij} = \sum_k \left( \mathbf{P}\right)^2_{ik} \mathbf{P}_{kj}$$</span></span>
<span id="cb20-49"><a href="#cb20-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-50"><a href="#cb20-50" aria-hidden="true" tabindex="-1"></a><span class="in">where:</span></span>
<span id="cb20-51"><a href="#cb20-51" aria-hidden="true" tabindex="-1"></a><span class="in">1. The probability of going from $i$ to $k$ in two steps is $\left( \mathbf{P}\right)^2_{ik}$.</span></span>
<span id="cb20-52"><a href="#cb20-52" aria-hidden="true" tabindex="-1"></a><span class="in">2. The probability of going from $k$ to $j$ in one step is $\mathbf{P}_{kj}$.</span></span>
<span id="cb20-53"><a href="#cb20-53" aria-hidden="true" tabindex="-1"></a><span class="in">3. The probability of this specific path ($i$ →...→$k$ → $j$) is the product $\left( \mathbf{P}\right)^2_{ik} \mathbf{P}_{kj}$.</span></span>
<span id="cb20-54"><a href="#cb20-54" aria-hidden="true" tabindex="-1"></a><span class="in">4. We sum over all possible intermediate nodes $k$ to get the total probability.</span></span>
<span id="cb20-55"><a href="#cb20-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-56"><a href="#cb20-56" aria-hidden="true" tabindex="-1"></a><span class="in">And we can extend this reasoning for any number of steps $t$.</span></span>
<span id="cb20-57"><a href="#cb20-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-58"><a href="#cb20-58" aria-hidden="true" tabindex="-1"></a><span class="in">In summary, for any number of steps $t$, $\left( \mathbf{P}^t \right)_{ij}$ gives the probability of being at node $j$ after $t$ steps, starting from node $i$.</span></span>
<span id="cb20-59"><a href="#cb20-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-60"><a href="#cb20-60" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-61"><a href="#cb20-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-62"><a href="#cb20-62" aria-hidden="true" tabindex="-1"></a>As $T$ becomes very large, the probability distribution of being at each node, $\mathbf{x}(t)$, approaches a constant value:</span>
<span id="cb20-63"><a href="#cb20-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-64"><a href="#cb20-64" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-65"><a href="#cb20-65" aria-hidden="true" tabindex="-1"></a>\mathbf{x}(t+1) =\mathbf{x}(t) \mathbf{P}</span>
<span id="cb20-66"><a href="#cb20-66" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-67"><a href="#cb20-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-68"><a href="#cb20-68" aria-hidden="true" tabindex="-1"></a>This is an eigenvector equation. The solution, given by the Perron-Frobenius theorem, is called the stationary distribution:</span>
<span id="cb20-69"><a href="#cb20-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-70"><a href="#cb20-70" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-71"><a href="#cb20-71" aria-hidden="true" tabindex="-1"></a>\mathbf{x}(\infty) = \mathbb{\pi}, \; \mathbf{\pi} = <span class="co">[</span><span class="ot">\pi_1, \ldots, \pi_N</span><span class="co">]</span></span>
<span id="cb20-72"><a href="#cb20-72" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-73"><a href="#cb20-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-74"><a href="#cb20-74" aria-hidden="true" tabindex="-1"></a>For undirected networks, this stationary distribution always exists and is proportional to the degree of each node:</span>
<span id="cb20-75"><a href="#cb20-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-76"><a href="#cb20-76" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-77"><a href="#cb20-77" aria-hidden="true" tabindex="-1"></a>\pi_j = \frac{k_j}{\sum_{\ell} k_\ell} \propto k_j</span>
<span id="cb20-78"><a href="#cb20-78" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-79"><a href="#cb20-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-80"><a href="#cb20-80" aria-hidden="true" tabindex="-1"></a>This means the probability of being at node $j$ in the long run is proportional to the degree of node $j$. The normalization ensures that the sum of all probabilities is 1, i.e., $\sum_{j=1}^N \pi_j = 1$.</span>
<span id="cb20-81"><a href="#cb20-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-82"><a href="#cb20-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-83"><a href="#cb20-83" aria-hidden="true" tabindex="-1"></a><span class="fu">## Experiment</span></span>
<span id="cb20-84"><a href="#cb20-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-85"><a href="#cb20-85" aria-hidden="true" tabindex="-1"></a>Let us demonstrate the above math by using a small network using Python. Let us consider a small network of 5 nodes, which looks like this:</span>
<span id="cb20-86"><a href="#cb20-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-89"><a href="#cb20-89" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-90"><a href="#cb20-90" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> igraph <span class="im">as</span> ig</span>
<span id="cb20-91"><a href="#cb20-91" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb20-92"><a href="#cb20-92" aria-hidden="true" tabindex="-1"></a>edge_list <span class="op">=</span> []</span>
<span id="cb20-93"><a href="#cb20-93" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>):</span>
<span id="cb20-94"><a href="#cb20-94" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(i<span class="op">+</span><span class="dv">1</span>, <span class="dv">5</span>):</span>
<span id="cb20-95"><a href="#cb20-95" aria-hidden="true" tabindex="-1"></a>        edge_list.append((i, j))</span>
<span id="cb20-96"><a href="#cb20-96" aria-hidden="true" tabindex="-1"></a>        edge_list.append((i<span class="op">+</span><span class="dv">5</span>, j<span class="op">+</span><span class="dv">5</span>))</span>
<span id="cb20-97"><a href="#cb20-97" aria-hidden="true" tabindex="-1"></a>edge_list.append((<span class="dv">0</span>, <span class="dv">6</span>))</span>
<span id="cb20-98"><a href="#cb20-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-99"><a href="#cb20-99" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> ig.Graph(edge_list)</span>
<span id="cb20-100"><a href="#cb20-100" aria-hidden="true" tabindex="-1"></a>ig.plot(g, vertex_size<span class="op">=</span><span class="dv">20</span>, vertex_label<span class="op">=</span>np.arange(g.vcount()))</span>
<span id="cb20-101"><a href="#cb20-101" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-102"><a href="#cb20-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-103"><a href="#cb20-103" aria-hidden="true" tabindex="-1"></a>The transition probability matrix $P$ is given by:</span>
<span id="cb20-104"><a href="#cb20-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-107"><a href="#cb20-107" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-108"><a href="#cb20-108" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy.sparse <span class="im">as</span> sparse</span>
<span id="cb20-109"><a href="#cb20-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-110"><a href="#cb20-110" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> g.get_adjacency_sparse()</span>
<span id="cb20-111"><a href="#cb20-111" aria-hidden="true" tabindex="-1"></a>deg <span class="op">=</span> np.array(A.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>)).flatten()</span>
<span id="cb20-112"><a href="#cb20-112" aria-hidden="true" tabindex="-1"></a>Dinv <span class="op">=</span> sparse.diags(<span class="dv">1</span><span class="op">/</span>deg)</span>
<span id="cb20-113"><a href="#cb20-113" aria-hidden="true" tabindex="-1"></a>P <span class="op">=</span> Dinv <span class="op">@</span> A</span>
<span id="cb20-114"><a href="#cb20-114" aria-hidden="true" tabindex="-1"></a>P.toarray()</span>
<span id="cb20-115"><a href="#cb20-115" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-116"><a href="#cb20-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-117"><a href="#cb20-117" aria-hidden="true" tabindex="-1"></a>Let us compute the stationary distribution by using the power method.</span>
<span id="cb20-118"><a href="#cb20-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-121"><a href="#cb20-121" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-122"><a href="#cb20-122" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb20-123"><a href="#cb20-123" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb20-124"><a href="#cb20-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-125"><a href="#cb20-125" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.zeros(g.vcount())</span>
<span id="cb20-126"><a href="#cb20-126" aria-hidden="true" tabindex="-1"></a>x[<span class="dv">1</span>] <span class="op">=</span> <span class="dv">1</span> <span class="co"># Start from node 1</span></span>
<span id="cb20-127"><a href="#cb20-127" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb20-128"><a href="#cb20-128" aria-hidden="true" tabindex="-1"></a>xt <span class="op">=</span> []</span>
<span id="cb20-129"><a href="#cb20-129" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(T):</span>
<span id="cb20-130"><a href="#cb20-130" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> x.reshape(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>) <span class="op">@</span> P</span>
<span id="cb20-131"><a href="#cb20-131" aria-hidden="true" tabindex="-1"></a>    xt.append(x)</span>
<span id="cb20-132"><a href="#cb20-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-133"><a href="#cb20-133" aria-hidden="true" tabindex="-1"></a>xt <span class="op">=</span> np.vstack(xt) <span class="co"># Stack the results vertically</span></span>
<span id="cb20-134"><a href="#cb20-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-135"><a href="#cb20-135" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">7</span>,<span class="dv">5</span>))</span>
<span id="cb20-136"><a href="#cb20-136" aria-hidden="true" tabindex="-1"></a>palette <span class="op">=</span> sns.color_palette().as_hex()</span>
<span id="cb20-137"><a href="#cb20-137" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(g.vcount()):</span>
<span id="cb20-138"><a href="#cb20-138" aria-hidden="true" tabindex="-1"></a>    sns.lineplot(x<span class="op">=</span><span class="bu">range</span>(T), y<span class="op">=</span>xt[:, i], label<span class="op">=</span><span class="ss">f"Node </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>, ax<span class="op">=</span>ax, color<span class="op">=</span>palette[i])</span>
<span id="cb20-139"><a href="#cb20-139" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"Time"</span>)</span>
<span id="cb20-140"><a href="#cb20-140" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"Probability"</span>)</span>
<span id="cb20-141"><a href="#cb20-141" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">"Stationary distribution of a random walk"</span>)</span>
<span id="cb20-142"><a href="#cb20-142" aria-hidden="true" tabindex="-1"></a>ax.legend(bbox_to_anchor<span class="op">=</span>(<span class="fl">1.05</span>, <span class="dv">1</span>), loc<span class="op">=</span><span class="st">'upper left'</span>)</span>
<span id="cb20-143"><a href="#cb20-143" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb20-144"><a href="#cb20-144" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb20-145"><a href="#cb20-145" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-146"><a href="#cb20-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-147"><a href="#cb20-147" aria-hidden="true" tabindex="-1"></a>We see that the distributions of the walker converges, and there are three characteristic features in the convergence:</span>
<span id="cb20-148"><a href="#cb20-148" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>The distribution of the walker occilates with a decying amplitude and eventually converges.</span>
<span id="cb20-149"><a href="#cb20-149" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Nodes of the same degree converge to the same stationary probability.</span>
<span id="cb20-150"><a href="#cb20-150" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Nodes with higher degree converge to the higher stationary probability.</span>
<span id="cb20-151"><a href="#cb20-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-152"><a href="#cb20-152" aria-hidden="true" tabindex="-1"></a>To validate the last two observation, let us compare the stationary distribution of a random walker with the expected stationary distribution, which is proportional to the degree of the nodes.</span>
<span id="cb20-153"><a href="#cb20-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-156"><a href="#cb20-156" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-157"><a href="#cb20-157" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb20-158"><a href="#cb20-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-159"><a href="#cb20-159" aria-hidden="true" tabindex="-1"></a>n_edges <span class="op">=</span> np.<span class="bu">sum</span>(deg) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb20-160"><a href="#cb20-160" aria-hidden="true" tabindex="-1"></a>expected_stationary_dist <span class="op">=</span> deg <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> n_edges)</span>
<span id="cb20-161"><a href="#cb20-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-162"><a href="#cb20-162" aria-hidden="true" tabindex="-1"></a>pd.DataFrame({</span>
<span id="cb20-163"><a href="#cb20-163" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Expected stationary distribution"</span>: expected_stationary_dist,</span>
<span id="cb20-164"><a href="#cb20-164" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Stationary distribution of a random walk"</span>: xt[<span class="op">-</span><span class="dv">1</span>].flatten()</span>
<span id="cb20-165"><a href="#cb20-165" aria-hidden="true" tabindex="-1"></a>}).style.<span class="bu">format</span>(<span class="st">"</span><span class="sc">{:.4f}</span><span class="st">"</span>).set_caption(<span class="st">"Comparison of Expected and Observed Stationary Distributions"</span>).background_gradient(cmap<span class="op">=</span><span class="st">'cividis'</span>, axis <span class="op">=</span> <span class="va">None</span>)</span>
<span id="cb20-166"><a href="#cb20-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-167"><a href="#cb20-167" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-168"><a href="#cb20-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-169"><a href="#cb20-169" aria-hidden="true" tabindex="-1"></a><span class="fu">## Time to reach the stationary state</span></span>
<span id="cb20-170"><a href="#cb20-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-171"><a href="#cb20-171" aria-hidden="true" tabindex="-1"></a>Let's explore how quickly a random walker reaches its stationary state. The convergence speed is influenced by two main factors: edge density and community structure. In sparse networks, the walker needs more steps to explore the entire network. Additionally, the walker tends to remain within its starting community for some time.</span>
<span id="cb20-172"><a href="#cb20-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-173"><a href="#cb20-173" aria-hidden="true" tabindex="-1"></a>The mixing time, denoted as $t_{\text{mix}}$, is defined as the minimum number of steps required for a random walk to get close to the stationary distribution, regardless of the starting point, with the maximum error less than $\epsilon$:</span>
<span id="cb20-174"><a href="#cb20-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-175"><a href="#cb20-175" aria-hidden="true" tabindex="-1"></a>$$t_{\text{mix}} = \min<span class="sc">\{</span>t : \max_{{\bf x}(0)} <span class="sc">\|</span>{\bf x}(t) - {\bf \pi}<span class="sc">\|</span>_{1} \leq \epsilon<span class="sc">\}</span>$$</span>
<span id="cb20-176"><a href="#cb20-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-177"><a href="#cb20-177" aria-hidden="true" tabindex="-1"></a>where $<span class="sc">\|</span>{\bf x}(t) - {\bf \pi}<span class="sc">\|</span>_{1} = 2\max_{i} |x_i(t) - \pi_i|$ represents the L1 distance between two probability distributions. The choice of $\epsilon$ is arbitrary.</span>
<span id="cb20-178"><a href="#cb20-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-179"><a href="#cb20-179" aria-hidden="true" tabindex="-1"></a>We know that the distribution of a walker after $t$ steps is given by:</span>
<span id="cb20-180"><a href="#cb20-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-181"><a href="#cb20-181" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-182"><a href="#cb20-182" aria-hidden="true" tabindex="-1"></a>\mathbf{x}(t) =  \mathbf{x}(0) \mathbf{P} ^{t}</span>
<span id="cb20-183"><a href="#cb20-183" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-184"><a href="#cb20-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-185"><a href="#cb20-185" aria-hidden="true" tabindex="-1"></a>To find this distribution, we need to compute $\mathbf{P}^t$. However, we face a challenge: $\mathbf{P}$ is not diagonalizable.</span>
<span id="cb20-186"><a href="#cb20-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-187"><a href="#cb20-187" aria-hidden="true" tabindex="-1"></a>A diagonalizable matrix $\mathbf{S}$ can be written as $\mathbf{S} = \mathbf{Q} \mathbf{\Lambda} \mathbf{Q}^{-1}$, where $\mathbf{\Lambda}$ is a diagonal matrix and $\mathbf{Q}$ is an orthogonal matrix. Visually, it looks like this:</span>
<span id="cb20-188"><a href="#cb20-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-189"><a href="#cb20-189" aria-hidden="true" tabindex="-1"></a><span class="al">![](../figs/diagonalizable.jpg)</span></span>
<span id="cb20-190"><a href="#cb20-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-191"><a href="#cb20-191" aria-hidden="true" tabindex="-1"></a>It is useful because we can then compute the power of the matrix as follows:</span>
<span id="cb20-192"><a href="#cb20-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-193"><a href="#cb20-193" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-194"><a href="#cb20-194" aria-hidden="true" tabindex="-1"></a>\mathbf{S}^t = \mathbf{Q} \mathbf{\Lambda}^t \mathbf{Q}^{-1}</span>
<span id="cb20-195"><a href="#cb20-195" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-196"><a href="#cb20-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-197"><a href="#cb20-197" aria-hidden="true" tabindex="-1"></a>And it is easy to find ${\bf Q}$ and ${\bf \Lambda}$ by using eigenvalue decomposition if ${\bf S}$ is symmetric and consists only of real values. Namely, the eigenvectors form ${\cal Q}$ and the eigenvalues form the diagonal matrix ${\cal \Lambda}$.</span>
<span id="cb20-198"><a href="#cb20-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-201"><a href="#cb20-201" aria-hidden="true" tabindex="-1"></a><span class="in">```{note}</span></span>
<span id="cb20-202"><a href="#cb20-202" aria-hidden="true" tabindex="-1"></a><span class="in">Let us demonstrate the above relation by calculating $\mathbf{S}^2$.</span></span>
<span id="cb20-203"><a href="#cb20-203" aria-hidden="true" tabindex="-1"></a><span class="in">$$</span></span>
<span id="cb20-204"><a href="#cb20-204" aria-hidden="true" tabindex="-1"></a><span class="in">\begin{align}</span></span>
<span id="cb20-205"><a href="#cb20-205" aria-hidden="true" tabindex="-1"></a><span class="in">\mathbf{S}^2 &amp;= \mathbf{Q} \mathbf{\Lambda} \mathbf{Q}^{-1} \mathbf{Q} \mathbf{\Lambda} \mathbf{Q}^{-1} \\</span></span>
<span id="cb20-206"><a href="#cb20-206" aria-hidden="true" tabindex="-1"></a><span class="in">&amp;= \mathbf{Q} \mathbf{\Lambda}^2 \mathbf{Q}^{-1}.</span></span>
<span id="cb20-207"><a href="#cb20-207" aria-hidden="true" tabindex="-1"></a><span class="in">\end{align}</span></span>
<span id="cb20-208"><a href="#cb20-208" aria-hidden="true" tabindex="-1"></a><span class="in">$$</span></span>
<span id="cb20-209"><a href="#cb20-209" aria-hidden="true" tabindex="-1"></a><span class="in">(Note that $\mathbf{Q} \mathbf{Q}^{-1} = {\bf I}$.)</span></span>
<span id="cb20-210"><a href="#cb20-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-211"><a href="#cb20-211" aria-hidden="true" tabindex="-1"></a><span class="in">![](../figs/diagonalizable-squared.jpg)</span></span>
<span id="cb20-212"><a href="#cb20-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-213"><a href="#cb20-213" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-214"><a href="#cb20-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-215"><a href="#cb20-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-216"><a href="#cb20-216" aria-hidden="true" tabindex="-1"></a>$\mathbf{P}$ is also diagonalizable but not symmetric like $\mathbf{\overline A}$ so that we cannot use the above relation directly. So we do a trick by rewriteing $\mathbf{P}$ as:</span>
<span id="cb20-217"><a href="#cb20-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-218"><a href="#cb20-218" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-219"><a href="#cb20-219" aria-hidden="true" tabindex="-1"></a>\mathbf{P} = \mathbf{D}^{-1} \mathbf{A} = \mathbf{D}^{-\frac{1}{2}} \overline {\bf A} \mathbf{D}^{\frac{1}{2}}</span>
<span id="cb20-220"><a href="#cb20-220" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-221"><a href="#cb20-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-222"><a href="#cb20-222" aria-hidden="true" tabindex="-1"></a>where $\overline{\bf A} = \mathbf{D}^{-\frac{1}{2}} \mathbf{A} \mathbf{D}^{-\frac{1}{2}}$ is the normalized adjacency matrix.</span>
<span id="cb20-223"><a href="#cb20-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-224"><a href="#cb20-224" aria-hidden="true" tabindex="-1"></a>The advantage is that $\overline{\bf A}$ is diagonalizable: $\overline{\bf A} = \mathbf{Q} \mathbf{\Lambda} \mathbf{Q}^\top$. Using this, we can compute $\mathbf{P}^t$:</span>
<span id="cb20-225"><a href="#cb20-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-226"><a href="#cb20-226" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-227"><a href="#cb20-227" aria-hidden="true" tabindex="-1"></a>\mathbf{P}^t = \mathbf{D}^{-\frac{1}{2}} \mathbf{Q} \mathbf{\Lambda}^t \mathbf{Q}^\top \mathbf{D}^{\frac{1}{2}} = \mathbf{Q}_L \mathbf{\Lambda}^t \mathbf{Q}_R ^\top</span>
<span id="cb20-228"><a href="#cb20-228" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-229"><a href="#cb20-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-230"><a href="#cb20-230" aria-hidden="true" tabindex="-1"></a>where $\mathbf{Q}_L = \mathbf{D}^{-\frac{1}{2}} \mathbf{Q}$ and $\mathbf{Q}_R = \mathbf{D}^{\frac{1}{2}} \mathbf{Q}$.</span>
<span id="cb20-231"><a href="#cb20-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-232"><a href="#cb20-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-235"><a href="#cb20-235" aria-hidden="true" tabindex="-1"></a><span class="in">```{note}</span></span>
<span id="cb20-236"><a href="#cb20-236" aria-hidden="true" tabindex="-1"></a><span class="in">Let us demonstrate the above relation by calculating $\mathbf{P}^2$.</span></span>
<span id="cb20-237"><a href="#cb20-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-238"><a href="#cb20-238" aria-hidden="true" tabindex="-1"></a><span class="in">$$</span></span>
<span id="cb20-239"><a href="#cb20-239" aria-hidden="true" tabindex="-1"></a><span class="in">\begin{align}</span></span>
<span id="cb20-240"><a href="#cb20-240" aria-hidden="true" tabindex="-1"></a><span class="in">\mathbf{P}^2 &amp;= \mathbf{D}^{-\frac{1}{2}} \overline{\bf A} \mathbf{D}^{\frac{1}{2}} \mathbf{D}^{-\frac{1}{2}} \overline{\bf A} \mathbf{D}^{\frac{1}{2}}\\</span></span>
<span id="cb20-241"><a href="#cb20-241" aria-hidden="true" tabindex="-1"></a><span class="in">&amp;=  \mathbf{D}^{-\frac{1}{2}} \overline{\bf A} ^2 \mathbf{D}^{\frac{1}{2}}\\</span></span>
<span id="cb20-242"><a href="#cb20-242" aria-hidden="true" tabindex="-1"></a><span class="in">&amp;= \mathbf{Q}_L \mathbf{\Lambda}^2 \mathbf{Q}_R ^\top</span></span>
<span id="cb20-243"><a href="#cb20-243" aria-hidden="true" tabindex="-1"></a><span class="in">\end{align}</span></span>
<span id="cb20-244"><a href="#cb20-244" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-245"><a href="#cb20-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-246"><a href="#cb20-246" aria-hidden="true" tabindex="-1"></a>The probability distribution after $t$ steps is then:</span>
<span id="cb20-247"><a href="#cb20-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-248"><a href="#cb20-248" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-249"><a href="#cb20-249" aria-hidden="true" tabindex="-1"></a>\mathbf{x}(t) = \mathbf{x}(0) \mathbf{Q}_L \mathbf{\Lambda}^t \mathbf{Q}_R ^\top</span>
<span id="cb20-250"><a href="#cb20-250" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-251"><a href="#cb20-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-252"><a href="#cb20-252" aria-hidden="true" tabindex="-1"></a>We can rewrite this in a more intuitive form:</span>
<span id="cb20-253"><a href="#cb20-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-254"><a href="#cb20-254" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-255"><a href="#cb20-255" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb20-256"><a href="#cb20-256" aria-hidden="true" tabindex="-1"></a>x_1(t) <span class="sc">\\</span></span>
<span id="cb20-257"><a href="#cb20-257" aria-hidden="true" tabindex="-1"></a>x_2(t) <span class="sc">\\</span></span>
<span id="cb20-258"><a href="#cb20-258" aria-hidden="true" tabindex="-1"></a>\vdots <span class="sc">\\</span></span>
<span id="cb20-259"><a href="#cb20-259" aria-hidden="true" tabindex="-1"></a>x_N(t)</span>
<span id="cb20-260"><a href="#cb20-260" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb20-261"><a href="#cb20-261" aria-hidden="true" tabindex="-1"></a> =</span>
<span id="cb20-262"><a href="#cb20-262" aria-hidden="true" tabindex="-1"></a> \sum_{\ell=1}^N</span>
<span id="cb20-263"><a href="#cb20-263" aria-hidden="true" tabindex="-1"></a> \left[</span>
<span id="cb20-264"><a href="#cb20-264" aria-hidden="true" tabindex="-1"></a> \lambda_\ell^t</span>
<span id="cb20-265"><a href="#cb20-265" aria-hidden="true" tabindex="-1"></a> \begin{pmatrix}</span>
<span id="cb20-266"><a href="#cb20-266" aria-hidden="true" tabindex="-1"></a> q^{(L)}_{\ell 1} <span class="sc">\\</span></span>
<span id="cb20-267"><a href="#cb20-267" aria-hidden="true" tabindex="-1"></a> q^{(L)}_{\ell 2} <span class="sc">\\</span></span>
<span id="cb20-268"><a href="#cb20-268" aria-hidden="true" tabindex="-1"></a> \vdots <span class="sc">\\</span></span>
<span id="cb20-269"><a href="#cb20-269" aria-hidden="true" tabindex="-1"></a> q^{(L)}_{\ell N}</span>
<span id="cb20-270"><a href="#cb20-270" aria-hidden="true" tabindex="-1"></a> \end{pmatrix}</span>
<span id="cb20-271"><a href="#cb20-271" aria-hidden="true" tabindex="-1"></a> \langle\mathbf{q}^{(R)}_{\ell},  \mathbf{x}(0) \rangle</span>
<span id="cb20-272"><a href="#cb20-272" aria-hidden="true" tabindex="-1"></a> \right]</span>
<span id="cb20-273"><a href="#cb20-273" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-274"><a href="#cb20-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-277"><a href="#cb20-277" aria-hidden="true" tabindex="-1"></a><span class="in">```{note}</span></span>
<span id="cb20-278"><a href="#cb20-278" aria-hidden="true" tabindex="-1"></a><span class="in">Visualize the above equation by using the following figure.</span></span>
<span id="cb20-279"><a href="#cb20-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-280"><a href="#cb20-280" aria-hidden="true" tabindex="-1"></a><span class="in">![](../figs/diagonalizable-sum.jpg)</span></span>
<span id="cb20-281"><a href="#cb20-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-282"><a href="#cb20-282" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-283"><a href="#cb20-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-284"><a href="#cb20-284" aria-hidden="true" tabindex="-1"></a>The term $\lambda_\ell^t$ represents the contribution of each eigenvalue to the stationary distribution over time. As $t$ increases, all terms decay exponentially except for the largest eigenvalue ($\lambda_1 = 1$). This explains how the random walk converges to the stationary distribution:</span>
<span id="cb20-285"><a href="#cb20-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-286"><a href="#cb20-286" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-287"><a href="#cb20-287" aria-hidden="true" tabindex="-1"></a>\pi_i = \lim_{t\to\infty} x_i(t) = \begin{pmatrix} q^{(L)}_{1 1} \\ q^{(L)}_{1 2} \\ \vdots \\ q^{(L)}_{1 N} \end{pmatrix} \langle\mathbf{q}^{(R)}_{1},  \mathbf{x}(0) \rangle</span>
<span id="cb20-288"><a href="#cb20-288" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-289"><a href="#cb20-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-290"><a href="#cb20-290" aria-hidden="true" tabindex="-1"></a>The second largest eigenvalue primarily determines the convergence speed to the stationary distribution. A larger second eigenvalue leads to slower convergence. Thus, the mixing time is closely related to the second largest eigenvalue.</span>
<span id="cb20-291"><a href="#cb20-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-292"><a href="#cb20-292" aria-hidden="true" tabindex="-1"></a>Levin-Peres-Wilmer theorem states that the mixing time is bounded by the relaxation time as</span>
<span id="cb20-293"><a href="#cb20-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-294"><a href="#cb20-294" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-295"><a href="#cb20-295" aria-hidden="true" tabindex="-1"></a>t_{\text{mix}} &lt; \tau \log \left( \frac{1}{\epsilon \min_{i} \pi_i} \right), \quad \tau = \frac{1}{1-\lambda_2}</span>
<span id="cb20-296"><a href="#cb20-296" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-297"><a href="#cb20-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-298"><a href="#cb20-298" aria-hidden="true" tabindex="-1"></a>where $\lambda_2$ is the second largest eigenvalue of the normalized adjacency matrix. The mixing time is known to be bounded by the relaxation time as</span>
<span id="cb20-299"><a href="#cb20-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-300"><a href="#cb20-300" aria-hidden="true" tabindex="-1"></a>More commonly, it is expressed using the second smallest eigenvalue $\mu$ of the normalized laplacian matrix as</span>
<span id="cb20-301"><a href="#cb20-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-302"><a href="#cb20-302" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-303"><a href="#cb20-303" aria-hidden="true" tabindex="-1"></a>t_{\text{mix}} \leq \frac{1}{\mu}</span>
<span id="cb20-304"><a href="#cb20-304" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-305"><a href="#cb20-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-306"><a href="#cb20-306" aria-hidden="true" tabindex="-1"></a>where $\mu = 1-\lambda_2$.</span>
<span id="cb20-307"><a href="#cb20-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-308"><a href="#cb20-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-309"><a href="#cb20-309" aria-hidden="true" tabindex="-1"></a><span class="fu">### Compute the mixing time</span></span>
<span id="cb20-310"><a href="#cb20-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-311"><a href="#cb20-311" aria-hidden="true" tabindex="-1"></a>Let us demonstrate the above math by using the network of two cliques.</span>
<span id="cb20-312"><a href="#cb20-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-313"><a href="#cb20-313" aria-hidden="true" tabindex="-1"></a><span class="fu">### Normalized Adjacency Matrix</span></span>
<span id="cb20-314"><a href="#cb20-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-315"><a href="#cb20-315" aria-hidden="true" tabindex="-1"></a>First, let us construct the normalized adjacency matrix $\overline{\bf A}$ of the network.</span>
<span id="cb20-316"><a href="#cb20-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-319"><a href="#cb20-319" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-320"><a href="#cb20-320" aria-hidden="true" tabindex="-1"></a>Dinv_sqrt <span class="op">=</span> sparse.diags(<span class="fl">1.0</span><span class="op">/</span>np.sqrt(deg))</span>
<span id="cb20-321"><a href="#cb20-321" aria-hidden="true" tabindex="-1"></a>A_norm <span class="op">=</span> Dinv_sqrt <span class="op">@</span> A <span class="op">@</span> Dinv_sqrt</span>
<span id="cb20-322"><a href="#cb20-322" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-323"><a href="#cb20-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-324"><a href="#cb20-324" aria-hidden="true" tabindex="-1"></a>Next, let us compute the eigenvalues and eigenvectors of the normalized adjacency matrix.</span>
<span id="cb20-325"><a href="#cb20-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-328"><a href="#cb20-328" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-329"><a href="#cb20-329" aria-hidden="true" tabindex="-1"></a>evals, evecs <span class="op">=</span> np.linalg.eigh(A_norm.toarray())</span>
<span id="cb20-330"><a href="#cb20-330" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-331"><a href="#cb20-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-334"><a href="#cb20-334" aria-hidden="true" tabindex="-1"></a><span class="in">```{note}</span></span>
<span id="cb20-335"><a href="#cb20-335" aria-hidden="true" tabindex="-1"></a><span class="in">`evals` and `evecs` are sorted in descending order of the eigenvalues. `evecs[:, 0]` is the eigenvector corresponding to the largest eigenvalue, which is always 1.</span></span>
<span id="cb20-336"><a href="#cb20-336" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-337"><a href="#cb20-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-340"><a href="#cb20-340" aria-hidden="true" tabindex="-1"></a><span class="in">```{warning}</span></span>
<span id="cb20-341"><a href="#cb20-341" aria-hidden="true" tabindex="-1"></a><span class="in">There is a similar function called `np.linalg.eig` which returns the eigenvalues and eigenvectors. It can be used for any matrices, while `np.linalg.eigh` is specifically for symmetric matrices. `np.linalg.eigh` is faster and more stable and therefore preferred if your matrix is symmetric. `np.linalg.eig` is more susceptible to numerical errors and therefore less stable.</span></span>
<span id="cb20-342"><a href="#cb20-342" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-343"><a href="#cb20-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-344"><a href="#cb20-344" aria-hidden="true" tabindex="-1"></a>The eigenvalues and eigenvectors are shown below.</span>
<span id="cb20-347"><a href="#cb20-347" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-348"><a href="#cb20-348" aria-hidden="true" tabindex="-1"></a>pd.DataFrame({</span>
<span id="cb20-349"><a href="#cb20-349" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Eigenvalue"</span>: evals</span>
<span id="cb20-350"><a href="#cb20-350" aria-hidden="true" tabindex="-1"></a>}).T.style.background_gradient(cmap<span class="op">=</span><span class="st">'cividis'</span>, axis <span class="op">=</span> <span class="dv">1</span>).set_caption(<span class="st">"Eigenvalues of the normalized adjacency matrix"</span>)</span>
<span id="cb20-351"><a href="#cb20-351" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-352"><a href="#cb20-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-355"><a href="#cb20-355" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-356"><a href="#cb20-356" aria-hidden="true" tabindex="-1"></a>pd.DataFrame({</span>
<span id="cb20-357"><a href="#cb20-357" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Eigenvector </span><span class="sc">%i</span><span class="st">"</span> <span class="op">%</span> i: evecs[:, i]</span>
<span id="cb20-358"><a href="#cb20-358" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>)</span>
<span id="cb20-359"><a href="#cb20-359" aria-hidden="true" tabindex="-1"></a>}).style.background_gradient(cmap<span class="op">=</span><span class="st">'cividis'</span>, axis <span class="op">=</span> <span class="va">None</span>).set_caption(<span class="st">"Eigenvectors of the normalized adjacency matrix"</span>)</span>
<span id="cb20-360"><a href="#cb20-360" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-361"><a href="#cb20-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-362"><a href="#cb20-362" aria-hidden="true" tabindex="-1"></a>Notice that the largest eigenvalue is 1, which is always true for a normalized adjacency matrix.</span>
<span id="cb20-363"><a href="#cb20-363" aria-hidden="true" tabindex="-1"></a>The largest eigenvector (the leftmost one) is associated with the stationary distribution of the random walk.</span>
<span id="cb20-364"><a href="#cb20-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-367"><a href="#cb20-367" aria-hidden="true" tabindex="-1"></a><span class="in">```{note}</span></span>
<span id="cb20-368"><a href="#cb20-368" aria-hidden="true" tabindex="-1"></a><span class="in">The sign of the eigenvector is indeterminate, which means we can choose the sign of the eigenvector arbitrarily. In fact, `np.linalg.eigh` returns the eigenvector whose sign can vary for a different run.</span></span>
<span id="cb20-369"><a href="#cb20-369" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-370"><a href="#cb20-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-371"><a href="#cb20-371" aria-hidden="true" tabindex="-1"></a>We decompose $\overline{\bf A}$ as</span>
<span id="cb20-372"><a href="#cb20-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-373"><a href="#cb20-373" aria-hidden="true" tabindex="-1"></a>$$\overline {\bf A} = {\bf Q}{\bf \Lambda}{\bf Q}^\top$$</span>
<span id="cb20-374"><a href="#cb20-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-375"><a href="#cb20-375" aria-hidden="true" tabindex="-1"></a>where ${\bf Q}$ corresponds to <span class="in">`eigvecs`</span> and ${\bf \Lambda}$ corresponds to <span class="in">`np.diag(evals)`</span> (since ${\bf \Lambda}$ is a diagonal matrix). Let's see if this is correct:</span>
<span id="cb20-376"><a href="#cb20-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-379"><a href="#cb20-379" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-380"><a href="#cb20-380" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(A_norm.toarray()).style.background_gradient(cmap<span class="op">=</span><span class="st">'cividis'</span>, axis <span class="op">=</span> <span class="va">None</span>).set_caption(<span class="st">"Normalized Adjacency Matrix"</span>)</span>
<span id="cb20-381"><a href="#cb20-381" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-382"><a href="#cb20-382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-385"><a href="#cb20-385" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-386"><a href="#cb20-386" aria-hidden="true" tabindex="-1"></a>A_norm_reconstructed <span class="op">=</span> evecs <span class="op">@</span> np.diag(evals) <span class="op">@</span> evecs.T</span>
<span id="cb20-387"><a href="#cb20-387" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(A_norm_reconstructed).style.background_gradient(cmap<span class="op">=</span><span class="st">'cividis'</span>, axis <span class="op">=</span> <span class="va">None</span>).set_caption(<span class="st">"Reconstruction of the Normalized Adjacency Matrix"</span>)</span>
<span id="cb20-388"><a href="#cb20-388" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-389"><a href="#cb20-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-390"><a href="#cb20-390" aria-hidden="true" tabindex="-1"></a>Notice that the reconstruction is not perfect due to the numerical error, although overall the structure is correct.</span>
<span id="cb20-391"><a href="#cb20-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-392"><a href="#cb20-392" aria-hidden="true" tabindex="-1"></a><span class="fu">### Multi-step Transition Probability</span></span>
<span id="cb20-393"><a href="#cb20-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-394"><a href="#cb20-394" aria-hidden="true" tabindex="-1"></a>Let us first conform whether we can compute the transition probability after $t$ steps by using the eigenvalues and eigenvectors.</span>
<span id="cb20-395"><a href="#cb20-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-398"><a href="#cb20-398" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-399"><a href="#cb20-399" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb20-400"><a href="#cb20-400" aria-hidden="true" tabindex="-1"></a>x_0 <span class="op">=</span> np.zeros(g.vcount())</span>
<span id="cb20-401"><a href="#cb20-401" aria-hidden="true" tabindex="-1"></a>x_0[<span class="dv">0</span>] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb20-402"><a href="#cb20-402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-403"><a href="#cb20-403" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute x_t by using the eigenvalues and eigenvectors</span></span>
<span id="cb20-404"><a href="#cb20-404" aria-hidden="true" tabindex="-1"></a>Q_L <span class="op">=</span> np.diag(<span class="fl">1.0</span><span class="op">/</span>np.sqrt(deg)) <span class="op">@</span> evecs</span>
<span id="cb20-405"><a href="#cb20-405" aria-hidden="true" tabindex="-1"></a>Q_R <span class="op">=</span> np.diag(np.sqrt(deg)) <span class="op">@</span> evecs</span>
<span id="cb20-406"><a href="#cb20-406" aria-hidden="true" tabindex="-1"></a>x_t <span class="op">=</span> x_0 <span class="op">@</span> Q_L <span class="op">@</span> np.diag(evals<span class="op">**</span>t) <span class="op">@</span> Q_R.T</span>
<span id="cb20-407"><a href="#cb20-407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-408"><a href="#cb20-408" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute x_t by using the power iteration</span></span>
<span id="cb20-409"><a href="#cb20-409" aria-hidden="true" tabindex="-1"></a>x_t_power <span class="op">=</span> x_0.copy()</span>
<span id="cb20-410"><a href="#cb20-410" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(t):</span>
<span id="cb20-411"><a href="#cb20-411" aria-hidden="true" tabindex="-1"></a>    x_t_power <span class="op">=</span> x_t_power <span class="op">@</span> P</span>
<span id="cb20-412"><a href="#cb20-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-413"><a href="#cb20-413" aria-hidden="true" tabindex="-1"></a>pd.DataFrame({</span>
<span id="cb20-414"><a href="#cb20-414" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Eigenvector"</span>: x_t.flatten(),</span>
<span id="cb20-415"><a href="#cb20-415" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Power iteration"</span>: x_t_power.flatten()</span>
<span id="cb20-416"><a href="#cb20-416" aria-hidden="true" tabindex="-1"></a>}).style.background_gradient(cmap<span class="op">=</span><span class="st">'cividis'</span>, axis <span class="op">=</span> <span class="va">None</span>).set_caption(<span class="st">"Comparison of Eigenvector and Power Iteration"</span>)</span>
<span id="cb20-417"><a href="#cb20-417" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-418"><a href="#cb20-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-419"><a href="#cb20-419" aria-hidden="true" tabindex="-1"></a><span class="fu">### Relaxation Time and Mixing Time</span></span>
<span id="cb20-420"><a href="#cb20-420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-421"><a href="#cb20-421" aria-hidden="true" tabindex="-1"></a>Let us measure the relaxation time of the random walk.</span>
<span id="cb20-422"><a href="#cb20-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-425"><a href="#cb20-425" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-426"><a href="#cb20-426" aria-hidden="true" tabindex="-1"></a>evals, evecs <span class="op">=</span> np.linalg.eigh(A_norm.toarray())</span>
<span id="cb20-427"><a href="#cb20-427" aria-hidden="true" tabindex="-1"></a>lambda_2 <span class="op">=</span> <span class="op">-</span>np.sort(<span class="op">-</span>evals)[<span class="dv">1</span>]</span>
<span id="cb20-428"><a href="#cb20-428" aria-hidden="true" tabindex="-1"></a>tau <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> lambda_2</span>
<span id="cb20-429"><a href="#cb20-429" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"The relaxation time of the random walk is </span><span class="sc">{</span>tau<span class="sc">:.4f}</span><span class="ss">."</span>)</span>
<span id="cb20-430"><a href="#cb20-430" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Copyright 2024, Sadamori Kojaku</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions"><ul><li><a href="https://github.com/skojaku/adv-net-sci/edit/main/m07-random-walks/random-walks-math.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/skojaku/adv-net-sci/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/skojaku/adv-net-sci">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>