<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>32&nbsp; Random Walks: Implementation and Applications â€“ Advanced Topics in Network Science</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../m07-random-walks/03-exercises.html" rel="next">
<link href="../m07-random-walks/01-concepts.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-42d6d7f73c49f266427812bee1da63d1.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../m07-random-walks/00-preparation.html">M07: Random Walks</a></li><li class="breadcrumb-item"><a href="../m07-random-walks/02-coding.html"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Random Walks: Implementation and Applications</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../index.html" class="sidebar-logo-link">
      <img src="../logo.jpg" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Advanced Topics in Network Science</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Intro</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../intro/why-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../intro/zoo-of-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Zoo of networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../intro/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Setup</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">M01: Euler Tour</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m01-euler_tour/00-preparation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Preparation: Python and Graph Basics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m01-euler_tour/01-concepts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Euler Tour Concepts</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m01-euler_tour/02-coding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Compute with networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m01-euler_tour/03-exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Exercise</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">M02: Small World</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m02-small-world/00-preparation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Preparation: Distance and Path Analysis Prerequisites</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m02-small-world/01-concepts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Small-World Networks: Core Concepts</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m02-small-world/02-coding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Efficient Network Representation and Computing Paths</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m02-small-world/03-exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Exercises and Assignments</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m02-small-world/04-appendix.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Appendix: Tools and Advanced Topics</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">M03: Robustness</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m03-robustness/00-preparation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Preparation: Network Failure Analysis Prerequisites</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m03-robustness/01-concepts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Module 3: Robustness - Concepts</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m03-robustness/02-coding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Coding: Minimum Spanning Trees and Network Robustness</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m03-robustness/03-exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Robustness Analysis - Exercises</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m03-robustness/04-appendix.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Appendix</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">M04: Friendship Paradox</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-friendship-paradox/00-preparation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Preparation: Statistical Sampling and Probability Prerequisites</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-friendship-paradox/01-concepts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">M04 Concepts: The Friendship Paradox</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-friendship-paradox/02-coding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Degree distribution</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-friendship-paradox/03-exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">M04 Exercises: Friendship Paradox</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">M05: Clustering</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m05-clustering/00-preparation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Preparation: Linear Algebra and Optimization Prerequisites</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m05-clustering/01-concepts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Module 5: Clustering Concepts</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m05-clustering/02-coding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Clustering Algorithms and Implementation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m05-clustering/03-exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Hands-on: Clustering</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">M06: Centrality</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m06-centrality/00-preparation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Preparation: Advanced Linear Algebra for Network Analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m06-centrality/01-concepts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Module 6: Centrality Concepts</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m06-centrality/02-coding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Computing centrality with Python</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m06-centrality/03-exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Assignment</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true">
 <span class="menu-text">M07: Random Walks</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m07-random-walks/00-preparation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Preparation: Markov Chain Theory Prerequisites</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m07-random-walks/01-concepts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Random Walks: Core Concepts</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m07-random-walks/02-coding.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Random Walks: Implementation and Applications</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m07-random-walks/03-exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">Random Walks: Exercises and Practice</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true">
 <span class="menu-text">M08: Embedding</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m08-embedding/00-preparation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">Preparation: Dimensionality Reduction and Optimization Prerequisites</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m08-embedding/01-concepts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">Network Embedding Concepts</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m08-embedding/02-coding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">36</span>&nbsp; <span class="chapter-title">Embedding Methods: Implementation and Practice</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m08-embedding/03-exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">37</span>&nbsp; <span class="chapter-title">Exercises</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m08-embedding/04-appendix.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">38</span>&nbsp; <span class="chapter-title">Appendix</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="true">
 <span class="menu-text">M09: Graph Neural Networks</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m09-graph-neural-networks/00-preparation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">39</span>&nbsp; <span class="chapter-title">Preparation: Neural Networks and Deep Learning Prerequisites</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m09-graph-neural-networks/01-concepts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">40</span>&nbsp; <span class="chapter-title">Concepts: Graph Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m09-graph-neural-networks/02-coding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">41</span>&nbsp; <span class="chapter-title">Coding: Graph Neural Networks Implementation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m09-graph-neural-networks/03-exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">42</span>&nbsp; <span class="chapter-title">Exercises: Graph Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m09-graph-neural-networks/04-appendix.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">43</span>&nbsp; <span class="chapter-title">Appendix</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul class="collapse">
  <li><a href="#introduction-random-walks-everywhere" id="toc-introduction-random-walks-everywhere" class="nav-link active" data-scroll-target="#introduction-random-walks-everywhere"><span class="header-section-number">32.1</span> Introduction: Random Walks Everywhere</a></li>
  <li><a href="#random-walks-in-a-network" id="toc-random-walks-in-a-network" class="nav-link" data-scroll-target="#random-walks-in-a-network"><span class="header-section-number">32.2</span> Random walks in a network</a></li>
  <li><a href="#implementing-random-walks-in-python" id="toc-implementing-random-walks-in-python" class="nav-link" data-scroll-target="#implementing-random-walks-in-python"><span class="header-section-number">32.3</span> Implementing Random Walks in Python</a></li>
  <li><a href="#expected-behavior-of-random-walks" id="toc-expected-behavior-of-random-walks" class="nav-link" data-scroll-target="#expected-behavior-of-random-walks"><span class="header-section-number">32.4</span> Expected behavior of random walks</a></li>
  <li><a href="#mathematical-foundation-stationary-state" id="toc-mathematical-foundation-stationary-state" class="nav-link" data-scroll-target="#mathematical-foundation-stationary-state"><span class="header-section-number">32.5</span> Mathematical Foundation: Stationary State</a></li>
  <li><a href="#practical-demonstration" id="toc-practical-demonstration" class="nav-link" data-scroll-target="#practical-demonstration"><span class="header-section-number">32.6</span> Practical Demonstration</a></li>
  <li><a href="#community-structure" id="toc-community-structure" class="nav-link" data-scroll-target="#community-structure"><span class="header-section-number">32.7</span> Community structure</a></li>
  <li><a href="#advanced-topics-mixing-time-and-spectral-analysis" id="toc-advanced-topics-mixing-time-and-spectral-analysis" class="nav-link" data-scroll-target="#advanced-topics-mixing-time-and-spectral-analysis"><span class="header-section-number">32.8</span> Advanced Topics: Mixing Time and Spectral Analysis</a></li>
  <li><a href="#unifying-centrality-and-communities" id="toc-unifying-centrality-and-communities" class="nav-link" data-scroll-target="#unifying-centrality-and-communities"><span class="header-section-number">32.9</span> Unifying Centrality and Communities</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">32.10</span> Summary</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../m07-random-walks/00-preparation.html">M07: Random Walks</a></li><li class="breadcrumb-item"><a href="../m07-random-walks/02-coding.html"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Random Walks: Implementation and Applications</span></a></li></ol></nav>
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Random Walks: Implementation and Applications</span></h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="introduction-random-walks-everywhere" class="level2" data-number="32.1">
<h2 data-number="32.1" class="anchored" data-anchor-id="introduction-random-walks-everywhere"><span class="header-section-number">32.1</span> Introduction: Random Walks Everywhere</h2>
<p>Suppose you walk in a city. You are drunk and your feet have no idea where to go. You just take a step wherever your feet take you. At every intersection, you make a random decision and take a step. This is the core idea of a random walk.</p>
<p>While your feet are taking you to a random street, after making many steps and looking back, you will realize that you have been to certain places more frequently than others. If you were to map the frequency of your visits to each street, you will end up with a distribution that tells you about salient structure of the street network. It is surprising that this seemingly random, brainless behavior can tell us something deep about the structure of the city.</p>
<p><img src="../figs/random-walk.png" alt="Random walk on a network" width="50%" style="display: block; margin-left: auto; margin-right: auto;"></p>
</section>
<section id="random-walks-in-a-network" class="level2" data-number="32.2">
<h2 data-number="32.2" class="anchored" data-anchor-id="random-walks-in-a-network"><span class="header-section-number">32.2</span> Random walks in a network</h2>
<p>A random walk in undirected networks is the following process: 1. Start at a node <span class="math inline">i</span> 2. Randomly choose an edge to traverse to a neighbor node <span class="math inline">j</span> 3. Repeat step 2 until you have taken <span class="math inline">T</span> steps.</p>
<pre class="{note}"><code>In case of directed networks, a random walker can only move along the edge direction, and it can be that the random walker is stuck in a so-called "dead end" that does not have any outgoing edges.</code></pre>
<p>How does this simple process tell us something about the network structure? To get some insights, let us play with a simple interactive visualization.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Random Walk Simulation">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Random Walk Simulation
</div>
</div>
<div class="callout-body-container callout-body">
<p>:class: tip</p>
<p>Play with the <a href="vis/random-walks/index.html">Random Walk Simulator! ðŸŽ®âœ¨</a> and try to answer the following questions:</p>
<ol type="1">
<li>When the random walker makes many steps, where does it tend to visit most frequently?</li>
<li>When the walker makes only a few steps, where does it tend to visit?</li>
<li>Does the behavior of the walker inform us about centrality of the nodes?</li>
<li>Does the behavior of the walker inform us about communities in the network?</li>
</ol>
</div>
</div>
</section>
<section id="implementing-random-walks-in-python" class="level2" data-number="32.3">
<h2 data-number="32.3" class="anchored" data-anchor-id="implementing-random-walks-in-python"><span class="header-section-number">32.3</span> Implementing Random Walks in Python</h2>
<section id="simulating-random-walks" class="level3" data-number="32.3.1">
<h3 data-number="32.3.1" class="anchored" data-anchor-id="simulating-random-walks"><span class="header-section-number">32.3.1</span> Simulating Random Walks</h3>
<p>We will simulate random walks on a simple graph of five nodes as follows.</p>
<div id="e4c08bce" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> igraph</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> igraph.Graph()</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>g.add_vertices([<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>])</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>g.add_edges([(<span class="dv">0</span>, <span class="dv">1</span>), (<span class="dv">0</span>, <span class="dv">2</span>), (<span class="dv">0</span>, <span class="dv">3</span>), (<span class="dv">1</span>, <span class="dv">3</span>), (<span class="dv">2</span>, <span class="dv">3</span>), (<span class="dv">2</span>, <span class="dv">4</span>), (<span class="dv">3</span>, <span class="dv">4</span>)])</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>igraph.plot(g, vertex_size<span class="op">=</span><span class="dv">20</span>, vertex_label<span class="op">=</span>g.vs[<span class="st">"name"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>A random walk is characterized by the transition probabilities between nodes.</p>
<p><span class="math display">
P_{ij} = \frac{A_{ij}}{k_i}
</span></p>
<p>Let us first compute the transition probabilities and store them in a matrix, <span class="math inline">\mathbf{P}</span>.</p>
<div id="3316f794" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> g.get_adjacency_sparse().toarray()</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> np.array(g.degree())</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>n_nodes <span class="op">=</span> g.vcount()</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"># A simple but inefficient way to compute P</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>P <span class="op">=</span> np.zeros((n_nodes, n_nodes))</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_nodes):</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(n_nodes):</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> k[i] <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>            P[i, j] <span class="op">=</span> A[i, j] <span class="op">/</span> k[i]</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>            P[i, j] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Alternative, more efficient way to compute P</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>P <span class="op">=</span> A <span class="op">/</span> k[:, np.newaxis]</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="co"># or even more efficiently</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>P <span class="op">=</span> np.einsum(<span class="st">"ij,i-&gt;ij"</span>, A, <span class="dv">1</span> <span class="op">/</span> k)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="d4149c5d" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Transition probability matrix:</span><span class="ch">\n</span><span class="st">"</span>, P)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="61bb25e5" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>sns.heatmap(P, annot<span class="op">=</span><span class="va">True</span>, cmap<span class="op">=</span><span class="st">"YlGnBu"</span>)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Each row and column of <span class="math inline">\mathbf{P}</span> corresponds to a node, with entries representing the transition probabilities from the row node to the column node.</p>
<p>Now, let us simulate a random walk on this graph. We represent a position of the walker by a vector, <span class="math inline">\mathbf{x}</span>, with five elements, each of which represents a node. We mark the node that the walker is currently at by <code>1</code> and others as <code>0</code>.</p>
<div id="5c3ed256" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.array([<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>])</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>x[<span class="dv">0</span>] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Initial position of the walker:</span><span class="ch">\n</span><span class="st">"</span>, x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>This vector representation is convenient to get the probabilities of transitions to other nodes from the current node:</p>
<p><span class="math display">
\mathbf{x} \mathbf{P}
</span></p>
<p>which is translated into the following code:</p>
<div id="9620bf75" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> x <span class="op">@</span> P</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Position of the walker after one step:</span><span class="ch">\n</span><span class="st">"</span>, probs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>We can then draw the next node based on the probabilities</p>
<div id="e9bb99e3" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>next_node <span class="op">=</span> np.random.choice(n_nodes, p<span class="op">=</span>probs)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>x[:] <span class="op">=</span> <span class="dv">0</span> <span class="co"># zero out the vector</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>x[next_node] <span class="op">=</span> <span class="dv">1</span> <span class="co"># set the next node to 1</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Position of the walker after one step:</span><span class="ch">\n</span><span class="st">"</span>, x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>By repeating this process, we can simulate the random walk.</p>
</section>
<section id="exercise-01" class="level3" data-number="32.3.2">
<h3 data-number="32.3.2" class="anchored" data-anchor-id="exercise-01"><span class="header-section-number">32.3.2</span> Exercise 01</h3>
<p>Write the following function to simulate the random walk for a given number of steps and return the <span class="math inline">x</span> for each step.</p>
<div id="64cbf4ee" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> random_walk(A, n_steps):</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Simulate the random walk on a graph with adjacency matrix A.</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="co">        A (np.ndarray): The adjacency matrix of the graph.</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="co">        x (np.ndarray): The initial position of the walker.</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="co">        n_steps (int): The number of steps to simulate.</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="co">        np.ndarray: The position of the walker after each step.</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Your code here</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">pass</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
</section>
<section id="expected-behavior-of-random-walks" class="level2" data-number="32.4">
<h2 data-number="32.4" class="anchored" data-anchor-id="expected-behavior-of-random-walks"><span class="header-section-number">32.4</span> Expected behavior of random walks</h2>
<p>What is the expected position of the walker after multiple steps? It is easy to compute the expected position of the walker after one step from initial position <span class="math inline">x(0)</span>:</p>
<p><span class="math display">
\mathbb{E}[x(1)] = x(0) P
</span></p>
<p>where <span class="math inline">x(t)</span> is the probability distribution of the walker at time <span class="math inline">t</span>. In Python, the expected position of the walker at time <span class="math inline">t=1</span> is given by</p>
<div id="358fcdeb" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>x_0 <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>])</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>x_1 <span class="op">=</span> x_0 <span class="op">@</span> P</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Expected position of the walker after one step:</span><span class="ch">\n</span><span class="st">"</span>, x_1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>For the second step, the expected position of the walker is given by</p>
<p><span class="math display">
\mathbb{E}[x(2)] = \mathbb{E}[x(1) P] = \mathbb{E}[x(0) P] P = x(0) P^2
</span></p>
<p>In other words,</p>
<div id="6291e9ae" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>x_2 <span class="op">=</span> x_1 <span class="op">@</span> P</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Expected position of the walker after two steps:</span><span class="ch">\n</span><span class="st">"</span>, x_2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Following the same argument, the expected position of the walker at time <span class="math inline">t</span> is given by</p>
<p><span class="math display">
\mathbb{E}[x(t)] = x(0) P^t
</span></p>
<section id="exercise-02" class="level3" data-number="32.4.1">
<h3 data-number="32.4.1" class="anchored" data-anchor-id="exercise-02"><span class="header-section-number">32.4.1</span> Exercise 02</h3>
<p>Write a function to compute the expected position of the walker at time <span class="math inline">t</span> using the above formula:</p>
<div id="7f913fd3" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> expected_position(A, x_0, t):</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Compute the expected position of the walker at time t.</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="co">        A (np.ndarray): The adjacency matrix of the graph.</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="co">        x_0 (np.ndarray): The initial position of the walker.</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="co">        t (int): The number of steps to simulate.</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Your code here</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">pass</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="exercise-03" class="level3" data-number="32.4.2">
<h3 data-number="32.4.2" class="anchored" data-anchor-id="exercise-03"><span class="header-section-number">32.4.2</span> Exercise 03</h3>
<p>Plot each element of <span class="math inline">x(t)</span> as a function of <span class="math inline">t</span> for <span class="math inline">t=0,1,2,\ldots, 1000</span>. Try different initial positions and compare the results!</p>
<p>Steps: 1. Define the initial position of the walker. 2. Compute the expected position of the walker at time <span class="math inline">t</span> using the function you wrote above. 3. Draw a line for each element of <span class="math inline">x(t)</span>, totalling 5 lines. 4. Create multiple such plots for different initial positions and compare them.</p>
</section>
</section>
<section id="mathematical-foundation-stationary-state" class="level2" data-number="32.5">
<h2 data-number="32.5" class="anchored" data-anchor-id="mathematical-foundation-stationary-state"><span class="header-section-number">32.5</span> Mathematical Foundation: Stationary State</h2>
<p>Letâ€™s dive into the math behind random walks in a way thatâ€™s easy to understand.</p>
<p>Imagine youâ€™re at node <span class="math inline">i</span> at time <span class="math inline">t</span>. You randomly move to a neighboring node <span class="math inline">j</span>. The probability of this move, called the transition probability <span class="math inline">p_{ij}</span>, is:</p>
<p><span class="math display">
p_{ij} = \frac{A_{ij}}{k_i},
</span></p>
<p>Here, <span class="math inline">A_{ij}</span> is an element of the adjacency matrix, and <span class="math inline">k_i</span> is the degree of node <span class="math inline">i</span>. For a network with <span class="math inline">N</span> nodes, we can represent all transition probabilities in a transition probability matrix <span class="math inline">P</span>:</p>
<p><span class="math display">
\mathbf{P} = \begin{pmatrix}
p_{11} &amp; p_{12} &amp; \cdots &amp; p_{1N} \\
p_{21} &amp; p_{22} &amp; \cdots &amp; p_{2N} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
p_{N1} &amp; p_{N2} &amp; \cdots &amp; p_{NN}
\end{pmatrix}
</span></p>
<p>This matrix <span class="math inline">P</span> encapsulates the entire random walk process. We can use it to calculate the probability of visiting each node after any number of steps. For instance:</p>
<ul>
<li>After one step: <span class="math inline">P_{ij} = p_{ij}</span></li>
<li>After two steps: <span class="math inline">\left(\mathbf{P}^{2}\right)_{ij} = \sum_{k} P_{ik} P_{kj}</span></li>
<li>After <span class="math inline">T</span> steps: <span class="math inline">\left(\mathbf{P}^{T}\right)_{ij}</span></li>
</ul>
<pre class="{note}"><code>Let's explore why $\mathbf{P}^2$ represents the transition probabilities after two steps.

First, recall that $\mathbf{P}_{ij}$ is the probability of moving from node $i$ to node $j$ in one step. Now, consider a two-step walk from $i$ to $j$. We can express this as:

$$(\mathbf{P}^2)_{ij} = \sum_k \mathbf{P}_{ik} \mathbf{P}_{kj}$$

This equation encapsulates a key idea: to go from $i$ to $j$ in two steps, we must pass through some intermediate node $k$. Let's break this down step by step:

1. The probability of the first step ($i$ to $k$) is $\mathbf{P}_{ik}$.
2. The probability of the second step ($k$ to $j$) is $\mathbf{P}_{kj}$.
3. The probability of this specific path ($i$ â†’ $k$ â†’ $j$) is the product $\mathbf{P}_{ik} \mathbf{P}_{kj}$.
4. We sum over all possible intermediate nodes $k$ to get the total probability.

Likewise, for three steps, we have:

$$(\mathbf{P}^3)_{ij} = \sum_k \left( \mathbf{P}\right)^2_{ik} \mathbf{P}_{kj}$$

where:
1. The probability of going from $i$ to $k$ in two steps is $\left( \mathbf{P}\right)^2_{ik}$.
2. The probability of going from $k$ to $j$ in one step is $\mathbf{P}_{kj}$.
3. The probability of this specific path ($i$ â†’...â†’$k$ â†’ $j$) is the product $\left( \mathbf{P}\right)^2_{ik} \mathbf{P}_{kj}$.
4. We sum over all possible intermediate nodes $k$ to get the total probability.

And we can extend this reasoning for any number of steps $t$.

In summary, for any number of steps $t$, $\left( \mathbf{P}^t \right)_{ij}$ gives the probability of being at node $j$ after $t$ steps, starting from node $i$.
</code></pre>
<p>As <span class="math inline">T</span> becomes very large, the probability distribution of being at each node, <span class="math inline">\mathbf{x}(t)</span>, approaches a constant value:</p>
<p><span class="math display">
\mathbf{x}(t+1) =\mathbf{x}(t) \mathbf{P}
</span></p>
<p>This is an eigenvector equation. The solution, given by the Perron-Frobenius theorem, is called the stationary distribution:</p>
<p><span class="math display">
\mathbf{x}(\infty) = \mathbb{\pi}, \; \mathbf{\pi} = [\pi_1, \ldots, \pi_N]
</span></p>
<p>For undirected networks, this stationary distribution always exists and is proportional to the degree of each node:</p>
<p><span class="math display">
\pi_j = \frac{k_j}{\sum_{\ell} k_\ell} \propto k_j
</span></p>
<p>This means the probability of being at node <span class="math inline">j</span> in the long run is proportional to the degree of node <span class="math inline">j</span>. The normalization ensures that the sum of all probabilities is 1, i.e., <span class="math inline">\sum_{j=1}^N \pi_j = 1</span>.</p>
</section>
<section id="practical-demonstration" class="level2" data-number="32.6">
<h2 data-number="32.6" class="anchored" data-anchor-id="practical-demonstration"><span class="header-section-number">32.6</span> Practical Demonstration</h2>
<p>Let us demonstrate the above math by using a small network using Python. Let us consider a small network of 5 nodes, which looks like this:</p>
<div id="76ddfe9b" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> igraph <span class="im">as</span> ig</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>edge_list <span class="op">=</span> []</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>):</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(i<span class="op">+</span><span class="dv">1</span>, <span class="dv">5</span>):</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>        edge_list.append((i, j))</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>        edge_list.append((i<span class="op">+</span><span class="dv">5</span>, j<span class="op">+</span><span class="dv">5</span>))</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>edge_list.append((<span class="dv">0</span>, <span class="dv">6</span>))</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> ig.Graph(edge_list)</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>ig.plot(g, vertex_size<span class="op">=</span><span class="dv">20</span>, vertex_label<span class="op">=</span>np.arange(g.vcount()))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The transition probability matrix <span class="math inline">P</span> is given by:</p>
<div id="9ac8d6aa" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy.sparse <span class="im">as</span> sparse</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> g.get_adjacency_sparse()</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>deg <span class="op">=</span> np.array(A.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>)).flatten()</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>Dinv <span class="op">=</span> sparse.diags(<span class="dv">1</span><span class="op">/</span>deg)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>P <span class="op">=</span> Dinv <span class="op">@</span> A</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>P.toarray()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Let us compute the stationary distribution by using the power method.</p>
<div id="0f1a98a7" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.zeros(g.vcount())</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>x[<span class="dv">1</span>] <span class="op">=</span> <span class="dv">1</span> <span class="co"># Start from node 1</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>xt <span class="op">=</span> []</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(T):</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> x.reshape(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>) <span class="op">@</span> P</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>    xt.append(x)</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>xt <span class="op">=</span> np.vstack(xt) <span class="co"># Stack the results vertically</span></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">7</span>,<span class="dv">5</span>))</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>palette <span class="op">=</span> sns.color_palette().as_hex()</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(g.vcount()):</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>    sns.lineplot(x<span class="op">=</span><span class="bu">range</span>(T), y<span class="op">=</span>xt[:, i], label<span class="op">=</span><span class="ss">f"Node </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>, ax<span class="op">=</span>ax, color<span class="op">=</span>palette[i])</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"Time"</span>)</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"Probability"</span>)</span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">"Stationary distribution of a random walk"</span>)</span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>ax.legend(bbox_to_anchor<span class="op">=</span>(<span class="fl">1.05</span>, <span class="dv">1</span>), loc<span class="op">=</span><span class="st">'upper left'</span>)</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>We see that the distributions of the walker converges, and there are three characteristic features in the convergence: 1. The distribution of the walker oscillates with a decaying amplitude and eventually converges. 2. Nodes of the same degree converge to the same stationary probability. 3. Nodes with higher degree converge to the higher stationary probability.</p>
<p>To validate the last two observation, let us compare the stationary distribution of a random walker with the expected stationary distribution, which is proportional to the degree of the nodes.</p>
<div id="0140f42d" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>n_edges <span class="op">=</span> np.<span class="bu">sum</span>(deg) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>expected_stationary_dist <span class="op">=</span> deg <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> n_edges)</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>pd.DataFrame({</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Expected stationary distribution"</span>: expected_stationary_dist,</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Stationary distribution of a random walk"</span>: xt[<span class="op">-</span><span class="dv">1</span>].flatten()</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>}).style.<span class="bu">format</span>(<span class="st">"</span><span class="sc">{:.4f}</span><span class="st">"</span>).set_caption(<span class="st">"Comparison of Expected and Observed Stationary Distributions"</span>).background_gradient(cmap<span class="op">=</span><span class="st">'cividis'</span>, axis <span class="op">=</span> <span class="va">None</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="community-structure" class="level2" data-number="32.7">
<h2 data-number="32.7" class="anchored" data-anchor-id="community-structure"><span class="header-section-number">32.7</span> Community structure</h2>
<p>Random walks can capture community structure of a network. To see this, let us consider a network of a ring of cliques.</p>
<div id="a0359ba3" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> networkx <span class="im">as</span> nx</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> igraph</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>n_cliques <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>n_nodes_per_clique <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>G <span class="op">=</span> nx.ring_of_cliques(n_cliques, n_nodes_per_clique)</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> igraph.Graph().Adjacency(nx.to_numpy_array(G).tolist()).as_undirected()</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>membership <span class="op">=</span> np.repeat(np.arange(n_cliques), n_nodes_per_clique)</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>color_map <span class="op">=</span> [sns.color_palette()[i] <span class="cf">for</span> i <span class="kw">in</span> membership]</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>igraph.plot(g, vertex_size<span class="op">=</span><span class="dv">20</span>, vertex_color<span class="op">=</span>color_map)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Let us compute the expected position of the walker after 1 to 10 steps.</p>
<p><strong>Compute the transition matrix</strong>:</p>
<div id="a598ee97" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>:tags: [hide<span class="op">-</span>cell]</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> sparse</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the adjacency matrix and degree</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> g.get_adjacency_sparse()</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> np.array(g.degree())</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="co"># This is an efficient way to compute the transition matrix</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a><span class="co"># using scipy.sparse</span></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>P <span class="op">=</span> sparse.diags(<span class="dv">1</span> <span class="op">/</span> k) <span class="op">@</span> A</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>Compute the expected position of the walker after 1 to 300 steps</strong>:</p>
<div id="d041bee5" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>:tags: [hide<span class="op">-</span>cell]</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>x_t <span class="op">=</span> np.zeros(g.vcount())</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>x_t[<span class="dv">2</span>] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>x_list <span class="op">=</span> [x_t]</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">300</span>):</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>    x_t <span class="op">=</span> x_t <span class="op">@</span> P</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>    x_list.append(x_t)</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>x_list <span class="op">=</span> np.array(x_list)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>Plot the expected position of the walker at each step</strong>:</p>
<div id="a35e56c7" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>:tags: [hide<span class="op">-</span><span class="bu">input</span>]</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>cmap <span class="op">=</span> sns.color_palette(<span class="st">"viridis"</span>, as_cmap<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>sns.set_style(<span class="st">'white'</span>)</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>sns.<span class="bu">set</span>(font_scale<span class="op">=</span><span class="fl">1.2</span>)</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>sns.set_style(<span class="st">'ticks'</span>)</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">15</span>,<span class="dv">10</span>), ncols <span class="op">=</span> <span class="dv">3</span>, nrows <span class="op">=</span> <span class="dv">2</span>)</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>t_list <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">299</span>]</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, t <span class="kw">in</span> <span class="bu">enumerate</span>(t_list):</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>    igraph.plot(g, vertex_size<span class="op">=</span><span class="dv">20</span>, vertex_color<span class="op">=</span>[cmap(x_list[t][j] <span class="op">/</span> np.<span class="bu">max</span>(x_list[t])) <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(g.vcount())], target <span class="op">=</span> axes[i<span class="op">//</span><span class="dv">3</span>][i<span class="op">%</span><span class="dv">3</span>])</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>    axes[i<span class="op">//</span><span class="dv">3</span>][i<span class="op">%</span><span class="dv">3</span>].set_title(<span class="ss">f"$t$ = </span><span class="sc">{</span>t<span class="sc">}</span><span class="ss">"</span>, fontsize <span class="op">=</span> <span class="dv">25</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>where the color of each node represents the probability of the walker being at that node.</p>
<p>An important observation is that the walker spends more time in the clique that it started from and then diffuse to others. Thus, the position of the walker before reaching the steady state tells us the community structure of the network.</p>
<section id="exercise-04" class="level3" data-number="32.7.1">
<h3 data-number="32.7.1" class="anchored" data-anchor-id="exercise-04"><span class="header-section-number">32.7.1</span> Exercise 04</h3>
<ol type="1">
<li><p>Generate a network of 100 nodes with 4 communities using a stochastic block model, with inter-community edge probability <span class="math inline">0.05</span> and intra-community edge probability <span class="math inline">0.2</span>. Then, compute the expected position of the walker starting from node zero after <span class="math inline">x</span> steps. Plot the results for <span class="math inline">x = 0, 5, 10, 1000</span>.</p></li>
<li><p>Increase the inter-community edge probability to <span class="math inline">0.15</span> and repeat the simulation. Compare the results with the previous simulation.</p></li>
</ol>
</section>
</section>
<section id="advanced-topics-mixing-time-and-spectral-analysis" class="level2" data-number="32.8">
<h2 data-number="32.8" class="anchored" data-anchor-id="advanced-topics-mixing-time-and-spectral-analysis"><span class="header-section-number">32.8</span> Advanced Topics: Mixing Time and Spectral Analysis</h2>
<section id="time-to-reach-the-stationary-state" class="level3" data-number="32.8.1">
<h3 data-number="32.8.1" class="anchored" data-anchor-id="time-to-reach-the-stationary-state"><span class="header-section-number">32.8.1</span> Time to reach the stationary state</h3>
<p>Letâ€™s explore how quickly a random walker reaches its stationary state. The convergence speed is influenced by two main factors: edge density and community structure. In sparse networks, the walker needs more steps to explore the entire network. Additionally, the walker tends to remain within its starting community for some time.</p>
<p>The mixing time, denoted as <span class="math inline">t_{\text{mix}}</span>, is defined as the minimum number of steps required for a random walk to get close to the stationary distribution, regardless of the starting point, with the maximum error less than <span class="math inline">\epsilon</span>:</p>
<p><span class="math display">t_{\text{mix}} = \min\{t : \max_{{\bf x}(0)} \|{\bf x}(t) - {\bf \pi}\|_{1} \leq \epsilon\}</span></p>
<p>where <span class="math inline">\|{\bf x}(t) - {\bf \pi}\|_{1} = 2\max_{i} |x_i(t) - \pi_i|</span> represents the L1 distance between two probability distributions. The choice of <span class="math inline">\epsilon</span> is arbitrary.</p>
<p>We know that the distribution of a walker after <span class="math inline">t</span> steps is given by:</p>
<p><span class="math display">
\mathbf{x}(t) =  \mathbf{x}(0) \mathbf{P} ^{t}
</span></p>
<p>To find this distribution, we need to compute <span class="math inline">\mathbf{P}^t</span>. However, we face a challenge: <span class="math inline">\mathbf{P}</span> is not diagonalizable.</p>
<p>A diagonalizable matrix <span class="math inline">\mathbf{S}</span> can be written as <span class="math inline">\mathbf{S} = \mathbf{Q} \mathbf{\Lambda} \mathbf{Q}^{-1}</span>, where <span class="math inline">\mathbf{\Lambda}</span> is a diagonal matrix and <span class="math inline">\mathbf{Q}</span> is an orthogonal matrix. Visually, it looks like this:</p>
<p><img src="../figs/diagonalizable.jpg" class="img-fluid"></p>
<p>It is useful because we can then compute the power of the matrix as follows:</p>
<p><span class="math display">
\mathbf{S}^t = \mathbf{Q} \mathbf{\Lambda}^t \mathbf{Q}^{-1}
</span></p>
<p>And it is easy to find <span class="math inline">{\bf Q}</span> and <span class="math inline">{\bf \Lambda}</span> by using eigenvalue decomposition if <span class="math inline">{\bf S}</span> is symmetric and consists only of real values. Namely, the eigenvectors form <span class="math inline">{\cal Q}</span> and the eigenvalues form the diagonal matrix <span class="math inline">{\cal \Lambda}</span>.</p>
<pre class="{note}"><code>Let us demonstrate the above relation by calculating $\mathbf{S}^2$.
$$
\begin{align}
\mathbf{S}^2 &amp;= \mathbf{Q} \mathbf{\Lambda} \mathbf{Q}^{-1} \mathbf{Q} \mathbf{\Lambda} \mathbf{Q}^{-1} \\
&amp;= \mathbf{Q} \mathbf{\Lambda}^2 \mathbf{Q}^{-1}.
\end{align}
$$
(Note that $\mathbf{Q} \mathbf{Q}^{-1} = {\bf I}$.)

![](../figs/diagonalizable-squared.jpg)
</code></pre>
<p><span class="math inline">\mathbf{P}</span> is also diagonalizable but not symmetric like <span class="math inline">\mathbf{\overline A}</span> so that we cannot use the above relation directly. So we do a trick by rewriteing <span class="math inline">\mathbf{P}</span> as:</p>
<p><span class="math display">
\mathbf{P} = \mathbf{D}^{-1} \mathbf{A} = \mathbf{D}^{-\frac{1}{2}} \overline {\bf A} \mathbf{D}^{\frac{1}{2}}
</span></p>
<p>where <span class="math inline">\overline{\bf A} = \mathbf{D}^{-\frac{1}{2}} \mathbf{A} \mathbf{D}^{-\frac{1}{2}}</span> is the normalized adjacency matrix.</p>
<p>The advantage is that <span class="math inline">\overline{\bf A}</span> is diagonalizable: <span class="math inline">\overline{\bf A} = \mathbf{Q} \mathbf{\Lambda} \mathbf{Q}^\top</span>. Using this, we can compute <span class="math inline">\mathbf{P}^t</span>:</p>
<p><span class="math display">
\mathbf{P}^t = \mathbf{D}^{-\frac{1}{2}} \mathbf{Q} \mathbf{\Lambda}^t \mathbf{Q}^\top \mathbf{D}^{\frac{1}{2}} = \mathbf{Q}_L \mathbf{\Lambda}^t \mathbf{Q}_R ^\top
</span></p>
<p>where <span class="math inline">\mathbf{Q}_L = \mathbf{D}^{-\frac{1}{2}} \mathbf{Q}</span> and <span class="math inline">\mathbf{Q}_R = \mathbf{D}^{\frac{1}{2}} \mathbf{Q}</span>.</p>
<pre class="{note}"><code>Let us demonstrate the above relation by calculating $\mathbf{P}^2$.

$$
\begin{align}
\mathbf{P}^2 &amp;= \mathbf{D}^{-\frac{1}{2}} \overline{\bf A} \mathbf{D}^{\frac{1}{2}} \mathbf{D}^{-\frac{1}{2}} \overline{\bf A} \mathbf{D}^{\frac{1}{2}}\\
&amp;=  \mathbf{D}^{-\frac{1}{2}} \overline{\bf A} ^2 \mathbf{D}^{\frac{1}{2}}\\
&amp;= \mathbf{Q}_L \mathbf{\Lambda}^2 \mathbf{Q}_R ^\top
\end{align}</code></pre>
<p>The probability distribution after <span class="math inline">t</span> steps is then:</p>
<p><span class="math display">
\mathbf{x}(t) = \mathbf{x}(0) \mathbf{Q}_L \mathbf{\Lambda}^t \mathbf{Q}_R ^\top
</span></p>
<p>We can rewrite this in a more intuitive form:</p>
<p><span class="math display">
\begin{pmatrix}
x_1(t) \\
x_2(t) \\
\vdots \\
x_N(t)
\end{pmatrix}
=
\sum_{\ell=1}^N
\left[
\lambda_\ell^t
\begin{pmatrix}
q^{(L)}_{\ell 1} \\
q^{(L)}_{\ell 2} \\
\vdots \\
q^{(L)}_{\ell N}
\end{pmatrix}
\langle\mathbf{q}^{(R)}_{\ell},  \mathbf{x}(0) \rangle
\right]
</span></p>
<pre class="{note}"><code>Visualize the above equation by using the following figure.

![](../figs/diagonalizable-sum.jpg)
</code></pre>
<p>The term <span class="math inline">\lambda_\ell^t</span> represents the contribution of each eigenvalue to the stationary distribution over time. As <span class="math inline">t</span> increases, all terms decay exponentially except for the largest eigenvalue (<span class="math inline">\lambda_1 = 1</span>). This explains how the random walk converges to the stationary distribution:</p>
<p><span class="math display">
\pi_i = \lim_{t\to\infty} x_i(t) = \begin{pmatrix} q^{(L)}_{1 1} \\ q^{(L)}_{1 2} \\ \vdots \\ q^{(L)}_{1 N} \end{pmatrix} \langle\mathbf{q}^{(R)}_{1},  \mathbf{x}(0) \rangle
</span></p>
<p>The second largest eigenvalue primarily determines the convergence speed to the stationary distribution. A larger second eigenvalue leads to slower convergence. Thus, the mixing time is closely related to the second largest eigenvalue.</p>
<p>Levin-Peres-Wilmer theorem states that the mixing time is bounded by the relaxation time as</p>
<p><span class="math display">
t_{\text{mix}} &lt; \tau \log \left( \frac{1}{\epsilon \min_{i} \pi_i} \right), \quad \tau = \frac{1}{1-\lambda_2}
</span></p>
<p>where <span class="math inline">\lambda_2</span> is the second largest eigenvalue of the normalized adjacency matrix. The mixing time is known to be bounded by the relaxation time as</p>
<p>More commonly, it is expressed using the second smallest eigenvalue <span class="math inline">\mu</span> of the normalized laplacian matrix as</p>
<p><span class="math display">
t_{\text{mix}} \leq \frac{1}{\mu}
</span></p>
<p>where <span class="math inline">\mu = 1-\lambda_2</span>.</p>
</section>
<section id="compute-the-mixing-time" class="level3" data-number="32.8.2">
<h3 data-number="32.8.2" class="anchored" data-anchor-id="compute-the-mixing-time"><span class="header-section-number">32.8.2</span> Compute the mixing time</h3>
<p>Let us demonstrate the above math by using the network of two cliques.</p>
</section>
<section id="normalized-adjacency-matrix" class="level3" data-number="32.8.3">
<h3 data-number="32.8.3" class="anchored" data-anchor-id="normalized-adjacency-matrix"><span class="header-section-number">32.8.3</span> Normalized Adjacency Matrix</h3>
<p>First, let us construct the normalized adjacency matrix <span class="math inline">\overline{\bf A}</span> of the network.</p>
<div id="268617fb" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>Dinv_sqrt <span class="op">=</span> sparse.diags(<span class="fl">1.0</span><span class="op">/</span>np.sqrt(deg))</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>A_norm <span class="op">=</span> Dinv_sqrt <span class="op">@</span> A <span class="op">@</span> Dinv_sqrt</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Next, let us compute the eigenvalues and eigenvectors of the normalized adjacency matrix.</p>
<div id="032ee876" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>evals, evecs <span class="op">=</span> np.linalg.eigh(A_norm.toarray())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<pre class="{note}"><code>`evals` and `evecs` are sorted in descending order of the eigenvalues. `evecs[:, 0]` is the eigenvector corresponding to the largest eigenvalue, which is always 1.</code></pre>
<pre class="{warning}"><code>There is a similar function called `np.linalg.eig` which returns the eigenvalues and eigenvectors. It can be used for any matrices, while `np.linalg.eigh` is specifically for symmetric matrices. `np.linalg.eigh` is faster and more stable and therefore preferred if your matrix is symmetric. `np.linalg.eig` is more susceptible to numerical errors and therefore less stable.</code></pre>
<p>The eigenvalues and eigenvectors are shown below.</p>
<div id="f762a4ec" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>pd.DataFrame({</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Eigenvalue"</span>: evals</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>}).T.style.background_gradient(cmap<span class="op">=</span><span class="st">'cividis'</span>, axis <span class="op">=</span> <span class="dv">1</span>).set_caption(<span class="st">"Eigenvalues of the normalized adjacency matrix"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="a029581d" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>pd.DataFrame({</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Eigenvector </span><span class="sc">%i</span><span class="st">"</span> <span class="op">%</span> i: evecs[:, i]</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>)</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>}).style.background_gradient(cmap<span class="op">=</span><span class="st">'cividis'</span>, axis <span class="op">=</span> <span class="va">None</span>).set_caption(<span class="st">"Eigenvectors of the normalized adjacency matrix"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Notice that the largest eigenvalue is 1, which is always true for a normalized adjacency matrix. The largest eigenvector (the leftmost one) is associated with the stationary distribution of the random walk.</p>
<pre class="{note}"><code>The sign of the eigenvector is indeterminate, which means we can choose the sign of the eigenvector arbitrarily. In fact, `np.linalg.eigh` returns the eigenvector whose sign can vary for a different run.</code></pre>
<p>We decompose <span class="math inline">\overline{\bf A}</span> as</p>
<p><span class="math display">\overline {\bf A} = {\bf Q}{\bf \Lambda}{\bf Q}^\top</span></p>
<p>where <span class="math inline">{\bf Q}</span> corresponds to <code>eigvecs</code> and <span class="math inline">{\bf \Lambda}</span> corresponds to <code>np.diag(evals)</code> (since <span class="math inline">{\bf \Lambda}</span> is a diagonal matrix). Letâ€™s see if this is correct:</p>
<div id="2444d22e" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(A_norm.toarray()).style.background_gradient(cmap<span class="op">=</span><span class="st">'cividis'</span>, axis <span class="op">=</span> <span class="va">None</span>).set_caption(<span class="st">"Normalized Adjacency Matrix"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="8fc7d0e5" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>A_norm_reconstructed <span class="op">=</span> evecs <span class="op">@</span> np.diag(evals) <span class="op">@</span> evecs.T</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(A_norm_reconstructed).style.background_gradient(cmap<span class="op">=</span><span class="st">'cividis'</span>, axis <span class="op">=</span> <span class="va">None</span>).set_caption(<span class="st">"Reconstruction of the Normalized Adjacency Matrix"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Notice that the reconstruction is not perfect due to the numerical error, although overall the structure is correct.</p>
</section>
<section id="multi-step-transition-probability" class="level3" data-number="32.8.4">
<h3 data-number="32.8.4" class="anchored" data-anchor-id="multi-step-transition-probability"><span class="header-section-number">32.8.4</span> Multi-step Transition Probability</h3>
<p>Let us first conform whether we can compute the transition probability after <span class="math inline">t</span> steps by using the eigenvalues and eigenvectors.</p>
<div id="fbe396f2" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>x_0 <span class="op">=</span> np.zeros(g.vcount())</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>x_0[<span class="dv">0</span>] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute x_t by using the eigenvalues and eigenvectors</span></span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>Q_L <span class="op">=</span> np.diag(<span class="fl">1.0</span><span class="op">/</span>np.sqrt(deg)) <span class="op">@</span> evecs</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>Q_R <span class="op">=</span> np.diag(np.sqrt(deg)) <span class="op">@</span> evecs</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>x_t <span class="op">=</span> x_0 <span class="op">@</span> Q_L <span class="op">@</span> np.diag(evals<span class="op">**</span>t) <span class="op">@</span> Q_R.T</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute x_t by using the power iteration</span></span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>x_t_power <span class="op">=</span> x_0.copy()</span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(t):</span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>    x_t_power <span class="op">=</span> x_t_power <span class="op">@</span> P</span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a>pd.DataFrame({</span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Eigenvector"</span>: x_t.flatten(),</span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Power iteration"</span>: x_t_power.flatten()</span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a>}).style.background_gradient(cmap<span class="op">=</span><span class="st">'cividis'</span>, axis <span class="op">=</span> <span class="va">None</span>).set_caption(<span class="st">"Comparison of Eigenvector and Power Iteration"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="relaxation-time-and-mixing-time" class="level3" data-number="32.8.5">
<h3 data-number="32.8.5" class="anchored" data-anchor-id="relaxation-time-and-mixing-time"><span class="header-section-number">32.8.5</span> Relaxation Time and Mixing Time</h3>
<p>Let us measure the relaxation time of the random walk.</p>
<div id="7d0932e1" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>evals, evecs <span class="op">=</span> np.linalg.eigh(A_norm.toarray())</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>lambda_2 <span class="op">=</span> <span class="op">-</span>np.sort(<span class="op">-</span>evals)[<span class="dv">1</span>]</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>tau <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> lambda_2</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"The relaxation time of the random walk is </span><span class="sc">{</span>tau<span class="sc">:.4f}</span><span class="ss">."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
</section>
<section id="unifying-centrality-and-communities" class="level2" data-number="32.9">
<h2 data-number="32.9" class="anchored" data-anchor-id="unifying-centrality-and-communities"><span class="header-section-number">32.9</span> Unifying Centrality and Communities</h2>
<section id="modularity-random-walk-perspective" class="level3" data-number="32.9.1">
<h3 data-number="32.9.1" class="anchored" data-anchor-id="modularity-random-walk-perspective"><span class="header-section-number">32.9.1</span> Modularity: Random Walk Perspective</h3>
<p>Modularity can be interpreted from a random walk perspective. Modularity is given by</p>
<p><span class="math display">
Q = \frac{1}{2m} \sum_{ij} \left( A_{ij} - \frac{d_i d_j}{2m} \right) \delta(c_i, c_j)
</span></p>
<p>where <span class="math inline">m</span> is the number of edges in the network, <span class="math inline">A_{ij}</span> is the adjacency matrix, <span class="math inline">d_i</span> is the degree of node <span class="math inline">i</span>, <span class="math inline">c_i</span> is the community of node <span class="math inline">i</span>, and <span class="math inline">\delta(c_i, c_j)</span> is the Kronecker delta function (which is 1 if <span class="math inline">c_i = c_j</span> and 0 otherwise).</p>
<p>We can rewrite the modularity using the language of random walks as follows.</p>
<p><span class="math display">
\begin{aligned}
Q &amp;= \sum_{ij} \left(\frac{A_{ij}}{2m}  - \frac{d_i}{2m} \frac{d_j}{2m} \right) \delta(c_i, c_j) \\
&amp;= \sum_{ij} \left(\pi_i P_{ij}  - \pi_i \pi_j \right) \delta(c_i, c_j)
\end{aligned}
</span> where <span class="math inline">\pi_i</span> is the stationary distribution of the random walk given by</p>
<p><span class="math display">
\pi_i = \frac{d_i}{2m}
</span></p>
<p>and <span class="math inline">P_{ij}</span> is the transition probability between nodes <span class="math inline">i</span> and <span class="math inline">j</span>.</p>
<pre class="{note}"><code>Let's break down this derivation step by step:

1. We start with the original modularity formula:

   $$Q = \frac{1}{2m} \sum_{ij} \left( A_{ij} - \frac{d_i d_j}{2m} \right) \delta(c_i, c_j)$$

2. First, we move the constant $1/(2m)$ to the inside of the parentheses:

   $$Q = \sum_{ij} \left(\frac{A_{ij}}{2m} - \frac{d_i d_j}{2m^2} \right) \delta(c_i, c_j)$$

3. Now, we recognize that $\frac{A_{ij}}{2m}$ can be rewritten as:

   $$\frac{A_{ij}}{2m} = \frac{d_i}{2m} \cdot \frac{A_{ij}}{d_i} = \pi_i P_{ij}$$

4. We also recognize that $\frac{d_i}{2m}$ is the stationary distribution of the random walk, which we denote as $\pi_i$:

   $$\frac{d_i}{2m} = \pi_i$$

5. Substituting these into our equation:

   $$Q = \sum_{ij} \left(\pi_i P_{ij} - \pi_i \pi_j \right) \delta(c_i, c_j)$$
</code></pre>
<p>The expression suggests that:</p>
<ol type="1">
<li>The first term, <span class="math inline">\pi_i P_{ij} \delta(c_i, c_j)</span>, represents the probability that a walker is at node <span class="math inline">i</span> and moves to node <span class="math inline">j</span> within the same community <strong>by one step</strong>.</li>
<li>The second term, <span class="math inline">\pi_i \pi_j</span>, represents the probability that a walker is at node <span class="math inline">i</span> and moves to another node <span class="math inline">j</span> within the same community <strong>after long steps</strong>.</li>
</ol>
<p>In summary, modularity compares short-term and long-term random walk probabilities. High modularity indicates that a random walker is more likely to stay within the same community after one step than after many steps.</p>
<pre class="{note}"><code>Building on this perspective from random walks, Delvenne et al. {footcite}`delvenne2010stability` extends the modularity by comparing multi-step and long-step transition probabilities of a random walk. This approach, known as "Markov stability", shows that the number of steps acts as a "resolution parameter" that determines the scale of detectable communities.</code></pre>
</section>
<section id="pagerank-random-walk-perspective" class="level3" data-number="32.9.2">
<h3 data-number="32.9.2" class="anchored" data-anchor-id="pagerank-random-walk-perspective"><span class="header-section-number">32.9.2</span> PageRank: Random Walk Perspective</h3>
<p>PageRank can be interpreted from a random walk perspective:</p>
<p><span class="math display">
c_i = (1-\beta) \sum_j P_{ji} c_j + \beta \cdot \frac{1}{N}
</span></p>
<p>Where: - <span class="math inline">c_i</span> is the PageRank of node <span class="math inline">i</span> - <span class="math inline">P_{ji}</span> is the transition probability from node <span class="math inline">j</span> to node <span class="math inline">i</span> - <span class="math inline">\beta</span> is the teleportation probability - <span class="math inline">N</span> is the total number of nodes</p>
<p>This equation represents a random walk where: 1. With probability <span class="math inline">(1-\beta)</span>, the walker follows a link to the next node. 2. With probability <span class="math inline">\beta</span>, the walker <em>teleports</em> to a random node in the network.</p>
<p>The PageRank <span class="math inline">c_i</span> is the stationary distribution of this random walk, representing the long-term probability of finding the walker at node <span class="math inline">i</span>.</p>
<pre class="{note}"><code>This sounds odd at first glance. But it makes sense when you think about what PageRank was invented for, i.e., Web search. It characterizes a web surfer as a random walker that chooses the next page by randomly jumping to a random page with probability $\beta$ or by following a link to a page with probability $1-\beta$. The web page with the largest PageRank means that the page is most likely to be visited by this random web surfer.</code></pre>
</section>
</section>
<section id="summary" class="level2" data-number="32.10">
<h2 data-number="32.10" class="anchored" data-anchor-id="summary"><span class="header-section-number">32.10</span> Summary</h2>
<p>Random walks provide a powerful unified framework for understanding both centrality measures and community structure in networks. Through our exploration, weâ€™ve seen how:</p>
<ol type="1">
<li><strong>Implementation</strong>: Random walks can be efficiently simulated and analyzed using matrix operations</li>
<li><strong>Stationary distributions</strong>: Connect to centrality measures, with degree centrality emerging naturally</li>
<li><strong>Temporal dynamics</strong>: Reveal community structure through short-term vs.&nbsp;long-term behavior</li>
<li><strong>Mathematical foundation</strong>: Links to spectral graph theory and eigenvalue analysis</li>
<li><strong>Applications</strong>: Provide new interpretations of modularity and PageRank</li>
</ol>
<p>This unified perspective opens up new possibilities for network analysis and algorithm design, making random walks one of the most versatile tools in network science.</p>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "î§‹";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../m07-random-walks/01-concepts.html" class="pagination-link" aria-label="Random Walks: Core Concepts">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Random Walks: Core Concepts</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../m07-random-walks/03-exercises.html" class="pagination-link" aria-label="Random Walks: Exercises and Practice">
        <span class="nav-page-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">Random Walks: Exercises and Practice</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb39" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="fu"># Random Walks: Implementation and Applications</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a><span class="fu">## Introduction: Random Walks Everywhere</span></span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>Suppose you walk in a city. You are drunk and your feet have no idea where to go. You just take a step wherever your feet take you. At every intersection, you make a random decision and take a step. This is the core idea of a random walk.</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>While your feet are taking you to a random street, after making many steps and looking back, you will realize that you have been to certain places more frequently than others. If you were to map the frequency of your visits to each street, you will end up with a distribution that tells you about salient structure of the street network. It is surprising that this seemingly random, brainless behavior can tell us something deep about the structure of the city.</span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">img</span><span class="ot"> src</span><span class="op">=</span><span class="st">"../figs/random-walk.png"</span><span class="ot"> alt</span><span class="op">=</span><span class="st">"Random walk on a network"</span><span class="ot"> width</span><span class="op">=</span><span class="st">"50%"</span><span class="ot"> style</span><span class="op">=</span><span class="st">"display: block; margin-left: auto; margin-right: auto;"</span><span class="dt">&gt;</span></span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a><span class="fu">## Random walks in a network</span></span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a>A random walk in undirected networks is the following process:</span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Start at a node $i$</span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Randomly choose an edge to traverse to a neighbor node $j$</span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Repeat step 2 until you have taken $T$ steps.</span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-20"><a href="#cb39-20" aria-hidden="true" tabindex="-1"></a><span class="in">```{note}</span></span>
<span id="cb39-21"><a href="#cb39-21" aria-hidden="true" tabindex="-1"></a><span class="in">In case of directed networks, a random walker can only move along the edge direction, and it can be that the random walker is stuck in a so-called "dead end" that does not have any outgoing edges.</span></span>
<span id="cb39-22"><a href="#cb39-22" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-23"><a href="#cb39-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-24"><a href="#cb39-24" aria-hidden="true" tabindex="-1"></a>How does this simple process tell us something about the network structure? To get some insights, let us play with a simple interactive visualization.</span>
<span id="cb39-25"><a href="#cb39-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-26"><a href="#cb39-26" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="Random Walk Simulation"}</span>
<span id="cb39-27"><a href="#cb39-27" aria-hidden="true" tabindex="-1"></a><span class="sc">:class:</span> tip</span>
<span id="cb39-28"><a href="#cb39-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-29"><a href="#cb39-29" aria-hidden="true" tabindex="-1"></a>Play with the <span class="co">[</span><span class="ot">Random Walk Simulator! ðŸŽ®âœ¨</span><span class="co">](vis/random-walks/index.html)</span> and try to answer the following questions:</span>
<span id="cb39-30"><a href="#cb39-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-31"><a href="#cb39-31" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>When the random walker makes many steps, where does it tend to visit most frequently?</span>
<span id="cb39-32"><a href="#cb39-32" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>When the walker makes only a few steps, where does it tend to visit?</span>
<span id="cb39-33"><a href="#cb39-33" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Does the behavior of the walker inform us about centrality of the nodes?</span>
<span id="cb39-34"><a href="#cb39-34" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Does the behavior of the walker inform us about communities in the network?</span>
<span id="cb39-35"><a href="#cb39-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-36"><a href="#cb39-36" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb39-37"><a href="#cb39-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-38"><a href="#cb39-38" aria-hidden="true" tabindex="-1"></a><span class="fu">## Implementing Random Walks in Python</span></span>
<span id="cb39-39"><a href="#cb39-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-40"><a href="#cb39-40" aria-hidden="true" tabindex="-1"></a><span class="fu">### Simulating Random Walks</span></span>
<span id="cb39-41"><a href="#cb39-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-42"><a href="#cb39-42" aria-hidden="true" tabindex="-1"></a>We will simulate random walks on a simple graph of five nodes as follows.</span>
<span id="cb39-43"><a href="#cb39-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-46"><a href="#cb39-46" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-47"><a href="#cb39-47" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb39-48"><a href="#cb39-48" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> igraph</span>
<span id="cb39-49"><a href="#cb39-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-50"><a href="#cb39-50" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> igraph.Graph()</span>
<span id="cb39-51"><a href="#cb39-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-52"><a href="#cb39-52" aria-hidden="true" tabindex="-1"></a>g.add_vertices([<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>])</span>
<span id="cb39-53"><a href="#cb39-53" aria-hidden="true" tabindex="-1"></a>g.add_edges([(<span class="dv">0</span>, <span class="dv">1</span>), (<span class="dv">0</span>, <span class="dv">2</span>), (<span class="dv">0</span>, <span class="dv">3</span>), (<span class="dv">1</span>, <span class="dv">3</span>), (<span class="dv">2</span>, <span class="dv">3</span>), (<span class="dv">2</span>, <span class="dv">4</span>), (<span class="dv">3</span>, <span class="dv">4</span>)])</span>
<span id="cb39-54"><a href="#cb39-54" aria-hidden="true" tabindex="-1"></a>igraph.plot(g, vertex_size<span class="op">=</span><span class="dv">20</span>, vertex_label<span class="op">=</span>g.vs[<span class="st">"name"</span>])</span>
<span id="cb39-55"><a href="#cb39-55" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-56"><a href="#cb39-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-57"><a href="#cb39-57" aria-hidden="true" tabindex="-1"></a>A random walk is characterized by the transition probabilities between nodes.</span>
<span id="cb39-58"><a href="#cb39-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-59"><a href="#cb39-59" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-60"><a href="#cb39-60" aria-hidden="true" tabindex="-1"></a>P_{ij} = \frac{A_{ij}}{k_i}</span>
<span id="cb39-61"><a href="#cb39-61" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-62"><a href="#cb39-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-63"><a href="#cb39-63" aria-hidden="true" tabindex="-1"></a>Let us first compute the transition probabilities and store them in a matrix, $\mathbf{P}$.</span>
<span id="cb39-64"><a href="#cb39-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-67"><a href="#cb39-67" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-68"><a href="#cb39-68" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> g.get_adjacency_sparse().toarray()</span>
<span id="cb39-69"><a href="#cb39-69" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> np.array(g.degree())</span>
<span id="cb39-70"><a href="#cb39-70" aria-hidden="true" tabindex="-1"></a>n_nodes <span class="op">=</span> g.vcount()</span>
<span id="cb39-71"><a href="#cb39-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-72"><a href="#cb39-72" aria-hidden="true" tabindex="-1"></a><span class="co"># A simple but inefficient way to compute P</span></span>
<span id="cb39-73"><a href="#cb39-73" aria-hidden="true" tabindex="-1"></a>P <span class="op">=</span> np.zeros((n_nodes, n_nodes))</span>
<span id="cb39-74"><a href="#cb39-74" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_nodes):</span>
<span id="cb39-75"><a href="#cb39-75" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(n_nodes):</span>
<span id="cb39-76"><a href="#cb39-76" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> k[i] <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb39-77"><a href="#cb39-77" aria-hidden="true" tabindex="-1"></a>            P[i, j] <span class="op">=</span> A[i, j] <span class="op">/</span> k[i]</span>
<span id="cb39-78"><a href="#cb39-78" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb39-79"><a href="#cb39-79" aria-hidden="true" tabindex="-1"></a>            P[i, j] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb39-80"><a href="#cb39-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-81"><a href="#cb39-81" aria-hidden="true" tabindex="-1"></a><span class="co"># Alternative, more efficient way to compute P</span></span>
<span id="cb39-82"><a href="#cb39-82" aria-hidden="true" tabindex="-1"></a>P <span class="op">=</span> A <span class="op">/</span> k[:, np.newaxis]</span>
<span id="cb39-83"><a href="#cb39-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-84"><a href="#cb39-84" aria-hidden="true" tabindex="-1"></a><span class="co"># or even more efficiently</span></span>
<span id="cb39-85"><a href="#cb39-85" aria-hidden="true" tabindex="-1"></a>P <span class="op">=</span> np.einsum(<span class="st">"ij,i-&gt;ij"</span>, A, <span class="dv">1</span> <span class="op">/</span> k)</span>
<span id="cb39-86"><a href="#cb39-86" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-87"><a href="#cb39-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-90"><a href="#cb39-90" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-91"><a href="#cb39-91" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Transition probability matrix:</span><span class="ch">\n</span><span class="st">"</span>, P)</span>
<span id="cb39-92"><a href="#cb39-92" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-93"><a href="#cb39-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-96"><a href="#cb39-96" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-97"><a href="#cb39-97" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb39-98"><a href="#cb39-98" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb39-99"><a href="#cb39-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-100"><a href="#cb39-100" aria-hidden="true" tabindex="-1"></a>sns.heatmap(P, annot<span class="op">=</span><span class="va">True</span>, cmap<span class="op">=</span><span class="st">"YlGnBu"</span>)</span>
<span id="cb39-101"><a href="#cb39-101" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb39-102"><a href="#cb39-102" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-103"><a href="#cb39-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-104"><a href="#cb39-104" aria-hidden="true" tabindex="-1"></a>Each row and column of $\mathbf{P}$ corresponds to a node, with entries representing the transition probabilities from the row node to the column node.</span>
<span id="cb39-105"><a href="#cb39-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-106"><a href="#cb39-106" aria-hidden="true" tabindex="-1"></a>Now, let us simulate a random walk on this graph. We represent a position of the walker by a vector, $\mathbf{x}$, with five elements, each of which represents a node. We mark the node that the walker is currently at by <span class="in">`1`</span> and others as <span class="in">`0`</span>.</span>
<span id="cb39-107"><a href="#cb39-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-110"><a href="#cb39-110" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-111"><a href="#cb39-111" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.array([<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>])</span>
<span id="cb39-112"><a href="#cb39-112" aria-hidden="true" tabindex="-1"></a>x[<span class="dv">0</span>] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb39-113"><a href="#cb39-113" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Initial position of the walker:</span><span class="ch">\n</span><span class="st">"</span>, x)</span>
<span id="cb39-114"><a href="#cb39-114" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-115"><a href="#cb39-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-116"><a href="#cb39-116" aria-hidden="true" tabindex="-1"></a>This vector representation is convenient to get the probabilities of transitions to other nodes from the current node:</span>
<span id="cb39-117"><a href="#cb39-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-118"><a href="#cb39-118" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-119"><a href="#cb39-119" aria-hidden="true" tabindex="-1"></a>\mathbf{x} \mathbf{P}</span>
<span id="cb39-120"><a href="#cb39-120" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-121"><a href="#cb39-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-122"><a href="#cb39-122" aria-hidden="true" tabindex="-1"></a>which is translated into the following code:</span>
<span id="cb39-123"><a href="#cb39-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-126"><a href="#cb39-126" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-127"><a href="#cb39-127" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> x <span class="op">@</span> P</span>
<span id="cb39-128"><a href="#cb39-128" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Position of the walker after one step:</span><span class="ch">\n</span><span class="st">"</span>, probs)</span>
<span id="cb39-129"><a href="#cb39-129" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-130"><a href="#cb39-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-131"><a href="#cb39-131" aria-hidden="true" tabindex="-1"></a>We can then draw the next node based on the probabilities</span>
<span id="cb39-132"><a href="#cb39-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-135"><a href="#cb39-135" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-136"><a href="#cb39-136" aria-hidden="true" tabindex="-1"></a>next_node <span class="op">=</span> np.random.choice(n_nodes, p<span class="op">=</span>probs)</span>
<span id="cb39-137"><a href="#cb39-137" aria-hidden="true" tabindex="-1"></a>x[:] <span class="op">=</span> <span class="dv">0</span> <span class="co"># zero out the vector</span></span>
<span id="cb39-138"><a href="#cb39-138" aria-hidden="true" tabindex="-1"></a>x[next_node] <span class="op">=</span> <span class="dv">1</span> <span class="co"># set the next node to 1</span></span>
<span id="cb39-139"><a href="#cb39-139" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Position of the walker after one step:</span><span class="ch">\n</span><span class="st">"</span>, x)</span>
<span id="cb39-140"><a href="#cb39-140" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-141"><a href="#cb39-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-142"><a href="#cb39-142" aria-hidden="true" tabindex="-1"></a>By repeating this process, we can simulate the random walk.</span>
<span id="cb39-143"><a href="#cb39-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-144"><a href="#cb39-144" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 01</span></span>
<span id="cb39-145"><a href="#cb39-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-146"><a href="#cb39-146" aria-hidden="true" tabindex="-1"></a>Write the following function to simulate the random walk for a given number of steps and return the $x$ for each step.</span>
<span id="cb39-147"><a href="#cb39-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-150"><a href="#cb39-150" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-151"><a href="#cb39-151" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> random_walk(A, n_steps):</span>
<span id="cb39-152"><a href="#cb39-152" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb39-153"><a href="#cb39-153" aria-hidden="true" tabindex="-1"></a><span class="co">    Simulate the random walk on a graph with adjacency matrix A.</span></span>
<span id="cb39-154"><a href="#cb39-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-155"><a href="#cb39-155" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb39-156"><a href="#cb39-156" aria-hidden="true" tabindex="-1"></a><span class="co">        A (np.ndarray): The adjacency matrix of the graph.</span></span>
<span id="cb39-157"><a href="#cb39-157" aria-hidden="true" tabindex="-1"></a><span class="co">        x (np.ndarray): The initial position of the walker.</span></span>
<span id="cb39-158"><a href="#cb39-158" aria-hidden="true" tabindex="-1"></a><span class="co">        n_steps (int): The number of steps to simulate.</span></span>
<span id="cb39-159"><a href="#cb39-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-160"><a href="#cb39-160" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb39-161"><a href="#cb39-161" aria-hidden="true" tabindex="-1"></a><span class="co">        np.ndarray: The position of the walker after each step.</span></span>
<span id="cb39-162"><a href="#cb39-162" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb39-163"><a href="#cb39-163" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Your code here</span></span>
<span id="cb39-164"><a href="#cb39-164" aria-hidden="true" tabindex="-1"></a>    <span class="cf">pass</span></span>
<span id="cb39-165"><a href="#cb39-165" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-166"><a href="#cb39-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-167"><a href="#cb39-167" aria-hidden="true" tabindex="-1"></a><span class="fu">## Expected behavior of random walks</span></span>
<span id="cb39-168"><a href="#cb39-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-169"><a href="#cb39-169" aria-hidden="true" tabindex="-1"></a>What is the expected position of the walker after multiple steps? It is easy to compute the expected position of the walker after one step from initial position $x(0)$:</span>
<span id="cb39-170"><a href="#cb39-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-171"><a href="#cb39-171" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-172"><a href="#cb39-172" aria-hidden="true" tabindex="-1"></a>\mathbb{E}<span class="co">[</span><span class="ot">x(1)</span><span class="co">]</span> = x(0) P</span>
<span id="cb39-173"><a href="#cb39-173" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-174"><a href="#cb39-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-175"><a href="#cb39-175" aria-hidden="true" tabindex="-1"></a>where $x(t)$ is the probability distribution of the walker at time $t$. In Python, the expected position of the walker at time $t=1$ is given by</span>
<span id="cb39-176"><a href="#cb39-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-179"><a href="#cb39-179" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-180"><a href="#cb39-180" aria-hidden="true" tabindex="-1"></a>x_0 <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>])</span>
<span id="cb39-181"><a href="#cb39-181" aria-hidden="true" tabindex="-1"></a>x_1 <span class="op">=</span> x_0 <span class="op">@</span> P</span>
<span id="cb39-182"><a href="#cb39-182" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Expected position of the walker after one step:</span><span class="ch">\n</span><span class="st">"</span>, x_1)</span>
<span id="cb39-183"><a href="#cb39-183" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-184"><a href="#cb39-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-185"><a href="#cb39-185" aria-hidden="true" tabindex="-1"></a>For the second step, the expected position of the walker is given by</span>
<span id="cb39-186"><a href="#cb39-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-187"><a href="#cb39-187" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-188"><a href="#cb39-188" aria-hidden="true" tabindex="-1"></a>\mathbb{E}<span class="co">[</span><span class="ot">x(2)</span><span class="co">]</span> = \mathbb{E}<span class="co">[</span><span class="ot">x(1) P</span><span class="co">]</span> = \mathbb{E}<span class="co">[</span><span class="ot">x(0) P</span><span class="co">]</span> P = x(0) P^2</span>
<span id="cb39-189"><a href="#cb39-189" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-190"><a href="#cb39-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-191"><a href="#cb39-191" aria-hidden="true" tabindex="-1"></a>In other words,</span>
<span id="cb39-192"><a href="#cb39-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-195"><a href="#cb39-195" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-196"><a href="#cb39-196" aria-hidden="true" tabindex="-1"></a>x_2 <span class="op">=</span> x_1 <span class="op">@</span> P</span>
<span id="cb39-197"><a href="#cb39-197" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Expected position of the walker after two steps:</span><span class="ch">\n</span><span class="st">"</span>, x_2)</span>
<span id="cb39-198"><a href="#cb39-198" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-199"><a href="#cb39-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-200"><a href="#cb39-200" aria-hidden="true" tabindex="-1"></a>Following the same argument, the expected position of the walker at time $t$ is given by</span>
<span id="cb39-201"><a href="#cb39-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-202"><a href="#cb39-202" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-203"><a href="#cb39-203" aria-hidden="true" tabindex="-1"></a>\mathbb{E}<span class="co">[</span><span class="ot">x(t)</span><span class="co">]</span> = x(0) P^t</span>
<span id="cb39-204"><a href="#cb39-204" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-205"><a href="#cb39-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-206"><a href="#cb39-206" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 02</span></span>
<span id="cb39-207"><a href="#cb39-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-208"><a href="#cb39-208" aria-hidden="true" tabindex="-1"></a>Write a function to compute the expected position of the walker at time $t$ using the above formula:</span>
<span id="cb39-209"><a href="#cb39-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-212"><a href="#cb39-212" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-213"><a href="#cb39-213" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> expected_position(A, x_0, t):</span>
<span id="cb39-214"><a href="#cb39-214" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb39-215"><a href="#cb39-215" aria-hidden="true" tabindex="-1"></a><span class="co">    Compute the expected position of the walker at time t.</span></span>
<span id="cb39-216"><a href="#cb39-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-217"><a href="#cb39-217" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb39-218"><a href="#cb39-218" aria-hidden="true" tabindex="-1"></a><span class="co">        A (np.ndarray): The adjacency matrix of the graph.</span></span>
<span id="cb39-219"><a href="#cb39-219" aria-hidden="true" tabindex="-1"></a><span class="co">        x_0 (np.ndarray): The initial position of the walker.</span></span>
<span id="cb39-220"><a href="#cb39-220" aria-hidden="true" tabindex="-1"></a><span class="co">        t (int): The number of steps to simulate.</span></span>
<span id="cb39-221"><a href="#cb39-221" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb39-222"><a href="#cb39-222" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Your code here</span></span>
<span id="cb39-223"><a href="#cb39-223" aria-hidden="true" tabindex="-1"></a>    <span class="cf">pass</span></span>
<span id="cb39-224"><a href="#cb39-224" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-225"><a href="#cb39-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-226"><a href="#cb39-226" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 03</span></span>
<span id="cb39-227"><a href="#cb39-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-228"><a href="#cb39-228" aria-hidden="true" tabindex="-1"></a>Plot each element of $x(t)$ as a function of $t$ for $t=0,1,2,\ldots, 1000$. Try different initial positions and compare the results!</span>
<span id="cb39-229"><a href="#cb39-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-230"><a href="#cb39-230" aria-hidden="true" tabindex="-1"></a>Steps:</span>
<span id="cb39-231"><a href="#cb39-231" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Define the initial position of the walker.</span>
<span id="cb39-232"><a href="#cb39-232" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Compute the expected position of the walker at time $t$ using the function you wrote above.</span>
<span id="cb39-233"><a href="#cb39-233" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Draw a line for each element of $x(t)$, totalling 5 lines.</span>
<span id="cb39-234"><a href="#cb39-234" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Create multiple such plots for different initial positions and compare them.</span>
<span id="cb39-235"><a href="#cb39-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-236"><a href="#cb39-236" aria-hidden="true" tabindex="-1"></a><span class="fu">## Mathematical Foundation: Stationary State</span></span>
<span id="cb39-237"><a href="#cb39-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-238"><a href="#cb39-238" aria-hidden="true" tabindex="-1"></a>Let's dive into the math behind random walks in a way that's easy to understand.</span>
<span id="cb39-239"><a href="#cb39-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-240"><a href="#cb39-240" aria-hidden="true" tabindex="-1"></a>Imagine you're at node $i$ at time $t$. You randomly move to a neighboring node $j$. The probability of this move, called the transition probability $p_{ij}$, is:</span>
<span id="cb39-241"><a href="#cb39-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-242"><a href="#cb39-242" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-243"><a href="#cb39-243" aria-hidden="true" tabindex="-1"></a>p_{ij} = \frac{A_{ij}}{k_i},</span>
<span id="cb39-244"><a href="#cb39-244" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-245"><a href="#cb39-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-246"><a href="#cb39-246" aria-hidden="true" tabindex="-1"></a>Here, $A_{ij}$ is an element of the adjacency matrix, and $k_i$ is the degree of node $i$. For a network with $N$ nodes, we can represent all transition probabilities in a transition probability matrix $P$:</span>
<span id="cb39-247"><a href="#cb39-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-248"><a href="#cb39-248" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-249"><a href="#cb39-249" aria-hidden="true" tabindex="-1"></a>\mathbf{P} = \begin{pmatrix}</span>
<span id="cb39-250"><a href="#cb39-250" aria-hidden="true" tabindex="-1"></a>p_{11} &amp; p_{12} &amp; \cdots &amp; p_{1N} <span class="sc">\\</span></span>
<span id="cb39-251"><a href="#cb39-251" aria-hidden="true" tabindex="-1"></a>p_{21} &amp; p_{22} &amp; \cdots &amp; p_{2N} <span class="sc">\\</span></span>
<span id="cb39-252"><a href="#cb39-252" aria-hidden="true" tabindex="-1"></a>\vdots &amp; \vdots &amp; \ddots &amp; \vdots <span class="sc">\\</span></span>
<span id="cb39-253"><a href="#cb39-253" aria-hidden="true" tabindex="-1"></a>p_{N1} &amp; p_{N2} &amp; \cdots &amp; p_{NN}</span>
<span id="cb39-254"><a href="#cb39-254" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb39-255"><a href="#cb39-255" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-256"><a href="#cb39-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-257"><a href="#cb39-257" aria-hidden="true" tabindex="-1"></a>This matrix $P$ encapsulates the entire random walk process. We can use it to calculate the probability of visiting each node after any number of steps. For instance:</span>
<span id="cb39-258"><a href="#cb39-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-259"><a href="#cb39-259" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>After one step: $P_{ij} = p_{ij}$</span>
<span id="cb39-260"><a href="#cb39-260" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>After two steps: $\left(\mathbf{P}^{2}\right)_{ij} = \sum_{k} P_{ik} P_{kj}$</span>
<span id="cb39-261"><a href="#cb39-261" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>After $T$ steps: $\left(\mathbf{P}^{T}\right)_{ij}$</span>
<span id="cb39-262"><a href="#cb39-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-265"><a href="#cb39-265" aria-hidden="true" tabindex="-1"></a><span class="in">```{note}</span></span>
<span id="cb39-266"><a href="#cb39-266" aria-hidden="true" tabindex="-1"></a><span class="in">Let's explore why $\mathbf{P}^2$ represents the transition probabilities after two steps.</span></span>
<span id="cb39-267"><a href="#cb39-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-268"><a href="#cb39-268" aria-hidden="true" tabindex="-1"></a><span class="in">First, recall that $\mathbf{P}_{ij}$ is the probability of moving from node $i$ to node $j$ in one step. Now, consider a two-step walk from $i$ to $j$. We can express this as:</span></span>
<span id="cb39-269"><a href="#cb39-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-270"><a href="#cb39-270" aria-hidden="true" tabindex="-1"></a><span class="in">$$(\mathbf{P}^2)_{ij} = \sum_k \mathbf{P}_{ik} \mathbf{P}_{kj}$$</span></span>
<span id="cb39-271"><a href="#cb39-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-272"><a href="#cb39-272" aria-hidden="true" tabindex="-1"></a><span class="in">This equation encapsulates a key idea: to go from $i$ to $j$ in two steps, we must pass through some intermediate node $k$. Let's break this down step by step:</span></span>
<span id="cb39-273"><a href="#cb39-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-274"><a href="#cb39-274" aria-hidden="true" tabindex="-1"></a><span class="in">1. The probability of the first step ($i$ to $k$) is $\mathbf{P}_{ik}$.</span></span>
<span id="cb39-275"><a href="#cb39-275" aria-hidden="true" tabindex="-1"></a><span class="in">2. The probability of the second step ($k$ to $j$) is $\mathbf{P}_{kj}$.</span></span>
<span id="cb39-276"><a href="#cb39-276" aria-hidden="true" tabindex="-1"></a><span class="in">3. The probability of this specific path ($i$ â†’ $k$ â†’ $j$) is the product $\mathbf{P}_{ik} \mathbf{P}_{kj}$.</span></span>
<span id="cb39-277"><a href="#cb39-277" aria-hidden="true" tabindex="-1"></a><span class="in">4. We sum over all possible intermediate nodes $k$ to get the total probability.</span></span>
<span id="cb39-278"><a href="#cb39-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-279"><a href="#cb39-279" aria-hidden="true" tabindex="-1"></a><span class="in">Likewise, for three steps, we have:</span></span>
<span id="cb39-280"><a href="#cb39-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-281"><a href="#cb39-281" aria-hidden="true" tabindex="-1"></a><span class="in">$$(\mathbf{P}^3)_{ij} = \sum_k \left( \mathbf{P}\right)^2_{ik} \mathbf{P}_{kj}$$</span></span>
<span id="cb39-282"><a href="#cb39-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-283"><a href="#cb39-283" aria-hidden="true" tabindex="-1"></a><span class="in">where:</span></span>
<span id="cb39-284"><a href="#cb39-284" aria-hidden="true" tabindex="-1"></a><span class="in">1. The probability of going from $i$ to $k$ in two steps is $\left( \mathbf{P}\right)^2_{ik}$.</span></span>
<span id="cb39-285"><a href="#cb39-285" aria-hidden="true" tabindex="-1"></a><span class="in">2. The probability of going from $k$ to $j$ in one step is $\mathbf{P}_{kj}$.</span></span>
<span id="cb39-286"><a href="#cb39-286" aria-hidden="true" tabindex="-1"></a><span class="in">3. The probability of this specific path ($i$ â†’...â†’$k$ â†’ $j$) is the product $\left( \mathbf{P}\right)^2_{ik} \mathbf{P}_{kj}$.</span></span>
<span id="cb39-287"><a href="#cb39-287" aria-hidden="true" tabindex="-1"></a><span class="in">4. We sum over all possible intermediate nodes $k$ to get the total probability.</span></span>
<span id="cb39-288"><a href="#cb39-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-289"><a href="#cb39-289" aria-hidden="true" tabindex="-1"></a><span class="in">And we can extend this reasoning for any number of steps $t$.</span></span>
<span id="cb39-290"><a href="#cb39-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-291"><a href="#cb39-291" aria-hidden="true" tabindex="-1"></a><span class="in">In summary, for any number of steps $t$, $\left( \mathbf{P}^t \right)_{ij}$ gives the probability of being at node $j$ after $t$ steps, starting from node $i$.</span></span>
<span id="cb39-292"><a href="#cb39-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-293"><a href="#cb39-293" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-294"><a href="#cb39-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-295"><a href="#cb39-295" aria-hidden="true" tabindex="-1"></a>As $T$ becomes very large, the probability distribution of being at each node, $\mathbf{x}(t)$, approaches a constant value:</span>
<span id="cb39-296"><a href="#cb39-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-297"><a href="#cb39-297" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-298"><a href="#cb39-298" aria-hidden="true" tabindex="-1"></a>\mathbf{x}(t+1) =\mathbf{x}(t) \mathbf{P}</span>
<span id="cb39-299"><a href="#cb39-299" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-300"><a href="#cb39-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-301"><a href="#cb39-301" aria-hidden="true" tabindex="-1"></a>This is an eigenvector equation. The solution, given by the Perron-Frobenius theorem, is called the stationary distribution:</span>
<span id="cb39-302"><a href="#cb39-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-303"><a href="#cb39-303" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-304"><a href="#cb39-304" aria-hidden="true" tabindex="-1"></a>\mathbf{x}(\infty) = \mathbb{\pi}, \; \mathbf{\pi} = <span class="co">[</span><span class="ot">\pi_1, \ldots, \pi_N</span><span class="co">]</span></span>
<span id="cb39-305"><a href="#cb39-305" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-306"><a href="#cb39-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-307"><a href="#cb39-307" aria-hidden="true" tabindex="-1"></a>For undirected networks, this stationary distribution always exists and is proportional to the degree of each node:</span>
<span id="cb39-308"><a href="#cb39-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-309"><a href="#cb39-309" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-310"><a href="#cb39-310" aria-hidden="true" tabindex="-1"></a>\pi_j = \frac{k_j}{\sum_{\ell} k_\ell} \propto k_j</span>
<span id="cb39-311"><a href="#cb39-311" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-312"><a href="#cb39-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-313"><a href="#cb39-313" aria-hidden="true" tabindex="-1"></a>This means the probability of being at node $j$ in the long run is proportional to the degree of node $j$. The normalization ensures that the sum of all probabilities is 1, i.e., $\sum_{j=1}^N \pi_j = 1$.</span>
<span id="cb39-314"><a href="#cb39-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-315"><a href="#cb39-315" aria-hidden="true" tabindex="-1"></a><span class="fu">## Practical Demonstration</span></span>
<span id="cb39-316"><a href="#cb39-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-317"><a href="#cb39-317" aria-hidden="true" tabindex="-1"></a>Let us demonstrate the above math by using a small network using Python. Let us consider a small network of 5 nodes, which looks like this:</span>
<span id="cb39-318"><a href="#cb39-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-321"><a href="#cb39-321" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-322"><a href="#cb39-322" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> igraph <span class="im">as</span> ig</span>
<span id="cb39-323"><a href="#cb39-323" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb39-324"><a href="#cb39-324" aria-hidden="true" tabindex="-1"></a>edge_list <span class="op">=</span> []</span>
<span id="cb39-325"><a href="#cb39-325" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>):</span>
<span id="cb39-326"><a href="#cb39-326" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(i<span class="op">+</span><span class="dv">1</span>, <span class="dv">5</span>):</span>
<span id="cb39-327"><a href="#cb39-327" aria-hidden="true" tabindex="-1"></a>        edge_list.append((i, j))</span>
<span id="cb39-328"><a href="#cb39-328" aria-hidden="true" tabindex="-1"></a>        edge_list.append((i<span class="op">+</span><span class="dv">5</span>, j<span class="op">+</span><span class="dv">5</span>))</span>
<span id="cb39-329"><a href="#cb39-329" aria-hidden="true" tabindex="-1"></a>edge_list.append((<span class="dv">0</span>, <span class="dv">6</span>))</span>
<span id="cb39-330"><a href="#cb39-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-331"><a href="#cb39-331" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> ig.Graph(edge_list)</span>
<span id="cb39-332"><a href="#cb39-332" aria-hidden="true" tabindex="-1"></a>ig.plot(g, vertex_size<span class="op">=</span><span class="dv">20</span>, vertex_label<span class="op">=</span>np.arange(g.vcount()))</span>
<span id="cb39-333"><a href="#cb39-333" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-334"><a href="#cb39-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-335"><a href="#cb39-335" aria-hidden="true" tabindex="-1"></a>The transition probability matrix $P$ is given by:</span>
<span id="cb39-336"><a href="#cb39-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-339"><a href="#cb39-339" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-340"><a href="#cb39-340" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy.sparse <span class="im">as</span> sparse</span>
<span id="cb39-341"><a href="#cb39-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-342"><a href="#cb39-342" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> g.get_adjacency_sparse()</span>
<span id="cb39-343"><a href="#cb39-343" aria-hidden="true" tabindex="-1"></a>deg <span class="op">=</span> np.array(A.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>)).flatten()</span>
<span id="cb39-344"><a href="#cb39-344" aria-hidden="true" tabindex="-1"></a>Dinv <span class="op">=</span> sparse.diags(<span class="dv">1</span><span class="op">/</span>deg)</span>
<span id="cb39-345"><a href="#cb39-345" aria-hidden="true" tabindex="-1"></a>P <span class="op">=</span> Dinv <span class="op">@</span> A</span>
<span id="cb39-346"><a href="#cb39-346" aria-hidden="true" tabindex="-1"></a>P.toarray()</span>
<span id="cb39-347"><a href="#cb39-347" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-348"><a href="#cb39-348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-349"><a href="#cb39-349" aria-hidden="true" tabindex="-1"></a>Let us compute the stationary distribution by using the power method.</span>
<span id="cb39-350"><a href="#cb39-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-353"><a href="#cb39-353" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-354"><a href="#cb39-354" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb39-355"><a href="#cb39-355" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb39-356"><a href="#cb39-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-357"><a href="#cb39-357" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.zeros(g.vcount())</span>
<span id="cb39-358"><a href="#cb39-358" aria-hidden="true" tabindex="-1"></a>x[<span class="dv">1</span>] <span class="op">=</span> <span class="dv">1</span> <span class="co"># Start from node 1</span></span>
<span id="cb39-359"><a href="#cb39-359" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb39-360"><a href="#cb39-360" aria-hidden="true" tabindex="-1"></a>xt <span class="op">=</span> []</span>
<span id="cb39-361"><a href="#cb39-361" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(T):</span>
<span id="cb39-362"><a href="#cb39-362" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> x.reshape(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>) <span class="op">@</span> P</span>
<span id="cb39-363"><a href="#cb39-363" aria-hidden="true" tabindex="-1"></a>    xt.append(x)</span>
<span id="cb39-364"><a href="#cb39-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-365"><a href="#cb39-365" aria-hidden="true" tabindex="-1"></a>xt <span class="op">=</span> np.vstack(xt) <span class="co"># Stack the results vertically</span></span>
<span id="cb39-366"><a href="#cb39-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-367"><a href="#cb39-367" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">7</span>,<span class="dv">5</span>))</span>
<span id="cb39-368"><a href="#cb39-368" aria-hidden="true" tabindex="-1"></a>palette <span class="op">=</span> sns.color_palette().as_hex()</span>
<span id="cb39-369"><a href="#cb39-369" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(g.vcount()):</span>
<span id="cb39-370"><a href="#cb39-370" aria-hidden="true" tabindex="-1"></a>    sns.lineplot(x<span class="op">=</span><span class="bu">range</span>(T), y<span class="op">=</span>xt[:, i], label<span class="op">=</span><span class="ss">f"Node </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>, ax<span class="op">=</span>ax, color<span class="op">=</span>palette[i])</span>
<span id="cb39-371"><a href="#cb39-371" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"Time"</span>)</span>
<span id="cb39-372"><a href="#cb39-372" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"Probability"</span>)</span>
<span id="cb39-373"><a href="#cb39-373" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">"Stationary distribution of a random walk"</span>)</span>
<span id="cb39-374"><a href="#cb39-374" aria-hidden="true" tabindex="-1"></a>ax.legend(bbox_to_anchor<span class="op">=</span>(<span class="fl">1.05</span>, <span class="dv">1</span>), loc<span class="op">=</span><span class="st">'upper left'</span>)</span>
<span id="cb39-375"><a href="#cb39-375" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb39-376"><a href="#cb39-376" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb39-377"><a href="#cb39-377" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-378"><a href="#cb39-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-379"><a href="#cb39-379" aria-hidden="true" tabindex="-1"></a>We see that the distributions of the walker converges, and there are three characteristic features in the convergence:</span>
<span id="cb39-380"><a href="#cb39-380" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>The distribution of the walker oscillates with a decaying amplitude and eventually converges.</span>
<span id="cb39-381"><a href="#cb39-381" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Nodes of the same degree converge to the same stationary probability.</span>
<span id="cb39-382"><a href="#cb39-382" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Nodes with higher degree converge to the higher stationary probability.</span>
<span id="cb39-383"><a href="#cb39-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-384"><a href="#cb39-384" aria-hidden="true" tabindex="-1"></a>To validate the last two observation, let us compare the stationary distribution of a random walker with the expected stationary distribution, which is proportional to the degree of the nodes.</span>
<span id="cb39-385"><a href="#cb39-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-388"><a href="#cb39-388" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-389"><a href="#cb39-389" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb39-390"><a href="#cb39-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-391"><a href="#cb39-391" aria-hidden="true" tabindex="-1"></a>n_edges <span class="op">=</span> np.<span class="bu">sum</span>(deg) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb39-392"><a href="#cb39-392" aria-hidden="true" tabindex="-1"></a>expected_stationary_dist <span class="op">=</span> deg <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> n_edges)</span>
<span id="cb39-393"><a href="#cb39-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-394"><a href="#cb39-394" aria-hidden="true" tabindex="-1"></a>pd.DataFrame({</span>
<span id="cb39-395"><a href="#cb39-395" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Expected stationary distribution"</span>: expected_stationary_dist,</span>
<span id="cb39-396"><a href="#cb39-396" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Stationary distribution of a random walk"</span>: xt[<span class="op">-</span><span class="dv">1</span>].flatten()</span>
<span id="cb39-397"><a href="#cb39-397" aria-hidden="true" tabindex="-1"></a>}).style.<span class="bu">format</span>(<span class="st">"</span><span class="sc">{:.4f}</span><span class="st">"</span>).set_caption(<span class="st">"Comparison of Expected and Observed Stationary Distributions"</span>).background_gradient(cmap<span class="op">=</span><span class="st">'cividis'</span>, axis <span class="op">=</span> <span class="va">None</span>)</span>
<span id="cb39-398"><a href="#cb39-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-399"><a href="#cb39-399" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-400"><a href="#cb39-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-401"><a href="#cb39-401" aria-hidden="true" tabindex="-1"></a><span class="fu">## Community structure</span></span>
<span id="cb39-402"><a href="#cb39-402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-403"><a href="#cb39-403" aria-hidden="true" tabindex="-1"></a>Random walks can capture community structure of a network.</span>
<span id="cb39-404"><a href="#cb39-404" aria-hidden="true" tabindex="-1"></a>To see this, let us consider a network of a ring of cliques.</span>
<span id="cb39-405"><a href="#cb39-405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-408"><a href="#cb39-408" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-409"><a href="#cb39-409" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> networkx <span class="im">as</span> nx</span>
<span id="cb39-410"><a href="#cb39-410" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> igraph</span>
<span id="cb39-411"><a href="#cb39-411" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb39-412"><a href="#cb39-412" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb39-413"><a href="#cb39-413" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb39-414"><a href="#cb39-414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-415"><a href="#cb39-415" aria-hidden="true" tabindex="-1"></a>n_cliques <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb39-416"><a href="#cb39-416" aria-hidden="true" tabindex="-1"></a>n_nodes_per_clique <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb39-417"><a href="#cb39-417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-418"><a href="#cb39-418" aria-hidden="true" tabindex="-1"></a>G <span class="op">=</span> nx.ring_of_cliques(n_cliques, n_nodes_per_clique)</span>
<span id="cb39-419"><a href="#cb39-419" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> igraph.Graph().Adjacency(nx.to_numpy_array(G).tolist()).as_undirected()</span>
<span id="cb39-420"><a href="#cb39-420" aria-hidden="true" tabindex="-1"></a>membership <span class="op">=</span> np.repeat(np.arange(n_cliques), n_nodes_per_clique)</span>
<span id="cb39-421"><a href="#cb39-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-422"><a href="#cb39-422" aria-hidden="true" tabindex="-1"></a>color_map <span class="op">=</span> [sns.color_palette()[i] <span class="cf">for</span> i <span class="kw">in</span> membership]</span>
<span id="cb39-423"><a href="#cb39-423" aria-hidden="true" tabindex="-1"></a>igraph.plot(g, vertex_size<span class="op">=</span><span class="dv">20</span>, vertex_color<span class="op">=</span>color_map)</span>
<span id="cb39-424"><a href="#cb39-424" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-425"><a href="#cb39-425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-426"><a href="#cb39-426" aria-hidden="true" tabindex="-1"></a>Let us compute the expected position of the walker after 1 to 10 steps.</span>
<span id="cb39-427"><a href="#cb39-427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-428"><a href="#cb39-428" aria-hidden="true" tabindex="-1"></a>**Compute the transition matrix**:</span>
<span id="cb39-429"><a href="#cb39-429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-432"><a href="#cb39-432" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-433"><a href="#cb39-433" aria-hidden="true" tabindex="-1"></a>:tags: [hide<span class="op">-</span>cell]</span>
<span id="cb39-434"><a href="#cb39-434" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> sparse</span>
<span id="cb39-435"><a href="#cb39-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-436"><a href="#cb39-436" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the adjacency matrix and degree</span></span>
<span id="cb39-437"><a href="#cb39-437" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> g.get_adjacency_sparse()</span>
<span id="cb39-438"><a href="#cb39-438" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> np.array(g.degree())</span>
<span id="cb39-439"><a href="#cb39-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-440"><a href="#cb39-440" aria-hidden="true" tabindex="-1"></a><span class="co"># This is an efficient way to compute the transition matrix</span></span>
<span id="cb39-441"><a href="#cb39-441" aria-hidden="true" tabindex="-1"></a><span class="co"># using scipy.sparse</span></span>
<span id="cb39-442"><a href="#cb39-442" aria-hidden="true" tabindex="-1"></a>P <span class="op">=</span> sparse.diags(<span class="dv">1</span> <span class="op">/</span> k) <span class="op">@</span> A</span>
<span id="cb39-443"><a href="#cb39-443" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-444"><a href="#cb39-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-445"><a href="#cb39-445" aria-hidden="true" tabindex="-1"></a>**Compute the expected position of the walker after 1 to 300 steps**:</span>
<span id="cb39-448"><a href="#cb39-448" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-449"><a href="#cb39-449" aria-hidden="true" tabindex="-1"></a>:tags: [hide<span class="op">-</span>cell]</span>
<span id="cb39-450"><a href="#cb39-450" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-451"><a href="#cb39-451" aria-hidden="true" tabindex="-1"></a>x_t <span class="op">=</span> np.zeros(g.vcount())</span>
<span id="cb39-452"><a href="#cb39-452" aria-hidden="true" tabindex="-1"></a>x_t[<span class="dv">2</span>] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb39-453"><a href="#cb39-453" aria-hidden="true" tabindex="-1"></a>x_list <span class="op">=</span> [x_t]</span>
<span id="cb39-454"><a href="#cb39-454" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">300</span>):</span>
<span id="cb39-455"><a href="#cb39-455" aria-hidden="true" tabindex="-1"></a>    x_t <span class="op">=</span> x_t <span class="op">@</span> P</span>
<span id="cb39-456"><a href="#cb39-456" aria-hidden="true" tabindex="-1"></a>    x_list.append(x_t)</span>
<span id="cb39-457"><a href="#cb39-457" aria-hidden="true" tabindex="-1"></a>x_list <span class="op">=</span> np.array(x_list)</span>
<span id="cb39-458"><a href="#cb39-458" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-459"><a href="#cb39-459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-460"><a href="#cb39-460" aria-hidden="true" tabindex="-1"></a>**Plot the expected position of the walker at each step**:</span>
<span id="cb39-461"><a href="#cb39-461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-464"><a href="#cb39-464" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-465"><a href="#cb39-465" aria-hidden="true" tabindex="-1"></a>:tags: [hide<span class="op">-</span><span class="bu">input</span>]</span>
<span id="cb39-466"><a href="#cb39-466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-467"><a href="#cb39-467" aria-hidden="true" tabindex="-1"></a>cmap <span class="op">=</span> sns.color_palette(<span class="st">"viridis"</span>, as_cmap<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb39-468"><a href="#cb39-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-469"><a href="#cb39-469" aria-hidden="true" tabindex="-1"></a>sns.set_style(<span class="st">'white'</span>)</span>
<span id="cb39-470"><a href="#cb39-470" aria-hidden="true" tabindex="-1"></a>sns.<span class="bu">set</span>(font_scale<span class="op">=</span><span class="fl">1.2</span>)</span>
<span id="cb39-471"><a href="#cb39-471" aria-hidden="true" tabindex="-1"></a>sns.set_style(<span class="st">'ticks'</span>)</span>
<span id="cb39-472"><a href="#cb39-472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-473"><a href="#cb39-473" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">15</span>,<span class="dv">10</span>), ncols <span class="op">=</span> <span class="dv">3</span>, nrows <span class="op">=</span> <span class="dv">2</span>)</span>
<span id="cb39-474"><a href="#cb39-474" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-475"><a href="#cb39-475" aria-hidden="true" tabindex="-1"></a>t_list <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">299</span>]</span>
<span id="cb39-476"><a href="#cb39-476" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, t <span class="kw">in</span> <span class="bu">enumerate</span>(t_list):</span>
<span id="cb39-477"><a href="#cb39-477" aria-hidden="true" tabindex="-1"></a>    igraph.plot(g, vertex_size<span class="op">=</span><span class="dv">20</span>, vertex_color<span class="op">=</span>[cmap(x_list[t][j] <span class="op">/</span> np.<span class="bu">max</span>(x_list[t])) <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(g.vcount())], target <span class="op">=</span> axes[i<span class="op">//</span><span class="dv">3</span>][i<span class="op">%</span><span class="dv">3</span>])</span>
<span id="cb39-478"><a href="#cb39-478" aria-hidden="true" tabindex="-1"></a>    axes[i<span class="op">//</span><span class="dv">3</span>][i<span class="op">%</span><span class="dv">3</span>].set_title(<span class="ss">f"$t$ = </span><span class="sc">{</span>t<span class="sc">}</span><span class="ss">"</span>, fontsize <span class="op">=</span> <span class="dv">25</span>)</span>
<span id="cb39-479"><a href="#cb39-479" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-480"><a href="#cb39-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-481"><a href="#cb39-481" aria-hidden="true" tabindex="-1"></a>where the color of each node represents the probability of the walker being at that node.</span>
<span id="cb39-482"><a href="#cb39-482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-483"><a href="#cb39-483" aria-hidden="true" tabindex="-1"></a>An important observation is that the walker spends more time in the clique that it started from and then diffuse to others. Thus, the position of the walker before reaching the steady state tells us the community structure of the network.</span>
<span id="cb39-484"><a href="#cb39-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-485"><a href="#cb39-485" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 04</span></span>
<span id="cb39-486"><a href="#cb39-486" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-487"><a href="#cb39-487" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Generate a network of 100 nodes with 4 communities using a stochastic block model, with inter-community edge probability $0.05$ and intra-community edge probability $0.2$. Then, compute the expected position of the walker starting from node zero after $x$ steps. Plot the results for $x = 0, 5, 10, 1000$.</span>
<span id="cb39-488"><a href="#cb39-488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-489"><a href="#cb39-489" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Increase the inter-community edge probability to $0.15$ and repeat the simulation. Compare the results with the previous simulation.</span>
<span id="cb39-490"><a href="#cb39-490" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-491"><a href="#cb39-491" aria-hidden="true" tabindex="-1"></a><span class="fu">## Advanced Topics: Mixing Time and Spectral Analysis</span></span>
<span id="cb39-492"><a href="#cb39-492" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-493"><a href="#cb39-493" aria-hidden="true" tabindex="-1"></a><span class="fu">### Time to reach the stationary state</span></span>
<span id="cb39-494"><a href="#cb39-494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-495"><a href="#cb39-495" aria-hidden="true" tabindex="-1"></a>Let's explore how quickly a random walker reaches its stationary state. The convergence speed is influenced by two main factors: edge density and community structure. In sparse networks, the walker needs more steps to explore the entire network. Additionally, the walker tends to remain within its starting community for some time.</span>
<span id="cb39-496"><a href="#cb39-496" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-497"><a href="#cb39-497" aria-hidden="true" tabindex="-1"></a>The mixing time, denoted as $t_{\text{mix}}$, is defined as the minimum number of steps required for a random walk to get close to the stationary distribution, regardless of the starting point, with the maximum error less than $\epsilon$:</span>
<span id="cb39-498"><a href="#cb39-498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-499"><a href="#cb39-499" aria-hidden="true" tabindex="-1"></a>$$t_{\text{mix}} = \min<span class="sc">\{</span>t : \max_{{\bf x}(0)} <span class="sc">\|</span>{\bf x}(t) - {\bf \pi}<span class="sc">\|</span>_{1} \leq \epsilon<span class="sc">\}</span>$$</span>
<span id="cb39-500"><a href="#cb39-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-501"><a href="#cb39-501" aria-hidden="true" tabindex="-1"></a>where $<span class="sc">\|</span>{\bf x}(t) - {\bf \pi}<span class="sc">\|</span>_{1} = 2\max_{i} |x_i(t) - \pi_i|$ represents the L1 distance between two probability distributions. The choice of $\epsilon$ is arbitrary.</span>
<span id="cb39-502"><a href="#cb39-502" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-503"><a href="#cb39-503" aria-hidden="true" tabindex="-1"></a>We know that the distribution of a walker after $t$ steps is given by:</span>
<span id="cb39-504"><a href="#cb39-504" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-505"><a href="#cb39-505" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-506"><a href="#cb39-506" aria-hidden="true" tabindex="-1"></a>\mathbf{x}(t) =  \mathbf{x}(0) \mathbf{P} ^{t}</span>
<span id="cb39-507"><a href="#cb39-507" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-508"><a href="#cb39-508" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-509"><a href="#cb39-509" aria-hidden="true" tabindex="-1"></a>To find this distribution, we need to compute $\mathbf{P}^t$. However, we face a challenge: $\mathbf{P}$ is not diagonalizable.</span>
<span id="cb39-510"><a href="#cb39-510" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-511"><a href="#cb39-511" aria-hidden="true" tabindex="-1"></a>A diagonalizable matrix $\mathbf{S}$ can be written as $\mathbf{S} = \mathbf{Q} \mathbf{\Lambda} \mathbf{Q}^{-1}$, where $\mathbf{\Lambda}$ is a diagonal matrix and $\mathbf{Q}$ is an orthogonal matrix. Visually, it looks like this:</span>
<span id="cb39-512"><a href="#cb39-512" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-513"><a href="#cb39-513" aria-hidden="true" tabindex="-1"></a><span class="al">![](../figs/diagonalizable.jpg)</span></span>
<span id="cb39-514"><a href="#cb39-514" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-515"><a href="#cb39-515" aria-hidden="true" tabindex="-1"></a>It is useful because we can then compute the power of the matrix as follows:</span>
<span id="cb39-516"><a href="#cb39-516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-517"><a href="#cb39-517" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-518"><a href="#cb39-518" aria-hidden="true" tabindex="-1"></a>\mathbf{S}^t = \mathbf{Q} \mathbf{\Lambda}^t \mathbf{Q}^{-1}</span>
<span id="cb39-519"><a href="#cb39-519" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-520"><a href="#cb39-520" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-521"><a href="#cb39-521" aria-hidden="true" tabindex="-1"></a>And it is easy to find ${\bf Q}$ and ${\bf \Lambda}$ by using eigenvalue decomposition if ${\bf S}$ is symmetric and consists only of real values. Namely, the eigenvectors form ${\cal Q}$ and the eigenvalues form the diagonal matrix ${\cal \Lambda}$.</span>
<span id="cb39-522"><a href="#cb39-522" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-525"><a href="#cb39-525" aria-hidden="true" tabindex="-1"></a><span class="in">```{note}</span></span>
<span id="cb39-526"><a href="#cb39-526" aria-hidden="true" tabindex="-1"></a><span class="in">Let us demonstrate the above relation by calculating $\mathbf{S}^2$.</span></span>
<span id="cb39-527"><a href="#cb39-527" aria-hidden="true" tabindex="-1"></a><span class="in">$$</span></span>
<span id="cb39-528"><a href="#cb39-528" aria-hidden="true" tabindex="-1"></a><span class="in">\begin{align}</span></span>
<span id="cb39-529"><a href="#cb39-529" aria-hidden="true" tabindex="-1"></a><span class="in">\mathbf{S}^2 &amp;= \mathbf{Q} \mathbf{\Lambda} \mathbf{Q}^{-1} \mathbf{Q} \mathbf{\Lambda} \mathbf{Q}^{-1} \\</span></span>
<span id="cb39-530"><a href="#cb39-530" aria-hidden="true" tabindex="-1"></a><span class="in">&amp;= \mathbf{Q} \mathbf{\Lambda}^2 \mathbf{Q}^{-1}.</span></span>
<span id="cb39-531"><a href="#cb39-531" aria-hidden="true" tabindex="-1"></a><span class="in">\end{align}</span></span>
<span id="cb39-532"><a href="#cb39-532" aria-hidden="true" tabindex="-1"></a><span class="in">$$</span></span>
<span id="cb39-533"><a href="#cb39-533" aria-hidden="true" tabindex="-1"></a><span class="in">(Note that $\mathbf{Q} \mathbf{Q}^{-1} = {\bf I}$.)</span></span>
<span id="cb39-534"><a href="#cb39-534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-535"><a href="#cb39-535" aria-hidden="true" tabindex="-1"></a><span class="in">![](../figs/diagonalizable-squared.jpg)</span></span>
<span id="cb39-536"><a href="#cb39-536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-537"><a href="#cb39-537" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-538"><a href="#cb39-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-539"><a href="#cb39-539" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-540"><a href="#cb39-540" aria-hidden="true" tabindex="-1"></a>$\mathbf{P}$ is also diagonalizable but not symmetric like $\mathbf{\overline A}$ so that we cannot use the above relation directly. So we do a trick by rewriteing $\mathbf{P}$ as:</span>
<span id="cb39-541"><a href="#cb39-541" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-542"><a href="#cb39-542" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-543"><a href="#cb39-543" aria-hidden="true" tabindex="-1"></a>\mathbf{P} = \mathbf{D}^{-1} \mathbf{A} = \mathbf{D}^{-\frac{1}{2}} \overline {\bf A} \mathbf{D}^{\frac{1}{2}}</span>
<span id="cb39-544"><a href="#cb39-544" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-545"><a href="#cb39-545" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-546"><a href="#cb39-546" aria-hidden="true" tabindex="-1"></a>where $\overline{\bf A} = \mathbf{D}^{-\frac{1}{2}} \mathbf{A} \mathbf{D}^{-\frac{1}{2}}$ is the normalized adjacency matrix.</span>
<span id="cb39-547"><a href="#cb39-547" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-548"><a href="#cb39-548" aria-hidden="true" tabindex="-1"></a>The advantage is that $\overline{\bf A}$ is diagonalizable: $\overline{\bf A} = \mathbf{Q} \mathbf{\Lambda} \mathbf{Q}^\top$. Using this, we can compute $\mathbf{P}^t$:</span>
<span id="cb39-549"><a href="#cb39-549" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-550"><a href="#cb39-550" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-551"><a href="#cb39-551" aria-hidden="true" tabindex="-1"></a>\mathbf{P}^t = \mathbf{D}^{-\frac{1}{2}} \mathbf{Q} \mathbf{\Lambda}^t \mathbf{Q}^\top \mathbf{D}^{\frac{1}{2}} = \mathbf{Q}_L \mathbf{\Lambda}^t \mathbf{Q}_R ^\top</span>
<span id="cb39-552"><a href="#cb39-552" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-553"><a href="#cb39-553" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-554"><a href="#cb39-554" aria-hidden="true" tabindex="-1"></a>where $\mathbf{Q}_L = \mathbf{D}^{-\frac{1}{2}} \mathbf{Q}$ and $\mathbf{Q}_R = \mathbf{D}^{\frac{1}{2}} \mathbf{Q}$.</span>
<span id="cb39-555"><a href="#cb39-555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-556"><a href="#cb39-556" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-559"><a href="#cb39-559" aria-hidden="true" tabindex="-1"></a><span class="in">```{note}</span></span>
<span id="cb39-560"><a href="#cb39-560" aria-hidden="true" tabindex="-1"></a><span class="in">Let us demonstrate the above relation by calculating $\mathbf{P}^2$.</span></span>
<span id="cb39-561"><a href="#cb39-561" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-562"><a href="#cb39-562" aria-hidden="true" tabindex="-1"></a><span class="in">$$</span></span>
<span id="cb39-563"><a href="#cb39-563" aria-hidden="true" tabindex="-1"></a><span class="in">\begin{align}</span></span>
<span id="cb39-564"><a href="#cb39-564" aria-hidden="true" tabindex="-1"></a><span class="in">\mathbf{P}^2 &amp;= \mathbf{D}^{-\frac{1}{2}} \overline{\bf A} \mathbf{D}^{\frac{1}{2}} \mathbf{D}^{-\frac{1}{2}} \overline{\bf A} \mathbf{D}^{\frac{1}{2}}\\</span></span>
<span id="cb39-565"><a href="#cb39-565" aria-hidden="true" tabindex="-1"></a><span class="in">&amp;=  \mathbf{D}^{-\frac{1}{2}} \overline{\bf A} ^2 \mathbf{D}^{\frac{1}{2}}\\</span></span>
<span id="cb39-566"><a href="#cb39-566" aria-hidden="true" tabindex="-1"></a><span class="in">&amp;= \mathbf{Q}_L \mathbf{\Lambda}^2 \mathbf{Q}_R ^\top</span></span>
<span id="cb39-567"><a href="#cb39-567" aria-hidden="true" tabindex="-1"></a><span class="in">\end{align}</span></span>
<span id="cb39-568"><a href="#cb39-568" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-569"><a href="#cb39-569" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-570"><a href="#cb39-570" aria-hidden="true" tabindex="-1"></a>The probability distribution after $t$ steps is then:</span>
<span id="cb39-571"><a href="#cb39-571" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-572"><a href="#cb39-572" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-573"><a href="#cb39-573" aria-hidden="true" tabindex="-1"></a>\mathbf{x}(t) = \mathbf{x}(0) \mathbf{Q}_L \mathbf{\Lambda}^t \mathbf{Q}_R ^\top</span>
<span id="cb39-574"><a href="#cb39-574" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-575"><a href="#cb39-575" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-576"><a href="#cb39-576" aria-hidden="true" tabindex="-1"></a>We can rewrite this in a more intuitive form:</span>
<span id="cb39-577"><a href="#cb39-577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-578"><a href="#cb39-578" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-579"><a href="#cb39-579" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb39-580"><a href="#cb39-580" aria-hidden="true" tabindex="-1"></a>x_1(t) <span class="sc">\\</span></span>
<span id="cb39-581"><a href="#cb39-581" aria-hidden="true" tabindex="-1"></a>x_2(t) <span class="sc">\\</span></span>
<span id="cb39-582"><a href="#cb39-582" aria-hidden="true" tabindex="-1"></a>\vdots <span class="sc">\\</span></span>
<span id="cb39-583"><a href="#cb39-583" aria-hidden="true" tabindex="-1"></a>x_N(t)</span>
<span id="cb39-584"><a href="#cb39-584" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb39-585"><a href="#cb39-585" aria-hidden="true" tabindex="-1"></a> =</span>
<span id="cb39-586"><a href="#cb39-586" aria-hidden="true" tabindex="-1"></a> \sum_{\ell=1}^N</span>
<span id="cb39-587"><a href="#cb39-587" aria-hidden="true" tabindex="-1"></a> \left[</span>
<span id="cb39-588"><a href="#cb39-588" aria-hidden="true" tabindex="-1"></a> \lambda_\ell^t</span>
<span id="cb39-589"><a href="#cb39-589" aria-hidden="true" tabindex="-1"></a> \begin{pmatrix}</span>
<span id="cb39-590"><a href="#cb39-590" aria-hidden="true" tabindex="-1"></a> q^{(L)}_{\ell 1} <span class="sc">\\</span></span>
<span id="cb39-591"><a href="#cb39-591" aria-hidden="true" tabindex="-1"></a> q^{(L)}_{\ell 2} <span class="sc">\\</span></span>
<span id="cb39-592"><a href="#cb39-592" aria-hidden="true" tabindex="-1"></a> \vdots <span class="sc">\\</span></span>
<span id="cb39-593"><a href="#cb39-593" aria-hidden="true" tabindex="-1"></a> q^{(L)}_{\ell N}</span>
<span id="cb39-594"><a href="#cb39-594" aria-hidden="true" tabindex="-1"></a> \end{pmatrix}</span>
<span id="cb39-595"><a href="#cb39-595" aria-hidden="true" tabindex="-1"></a> \langle\mathbf{q}^{(R)}_{\ell},  \mathbf{x}(0) \rangle</span>
<span id="cb39-596"><a href="#cb39-596" aria-hidden="true" tabindex="-1"></a> \right]</span>
<span id="cb39-597"><a href="#cb39-597" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-598"><a href="#cb39-598" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-601"><a href="#cb39-601" aria-hidden="true" tabindex="-1"></a><span class="in">```{note}</span></span>
<span id="cb39-602"><a href="#cb39-602" aria-hidden="true" tabindex="-1"></a><span class="in">Visualize the above equation by using the following figure.</span></span>
<span id="cb39-603"><a href="#cb39-603" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-604"><a href="#cb39-604" aria-hidden="true" tabindex="-1"></a><span class="in">![](../figs/diagonalizable-sum.jpg)</span></span>
<span id="cb39-605"><a href="#cb39-605" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-606"><a href="#cb39-606" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-607"><a href="#cb39-607" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-608"><a href="#cb39-608" aria-hidden="true" tabindex="-1"></a>The term $\lambda_\ell^t$ represents the contribution of each eigenvalue to the stationary distribution over time. As $t$ increases, all terms decay exponentially except for the largest eigenvalue ($\lambda_1 = 1$). This explains how the random walk converges to the stationary distribution:</span>
<span id="cb39-609"><a href="#cb39-609" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-610"><a href="#cb39-610" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-611"><a href="#cb39-611" aria-hidden="true" tabindex="-1"></a>\pi_i = \lim_{t\to\infty} x_i(t) = \begin{pmatrix} q^{(L)}_{1 1} \\ q^{(L)}_{1 2} \\ \vdots \\ q^{(L)}_{1 N} \end{pmatrix} \langle\mathbf{q}^{(R)}_{1},  \mathbf{x}(0) \rangle</span>
<span id="cb39-612"><a href="#cb39-612" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-613"><a href="#cb39-613" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-614"><a href="#cb39-614" aria-hidden="true" tabindex="-1"></a>The second largest eigenvalue primarily determines the convergence speed to the stationary distribution. A larger second eigenvalue leads to slower convergence. Thus, the mixing time is closely related to the second largest eigenvalue.</span>
<span id="cb39-615"><a href="#cb39-615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-616"><a href="#cb39-616" aria-hidden="true" tabindex="-1"></a>Levin-Peres-Wilmer theorem states that the mixing time is bounded by the relaxation time as</span>
<span id="cb39-617"><a href="#cb39-617" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-618"><a href="#cb39-618" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-619"><a href="#cb39-619" aria-hidden="true" tabindex="-1"></a>t_{\text{mix}} &lt; \tau \log \left( \frac{1}{\epsilon \min_{i} \pi_i} \right), \quad \tau = \frac{1}{1-\lambda_2}</span>
<span id="cb39-620"><a href="#cb39-620" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-621"><a href="#cb39-621" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-622"><a href="#cb39-622" aria-hidden="true" tabindex="-1"></a>where $\lambda_2$ is the second largest eigenvalue of the normalized adjacency matrix. The mixing time is known to be bounded by the relaxation time as</span>
<span id="cb39-623"><a href="#cb39-623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-624"><a href="#cb39-624" aria-hidden="true" tabindex="-1"></a>More commonly, it is expressed using the second smallest eigenvalue $\mu$ of the normalized laplacian matrix as</span>
<span id="cb39-625"><a href="#cb39-625" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-626"><a href="#cb39-626" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-627"><a href="#cb39-627" aria-hidden="true" tabindex="-1"></a>t_{\text{mix}} \leq \frac{1}{\mu}</span>
<span id="cb39-628"><a href="#cb39-628" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-629"><a href="#cb39-629" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-630"><a href="#cb39-630" aria-hidden="true" tabindex="-1"></a>where $\mu = 1-\lambda_2$.</span>
<span id="cb39-631"><a href="#cb39-631" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-632"><a href="#cb39-632" aria-hidden="true" tabindex="-1"></a><span class="fu">### Compute the mixing time</span></span>
<span id="cb39-633"><a href="#cb39-633" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-634"><a href="#cb39-634" aria-hidden="true" tabindex="-1"></a>Let us demonstrate the above math by using the network of two cliques.</span>
<span id="cb39-635"><a href="#cb39-635" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-636"><a href="#cb39-636" aria-hidden="true" tabindex="-1"></a><span class="fu">### Normalized Adjacency Matrix</span></span>
<span id="cb39-637"><a href="#cb39-637" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-638"><a href="#cb39-638" aria-hidden="true" tabindex="-1"></a>First, let us construct the normalized adjacency matrix $\overline{\bf A}$ of the network.</span>
<span id="cb39-639"><a href="#cb39-639" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-642"><a href="#cb39-642" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-643"><a href="#cb39-643" aria-hidden="true" tabindex="-1"></a>Dinv_sqrt <span class="op">=</span> sparse.diags(<span class="fl">1.0</span><span class="op">/</span>np.sqrt(deg))</span>
<span id="cb39-644"><a href="#cb39-644" aria-hidden="true" tabindex="-1"></a>A_norm <span class="op">=</span> Dinv_sqrt <span class="op">@</span> A <span class="op">@</span> Dinv_sqrt</span>
<span id="cb39-645"><a href="#cb39-645" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-646"><a href="#cb39-646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-647"><a href="#cb39-647" aria-hidden="true" tabindex="-1"></a>Next, let us compute the eigenvalues and eigenvectors of the normalized adjacency matrix.</span>
<span id="cb39-648"><a href="#cb39-648" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-651"><a href="#cb39-651" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-652"><a href="#cb39-652" aria-hidden="true" tabindex="-1"></a>evals, evecs <span class="op">=</span> np.linalg.eigh(A_norm.toarray())</span>
<span id="cb39-653"><a href="#cb39-653" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-654"><a href="#cb39-654" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-657"><a href="#cb39-657" aria-hidden="true" tabindex="-1"></a><span class="in">```{note}</span></span>
<span id="cb39-658"><a href="#cb39-658" aria-hidden="true" tabindex="-1"></a><span class="in">`evals` and `evecs` are sorted in descending order of the eigenvalues. `evecs[:, 0]` is the eigenvector corresponding to the largest eigenvalue, which is always 1.</span></span>
<span id="cb39-659"><a href="#cb39-659" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-660"><a href="#cb39-660" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-663"><a href="#cb39-663" aria-hidden="true" tabindex="-1"></a><span class="in">```{warning}</span></span>
<span id="cb39-664"><a href="#cb39-664" aria-hidden="true" tabindex="-1"></a><span class="in">There is a similar function called `np.linalg.eig` which returns the eigenvalues and eigenvectors. It can be used for any matrices, while `np.linalg.eigh` is specifically for symmetric matrices. `np.linalg.eigh` is faster and more stable and therefore preferred if your matrix is symmetric. `np.linalg.eig` is more susceptible to numerical errors and therefore less stable.</span></span>
<span id="cb39-665"><a href="#cb39-665" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-666"><a href="#cb39-666" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-667"><a href="#cb39-667" aria-hidden="true" tabindex="-1"></a>The eigenvalues and eigenvectors are shown below.</span>
<span id="cb39-670"><a href="#cb39-670" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-671"><a href="#cb39-671" aria-hidden="true" tabindex="-1"></a>pd.DataFrame({</span>
<span id="cb39-672"><a href="#cb39-672" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Eigenvalue"</span>: evals</span>
<span id="cb39-673"><a href="#cb39-673" aria-hidden="true" tabindex="-1"></a>}).T.style.background_gradient(cmap<span class="op">=</span><span class="st">'cividis'</span>, axis <span class="op">=</span> <span class="dv">1</span>).set_caption(<span class="st">"Eigenvalues of the normalized adjacency matrix"</span>)</span>
<span id="cb39-674"><a href="#cb39-674" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-675"><a href="#cb39-675" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-678"><a href="#cb39-678" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-679"><a href="#cb39-679" aria-hidden="true" tabindex="-1"></a>pd.DataFrame({</span>
<span id="cb39-680"><a href="#cb39-680" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Eigenvector </span><span class="sc">%i</span><span class="st">"</span> <span class="op">%</span> i: evecs[:, i]</span>
<span id="cb39-681"><a href="#cb39-681" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>)</span>
<span id="cb39-682"><a href="#cb39-682" aria-hidden="true" tabindex="-1"></a>}).style.background_gradient(cmap<span class="op">=</span><span class="st">'cividis'</span>, axis <span class="op">=</span> <span class="va">None</span>).set_caption(<span class="st">"Eigenvectors of the normalized adjacency matrix"</span>)</span>
<span id="cb39-683"><a href="#cb39-683" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-684"><a href="#cb39-684" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-685"><a href="#cb39-685" aria-hidden="true" tabindex="-1"></a>Notice that the largest eigenvalue is 1, which is always true for a normalized adjacency matrix.</span>
<span id="cb39-686"><a href="#cb39-686" aria-hidden="true" tabindex="-1"></a>The largest eigenvector (the leftmost one) is associated with the stationary distribution of the random walk.</span>
<span id="cb39-687"><a href="#cb39-687" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-690"><a href="#cb39-690" aria-hidden="true" tabindex="-1"></a><span class="in">```{note}</span></span>
<span id="cb39-691"><a href="#cb39-691" aria-hidden="true" tabindex="-1"></a><span class="in">The sign of the eigenvector is indeterminate, which means we can choose the sign of the eigenvector arbitrarily. In fact, `np.linalg.eigh` returns the eigenvector whose sign can vary for a different run.</span></span>
<span id="cb39-692"><a href="#cb39-692" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-693"><a href="#cb39-693" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-694"><a href="#cb39-694" aria-hidden="true" tabindex="-1"></a>We decompose $\overline{\bf A}$ as</span>
<span id="cb39-695"><a href="#cb39-695" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-696"><a href="#cb39-696" aria-hidden="true" tabindex="-1"></a>$$\overline {\bf A} = {\bf Q}{\bf \Lambda}{\bf Q}^\top$$</span>
<span id="cb39-697"><a href="#cb39-697" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-698"><a href="#cb39-698" aria-hidden="true" tabindex="-1"></a>where ${\bf Q}$ corresponds to <span class="in">`eigvecs`</span> and ${\bf \Lambda}$ corresponds to <span class="in">`np.diag(evals)`</span> (since ${\bf \Lambda}$ is a diagonal matrix). Let's see if this is correct:</span>
<span id="cb39-699"><a href="#cb39-699" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-702"><a href="#cb39-702" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-703"><a href="#cb39-703" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(A_norm.toarray()).style.background_gradient(cmap<span class="op">=</span><span class="st">'cividis'</span>, axis <span class="op">=</span> <span class="va">None</span>).set_caption(<span class="st">"Normalized Adjacency Matrix"</span>)</span>
<span id="cb39-704"><a href="#cb39-704" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-705"><a href="#cb39-705" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-708"><a href="#cb39-708" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-709"><a href="#cb39-709" aria-hidden="true" tabindex="-1"></a>A_norm_reconstructed <span class="op">=</span> evecs <span class="op">@</span> np.diag(evals) <span class="op">@</span> evecs.T</span>
<span id="cb39-710"><a href="#cb39-710" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(A_norm_reconstructed).style.background_gradient(cmap<span class="op">=</span><span class="st">'cividis'</span>, axis <span class="op">=</span> <span class="va">None</span>).set_caption(<span class="st">"Reconstruction of the Normalized Adjacency Matrix"</span>)</span>
<span id="cb39-711"><a href="#cb39-711" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-712"><a href="#cb39-712" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-713"><a href="#cb39-713" aria-hidden="true" tabindex="-1"></a>Notice that the reconstruction is not perfect due to the numerical error, although overall the structure is correct.</span>
<span id="cb39-714"><a href="#cb39-714" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-715"><a href="#cb39-715" aria-hidden="true" tabindex="-1"></a><span class="fu">### Multi-step Transition Probability</span></span>
<span id="cb39-716"><a href="#cb39-716" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-717"><a href="#cb39-717" aria-hidden="true" tabindex="-1"></a>Let us first conform whether we can compute the transition probability after $t$ steps by using the eigenvalues and eigenvectors.</span>
<span id="cb39-718"><a href="#cb39-718" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-721"><a href="#cb39-721" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-722"><a href="#cb39-722" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb39-723"><a href="#cb39-723" aria-hidden="true" tabindex="-1"></a>x_0 <span class="op">=</span> np.zeros(g.vcount())</span>
<span id="cb39-724"><a href="#cb39-724" aria-hidden="true" tabindex="-1"></a>x_0[<span class="dv">0</span>] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb39-725"><a href="#cb39-725" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-726"><a href="#cb39-726" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute x_t by using the eigenvalues and eigenvectors</span></span>
<span id="cb39-727"><a href="#cb39-727" aria-hidden="true" tabindex="-1"></a>Q_L <span class="op">=</span> np.diag(<span class="fl">1.0</span><span class="op">/</span>np.sqrt(deg)) <span class="op">@</span> evecs</span>
<span id="cb39-728"><a href="#cb39-728" aria-hidden="true" tabindex="-1"></a>Q_R <span class="op">=</span> np.diag(np.sqrt(deg)) <span class="op">@</span> evecs</span>
<span id="cb39-729"><a href="#cb39-729" aria-hidden="true" tabindex="-1"></a>x_t <span class="op">=</span> x_0 <span class="op">@</span> Q_L <span class="op">@</span> np.diag(evals<span class="op">**</span>t) <span class="op">@</span> Q_R.T</span>
<span id="cb39-730"><a href="#cb39-730" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-731"><a href="#cb39-731" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute x_t by using the power iteration</span></span>
<span id="cb39-732"><a href="#cb39-732" aria-hidden="true" tabindex="-1"></a>x_t_power <span class="op">=</span> x_0.copy()</span>
<span id="cb39-733"><a href="#cb39-733" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(t):</span>
<span id="cb39-734"><a href="#cb39-734" aria-hidden="true" tabindex="-1"></a>    x_t_power <span class="op">=</span> x_t_power <span class="op">@</span> P</span>
<span id="cb39-735"><a href="#cb39-735" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-736"><a href="#cb39-736" aria-hidden="true" tabindex="-1"></a>pd.DataFrame({</span>
<span id="cb39-737"><a href="#cb39-737" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Eigenvector"</span>: x_t.flatten(),</span>
<span id="cb39-738"><a href="#cb39-738" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Power iteration"</span>: x_t_power.flatten()</span>
<span id="cb39-739"><a href="#cb39-739" aria-hidden="true" tabindex="-1"></a>}).style.background_gradient(cmap<span class="op">=</span><span class="st">'cividis'</span>, axis <span class="op">=</span> <span class="va">None</span>).set_caption(<span class="st">"Comparison of Eigenvector and Power Iteration"</span>)</span>
<span id="cb39-740"><a href="#cb39-740" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-741"><a href="#cb39-741" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-742"><a href="#cb39-742" aria-hidden="true" tabindex="-1"></a><span class="fu">### Relaxation Time and Mixing Time</span></span>
<span id="cb39-743"><a href="#cb39-743" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-744"><a href="#cb39-744" aria-hidden="true" tabindex="-1"></a>Let us measure the relaxation time of the random walk.</span>
<span id="cb39-745"><a href="#cb39-745" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-748"><a href="#cb39-748" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb39-749"><a href="#cb39-749" aria-hidden="true" tabindex="-1"></a>evals, evecs <span class="op">=</span> np.linalg.eigh(A_norm.toarray())</span>
<span id="cb39-750"><a href="#cb39-750" aria-hidden="true" tabindex="-1"></a>lambda_2 <span class="op">=</span> <span class="op">-</span>np.sort(<span class="op">-</span>evals)[<span class="dv">1</span>]</span>
<span id="cb39-751"><a href="#cb39-751" aria-hidden="true" tabindex="-1"></a>tau <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> lambda_2</span>
<span id="cb39-752"><a href="#cb39-752" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"The relaxation time of the random walk is </span><span class="sc">{</span>tau<span class="sc">:.4f}</span><span class="ss">."</span>)</span>
<span id="cb39-753"><a href="#cb39-753" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-754"><a href="#cb39-754" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-755"><a href="#cb39-755" aria-hidden="true" tabindex="-1"></a><span class="fu">## Unifying Centrality and Communities</span></span>
<span id="cb39-756"><a href="#cb39-756" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-757"><a href="#cb39-757" aria-hidden="true" tabindex="-1"></a><span class="fu">### Modularity: Random Walk Perspective</span></span>
<span id="cb39-758"><a href="#cb39-758" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-759"><a href="#cb39-759" aria-hidden="true" tabindex="-1"></a>Modularity can be interpreted from a random walk perspective. Modularity is given by</span>
<span id="cb39-760"><a href="#cb39-760" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-761"><a href="#cb39-761" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-762"><a href="#cb39-762" aria-hidden="true" tabindex="-1"></a>Q = \frac{1}{2m} \sum_{ij} \left( A_{ij} - \frac{d_i d_j}{2m} \right) \delta(c_i, c_j)</span>
<span id="cb39-763"><a href="#cb39-763" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-764"><a href="#cb39-764" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-765"><a href="#cb39-765" aria-hidden="true" tabindex="-1"></a>where $m$ is the number of edges in the network, $A_{ij}$ is the adjacency matrix, $d_i$ is the degree of node $i$, $c_i$ is the community of node $i$, and $\delta(c_i, c_j)$ is the Kronecker delta function (which is 1 if $c_i = c_j$ and 0 otherwise).</span>
<span id="cb39-766"><a href="#cb39-766" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-767"><a href="#cb39-767" aria-hidden="true" tabindex="-1"></a>We can rewrite the modularity using the language of random walks as follows.</span>
<span id="cb39-768"><a href="#cb39-768" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-769"><a href="#cb39-769" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-770"><a href="#cb39-770" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb39-771"><a href="#cb39-771" aria-hidden="true" tabindex="-1"></a>Q &amp;= \sum_{ij} \left(\frac{A_{ij}}{2m}  - \frac{d_i}{2m} \frac{d_j}{2m} \right) \delta(c_i, c_j) <span class="sc">\\</span></span>
<span id="cb39-772"><a href="#cb39-772" aria-hidden="true" tabindex="-1"></a>&amp;= \sum_{ij} \left(\pi_i P_{ij}  - \pi_i \pi_j \right) \delta(c_i, c_j)</span>
<span id="cb39-773"><a href="#cb39-773" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb39-774"><a href="#cb39-774" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-775"><a href="#cb39-775" aria-hidden="true" tabindex="-1"></a>where $\pi_i$ is the stationary distribution of the random walk given by</span>
<span id="cb39-776"><a href="#cb39-776" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-777"><a href="#cb39-777" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-778"><a href="#cb39-778" aria-hidden="true" tabindex="-1"></a>\pi_i = \frac{d_i}{2m}</span>
<span id="cb39-779"><a href="#cb39-779" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-780"><a href="#cb39-780" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-781"><a href="#cb39-781" aria-hidden="true" tabindex="-1"></a>and $P_{ij}$ is the transition probability between nodes $i$ and $j$.</span>
<span id="cb39-782"><a href="#cb39-782" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-785"><a href="#cb39-785" aria-hidden="true" tabindex="-1"></a><span class="in">```{note}</span></span>
<span id="cb39-786"><a href="#cb39-786" aria-hidden="true" tabindex="-1"></a><span class="in">Let's break down this derivation step by step:</span></span>
<span id="cb39-787"><a href="#cb39-787" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-788"><a href="#cb39-788" aria-hidden="true" tabindex="-1"></a><span class="in">1. We start with the original modularity formula:</span></span>
<span id="cb39-789"><a href="#cb39-789" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-790"><a href="#cb39-790" aria-hidden="true" tabindex="-1"></a><span class="in">   $$Q = \frac{1}{2m} \sum_{ij} \left( A_{ij} - \frac{d_i d_j}{2m} \right) \delta(c_i, c_j)$$</span></span>
<span id="cb39-791"><a href="#cb39-791" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-792"><a href="#cb39-792" aria-hidden="true" tabindex="-1"></a><span class="in">2. First, we move the constant $1/(2m)$ to the inside of the parentheses:</span></span>
<span id="cb39-793"><a href="#cb39-793" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-794"><a href="#cb39-794" aria-hidden="true" tabindex="-1"></a><span class="in">   $$Q = \sum_{ij} \left(\frac{A_{ij}}{2m} - \frac{d_i d_j}{2m^2} \right) \delta(c_i, c_j)$$</span></span>
<span id="cb39-795"><a href="#cb39-795" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-796"><a href="#cb39-796" aria-hidden="true" tabindex="-1"></a><span class="in">3. Now, we recognize that $\frac{A_{ij}}{2m}$ can be rewritten as:</span></span>
<span id="cb39-797"><a href="#cb39-797" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-798"><a href="#cb39-798" aria-hidden="true" tabindex="-1"></a><span class="in">   $$\frac{A_{ij}}{2m} = \frac{d_i}{2m} \cdot \frac{A_{ij}}{d_i} = \pi_i P_{ij}$$</span></span>
<span id="cb39-799"><a href="#cb39-799" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-800"><a href="#cb39-800" aria-hidden="true" tabindex="-1"></a><span class="in">4. We also recognize that $\frac{d_i}{2m}$ is the stationary distribution of the random walk, which we denote as $\pi_i$:</span></span>
<span id="cb39-801"><a href="#cb39-801" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-802"><a href="#cb39-802" aria-hidden="true" tabindex="-1"></a><span class="in">   $$\frac{d_i}{2m} = \pi_i$$</span></span>
<span id="cb39-803"><a href="#cb39-803" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-804"><a href="#cb39-804" aria-hidden="true" tabindex="-1"></a><span class="in">5. Substituting these into our equation:</span></span>
<span id="cb39-805"><a href="#cb39-805" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-806"><a href="#cb39-806" aria-hidden="true" tabindex="-1"></a><span class="in">   $$Q = \sum_{ij} \left(\pi_i P_{ij} - \pi_i \pi_j \right) \delta(c_i, c_j)$$</span></span>
<span id="cb39-807"><a href="#cb39-807" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-808"><a href="#cb39-808" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-809"><a href="#cb39-809" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-810"><a href="#cb39-810" aria-hidden="true" tabindex="-1"></a>The expression suggests that:</span>
<span id="cb39-811"><a href="#cb39-811" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-812"><a href="#cb39-812" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>The first term, $\pi_i P_{ij} \delta(c_i, c_j)$, represents the probability that a walker is at node $i$ and moves to node $j$ within the same community **by one step**.</span>
<span id="cb39-813"><a href="#cb39-813" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>The second term, $\pi_i \pi_j$, represents the probability that a walker is at node $i$ and moves to another node $j$ within the same community **after long steps**.</span>
<span id="cb39-814"><a href="#cb39-814" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-815"><a href="#cb39-815" aria-hidden="true" tabindex="-1"></a>In summary, modularity compares short-term and long-term random walk probabilities. High modularity indicates that a random walker is more likely to stay within the same community after one step than after many steps.</span>
<span id="cb39-816"><a href="#cb39-816" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-819"><a href="#cb39-819" aria-hidden="true" tabindex="-1"></a><span class="in">```{note}</span></span>
<span id="cb39-820"><a href="#cb39-820" aria-hidden="true" tabindex="-1"></a><span class="in">Building on this perspective from random walks, Delvenne et al. {footcite}`delvenne2010stability` extends the modularity by comparing multi-step and long-step transition probabilities of a random walk. This approach, known as "Markov stability", shows that the number of steps acts as a "resolution parameter" that determines the scale of detectable communities.</span></span>
<span id="cb39-821"><a href="#cb39-821" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-822"><a href="#cb39-822" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-823"><a href="#cb39-823" aria-hidden="true" tabindex="-1"></a><span class="fu">### PageRank: Random Walk Perspective</span></span>
<span id="cb39-824"><a href="#cb39-824" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-825"><a href="#cb39-825" aria-hidden="true" tabindex="-1"></a>PageRank can be interpreted from a random walk perspective:</span>
<span id="cb39-826"><a href="#cb39-826" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-827"><a href="#cb39-827" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-828"><a href="#cb39-828" aria-hidden="true" tabindex="-1"></a>c_i = (1-\beta) \sum_j P_{ji} c_j + \beta \cdot \frac{1}{N}</span>
<span id="cb39-829"><a href="#cb39-829" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb39-830"><a href="#cb39-830" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-831"><a href="#cb39-831" aria-hidden="true" tabindex="-1"></a>Where:</span>
<span id="cb39-832"><a href="#cb39-832" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$c_i$ is the PageRank of node $i$</span>
<span id="cb39-833"><a href="#cb39-833" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$P_{ji}$ is the transition probability from node $j$ to node $i$</span>
<span id="cb39-834"><a href="#cb39-834" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\beta$ is the teleportation probability</span>
<span id="cb39-835"><a href="#cb39-835" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$N$ is the total number of nodes</span>
<span id="cb39-836"><a href="#cb39-836" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-837"><a href="#cb39-837" aria-hidden="true" tabindex="-1"></a>This equation represents a random walk where:</span>
<span id="cb39-838"><a href="#cb39-838" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>With probability $(1-\beta)$, the walker follows a link to the next node.</span>
<span id="cb39-839"><a href="#cb39-839" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>With probability $\beta$, the walker *teleports* to a random node in the network.</span>
<span id="cb39-840"><a href="#cb39-840" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-841"><a href="#cb39-841" aria-hidden="true" tabindex="-1"></a>The PageRank $c_i$ is the stationary distribution of this random walk, representing the long-term probability of finding the walker at node $i$.</span>
<span id="cb39-842"><a href="#cb39-842" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-845"><a href="#cb39-845" aria-hidden="true" tabindex="-1"></a><span class="in">```{note}</span></span>
<span id="cb39-846"><a href="#cb39-846" aria-hidden="true" tabindex="-1"></a><span class="in">This sounds odd at first glance. But it makes sense when you think about what PageRank was invented for, i.e., Web search. It characterizes a web surfer as a random walker that chooses the next page by randomly jumping to a random page with probability $\beta$ or by following a link to a page with probability $1-\beta$. The web page with the largest PageRank means that the page is most likely to be visited by this random web surfer.</span></span>
<span id="cb39-847"><a href="#cb39-847" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb39-848"><a href="#cb39-848" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-849"><a href="#cb39-849" aria-hidden="true" tabindex="-1"></a><span class="fu">## Summary</span></span>
<span id="cb39-850"><a href="#cb39-850" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-851"><a href="#cb39-851" aria-hidden="true" tabindex="-1"></a>Random walks provide a powerful unified framework for understanding both centrality measures and community structure in networks. Through our exploration, we've seen how:</span>
<span id="cb39-852"><a href="#cb39-852" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-853"><a href="#cb39-853" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Implementation**: Random walks can be efficiently simulated and analyzed using matrix operations</span>
<span id="cb39-854"><a href="#cb39-854" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Stationary distributions**: Connect to centrality measures, with degree centrality emerging naturally</span>
<span id="cb39-855"><a href="#cb39-855" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Temporal dynamics**: Reveal community structure through short-term vs. long-term behavior</span>
<span id="cb39-856"><a href="#cb39-856" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Mathematical foundation**: Links to spectral graph theory and eigenvalue analysis</span>
<span id="cb39-857"><a href="#cb39-857" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**Applications**: Provide new interpretations of modularity and PageRank</span>
<span id="cb39-858"><a href="#cb39-858" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-859"><a href="#cb39-859" aria-hidden="true" tabindex="-1"></a>This unified perspective opens up new possibilities for network analysis and algorithm design, making random walks one of the most versatile tools in network science.</span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Copyright 2024, Sadamori Kojaku</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/skojaku/adv-net-sci">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>