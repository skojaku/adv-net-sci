
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Ladder Lottery &#8212; Advanced Topics in Network Science</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'tmp';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="home.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.jpg" class="logo__image only-light" alt="Advanced Topics in Network Science - Home"/>
    <script>document.write(`<img src="_static/logo.jpg" class="logo__image only-dark" alt="Advanced Topics in Network Science - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="home.html">
                    Welcome to SSIE 641 Advanced Topics on Network Science
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Intro</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="intro/why-networks.html">Why should we care networks?</a></li>

<li class="toctree-l1"><a class="reference internal" href="intro/zoo-of-networks.html">Zoo of networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="intro/setup.html">Trouble shooting</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">M01: Euler Tour</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="m01-euler_tour/what-to-learn.html">Module 1: Euler Tour</a></li>
<li class="toctree-l1"><a class="reference internal" href="m01-euler_tour/puzzle.html">A puzzle</a></li>
<li class="toctree-l1"><a class="reference internal" href="m01-euler_tour/euler-path.html">Euler‚Äôs solution</a></li>
<li class="toctree-l1"><a class="reference internal" href="m01-euler_tour/how-to-code-network.html">Compute with networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="m01-euler_tour/coding-exercise.html">Exercise</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">M02: Small World</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="m02-small-world/what-to-learn.html">Module 2: Small-world</a></li>
<li class="toctree-l1"><a class="reference internal" href="m02-small-world/small-world-experiment.html">Small-world experiment</a></li>
<li class="toctree-l1"><a class="reference internal" href="m02-small-world/wikirace.html">Wikirace</a></li>
<li class="toctree-l1"><a class="reference internal" href="m02-small-world/pen-and-paper.html">Why is our social network small world?</a></li>
<li class="toctree-l1"><a class="reference internal" href="m02-small-world/connectedness.html">Walks, Trails, Paths, and Connectedness</a></li>
<li class="toctree-l1"><a class="reference internal" href="m02-small-world/which-tools.html">Toolbox for network analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="m02-small-world/compressed-sparse-row.html">Efficient representation for large sparse networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="m02-small-world/connectedness-hands-on.html">Computing the Shortest Paths and Connected Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="m02-small-world/assignment.html">Assignment</a></li>
<li class="toctree-l1"><a class="reference internal" href="m02-small-world/appendix.html">Appendix</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">M03: Robustness</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="m03-robustness/what-to-learn.html">Module 3: Robustness</a></li>
<li class="toctree-l1"><a class="reference internal" href="m03-robustness/exercise-power-grid.html">Building a cost-effective power grid network</a></li>
<li class="toctree-l1"><a class="reference internal" href="m03-robustness/minimum-spanning-tree.html">Minimum spanning tree</a></li>
<li class="toctree-l1"><a class="reference internal" href="m03-robustness/robustness.html">Network Robustness</a></li>
<li class="toctree-l1"><a class="reference internal" href="m03-robustness/robustness-hands-on.html">Hands-on: Robustness (Random attack)</a></li>
<li class="toctree-l1"><a class="reference internal" href="m03-robustness/percolation.html">Percolation</a></li>
<li class="toctree-l1"><a class="reference internal" href="m03-robustness/appendix.html">Appendix</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">M04: Friendship Paradox</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="m04-friendship-paradox/what-to-learn.html">Module 4: Friendship Paradox</a></li>
<li class="toctree-l1"><a class="reference internal" href="m04-friendship-paradox/experiment.html">In-class experiment</a></li>
<li class="toctree-l1"><a class="reference internal" href="m04-friendship-paradox/friendship-paradox.html">Friendship Paradox</a></li>
<li class="toctree-l1"><a class="reference internal" href="m04-friendship-paradox/vaccination-game.html">Vaccination Game</a></li>
<li class="toctree-l1"><a class="reference internal" href="m04-friendship-paradox/degree-distribution.html">Degree distribution</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">M05: Clustering</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="m05-clustering/what-to-learn.html">Module 5: Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="m05-clustering/what-is-community.html">What is community?</a></li>
<li class="toctree-l1"><a class="reference internal" href="m05-clustering/pen-and-paper.html">Pen and Paper</a></li>
<li class="toctree-l1"><a class="reference internal" href="m05-clustering/pattern-matching.html">Community detection (pattern matching)</a></li>
<li class="toctree-l1"><a class="reference internal" href="m05-clustering/graph-cut.html">Graph cut</a></li>
<li class="toctree-l1"><a class="reference internal" href="m05-clustering/ratio-normalized-cut.html">Balanced cut</a></li>
<li class="toctree-l1"><a class="reference internal" href="m05-clustering/modularity.html">Modularity</a></li>
<li class="toctree-l1"><a class="reference internal" href="m05-clustering/modularity-02.html">Modularity (Cont.)</a></li>
<li class="toctree-l1"><a class="reference internal" href="m05-clustering/stochastic-block-model.html">Stochastic Block Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="m05-clustering/exercise-clustering.html">Hands-on: Clustering</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">M06: Centrality</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="m06-centrality/what-to-learn.html">Module 6: Centrality</a></li>
<li class="toctree-l1"><a class="reference internal" href="m06-centrality/pen-and-paper.html">Pen and paper exercises</a></li>
<li class="toctree-l1"><a class="reference internal" href="m06-centrality/degree-distance-based-centrality.html">What is centrality?</a></li>
<li class="toctree-l1"><a class="reference internal" href="m06-centrality/eigencentrality.html">Centralities based on centralities</a></li>
<li class="toctree-l1"><a class="reference internal" href="m06-centrality/hands-on.html">Computing centrality with Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="m06-centrality/assignment.html">Assignment</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">M07: Random Walks</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="m07-random-walks/what-to-learn.html">Module 7: Random Walks</a></li>
<li class="toctree-l1"><a class="reference internal" href="m07-random-walks/amida-kuji.html">Ladder Lottery</a></li>
<li class="toctree-l1"><a class="reference internal" href="m07-random-walks/random-walks.html">Random Walks</a></li>
<li class="toctree-l1"><a class="reference internal" href="m07-random-walks/pen-and-paper.html">Pen and paper exercises</a></li>
<li class="toctree-l1"><a class="reference internal" href="m07-random-walks/random-walks-code.html">Random Walks in Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="m07-random-walks/random-walks-math.html">Characteristics of Random Walks</a></li>
<li class="toctree-l1"><a class="reference internal" href="m07-random-walks/unifying-centrality-and-communities.html">Random walks unify centrality and communities</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">M08: Embedding</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="m08-embedding/what-to-learn.html">Module 8: Embedding</a></li>
<li class="toctree-l1"><a class="reference internal" href="m08-embedding/pen-and-paper.html">Pen and paper exercises</a></li>
<li class="toctree-l1"><a class="reference internal" href="m08-embedding/spectral-embedding.html">Spectral Embedding</a></li>
<li class="toctree-l1"><a class="reference internal" href="m08-embedding/word2vec.html">word2vec</a></li>
<li class="toctree-l1"><a class="reference internal" href="m08-embedding/graph-embedding-w-word2vec.html">Graph embedding with word2vec</a></li>
<li class="toctree-l1"><a class="reference internal" href="m08-embedding/spectral-vs-neural-embedding.html">Spectral vs Neural Embedding</a></li>
<li class="toctree-l1"><a class="reference internal" href="m08-embedding/software.html">Software for Network Embedding</a></li>
<li class="toctree-l1"><a class="reference internal" href="m08-embedding/appendix.html">Appendix</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">M09: Graph Neural Networks</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="m09-graph-neural-networks/what-to-learn.html">Module 9: Graph Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="m09-graph-neural-networks/pen-and-paper.html">Pen and paper exercises</a></li>
<li class="toctree-l1"><a class="reference internal" href="m09-graph-neural-networks/image-processing.html">Preliminaries: Image Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="m09-graph-neural-networks/from-image-to-graph.html">From Image to Graph</a></li>
<li class="toctree-l1"><a class="reference internal" href="m09-graph-neural-networks/graph-convolutional-network.html">Graph Convolutional Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="m09-graph-neural-networks/popular-gnn.html">Popular Graph Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="m09-graph-neural-networks/appendix.html">Appendix</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/skojaku/adv-net-sci/gh-pages?urlpath=tree/docs/lecture-note/tmp.md" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Binder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Binder logo" src="_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="initThebeSBT()"
  class="btn btn-sm btn-launch-thebe dropdown-item"
  title="Launch Thebe"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-play"></i>
  </span>
<span class="btn__text-container">Live Code</span>
</button>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/skojaku/adv-net-sci" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/skojaku/adv-net-sci/issues/new?title=Issue%20on%20page%20%2Ftmp.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/tmp.ipynb" target="_blank"
   class="btn btn-sm btn-download-notebook-button dropdown-item"
   title="Download notebook file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-code"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li><a href="_sources/tmp.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Ladder Lottery</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Ladder Lottery</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-01">Exercise 01</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#expected-behavior-of-random-walks">Expected behavior of random walks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-02">Exercise 02</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-03">Exercise 03</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#community-structure">Community structure</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-04">Exercise 04</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#jupytext-formats-md-myst-text-representation-extension-md-format-name-myst-kernelspec-display-name-python-3-language-python-name-python3">jupytext:
formats: md:myst
text_representation:
extension: .md
format_name: myst
kernelspec:
display_name: Python 3
language: python
name: python3</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#characteristics-of-random-walks">Characteristics of Random Walks</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stationary-state">Stationary State</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#experiment">Experiment</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#time-to-reach-the-stationary-state">Time to reach the stationary state</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#compute-the-mixing-time">Compute the mixing time</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#normalized-adjacency-matrix">Normalized Adjacency Matrix</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-step-transition-probability">Multi-step Transition Probability</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#relaxation-time-and-mixing-time">Relaxation Time and Mixing Time</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">jupytext:
formats: md:myst
text_representation:
extension: .md
format_name: myst
kernelspec:
display_name: Python 3
language: python
name: python3</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#random-walks-unify-centrality-and-communities">Random walks unify centrality and communities</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#modularity-interpretation-from-random-walk-perspective">Modularity: Interpretation from random walk perspective</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pagerank-interpretation-from-random-walk-perspective">PageRank: Interpretation from random walk perspective</a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="ladder-lottery">
<h1>Ladder Lottery<a class="headerlink" href="#ladder-lottery" title="Link to this heading">#</a></h1>
<div class="tip admonition">
<p class="admonition-title">Ladder Lottery</p>
<p>Ladder Lottery is a fun East Asian game, also known as ‚ÄúÈ¨ºËÖ≥Âúñ‚Äù (Guijiaotu) in Chinese, ‚ÄúÈòøÂº•ÈôÄÁ±§‚Äù (Amida-kuzi) in Japanese, ‚ÄúÏÇ¨Îã§Î¶¨ÌÉÄÍ∏∞‚Äù (Sadaritagi) in Korean, and ‚ÄúLadder Lottery‚Äù in English. The game is played as follows:</p>
<ol class="arabic simple">
<li><p>A player is given a board with a set of vertical lines.</p></li>
<li><p>The player chooses a line and starts to move along the line</p></li>
<li><p>When hitting a horizontal line, the player must move along the horizontal line and then continue to move along the next vertical line.</p></li>
<li><p>The player wins if the player can hit a marked line at the bottom of the board.</p></li>
<li><p>You cannot see the horizontal lines in advance!</p></li>
</ol>
<p>Play the <a class="reference external" href="https://skojaku.github.io/adv-net-sci/vis/amida-kuji.html?">Ladder Lottery Game! üéÆ‚ú®</a> and try to answer the following questions:</p>
<ol class="arabic simple">
<li><p>Is tehre a strategy to maximize the probability of winning?</p></li>
<li><p>How does the probability of winning change as the number of horizontal lines increases?</p></li>
</ol>
<p><img alt="" src="https://upload.wikimedia.org/wikipedia/commons/6/64/Amidakuji_2022-05-10.gif" /></p>
<div class="highlight-# notranslate"><div class="highlight"><pre><span></span>
- [‚úçÔ∏è Pen and paper exercises](pen-and-paper/exercise.pdf)
---
jupytext:
  formats: md:myst
  text_representation:
    extension: .md
    format_name: myst
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---

# Random Walks in Python

## Simulating Random Walks

We will simulate random walks on a simple graph of five nodes as follows.

```{code-cell} ipython3
import numpy as np
import igraph

g = igraph.Graph()

g.add_vertices([0, 1, 2, 3, 4])
g.add_edges([(0, 1), (0, 2), (0, 3), (1, 3), (2, 3), (2, 4), (3, 4)])
igraph.plot(g, vertex_size=20, vertex_label=g.vs[&quot;name&quot;])
</pre></div>
</div>
</div>
<p>A random walk is characterized by the transition probabilities between nodes.</p>
<div class="math notranslate nohighlight">
\[
P_{ij} = \frac{A_{ij}}{k_i}
\]</div>
<p>Let us first compute the transition probabilities and store them in a matrix, <span class="math notranslate nohighlight">\(\mathbf{P}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">A</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">get_adjacency_sparse</span><span class="p">()</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
<span class="n">k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">degree</span><span class="p">())</span>
<span class="n">n_nodes</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">vcount</span><span class="p">()</span>

<span class="c1"># A simple but inefficient way to compute P</span>
<span class="n">P</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_nodes</span><span class="p">,</span> <span class="n">n_nodes</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_nodes</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_nodes</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">k</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">P</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">/</span> <span class="n">k</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">P</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

<span class="c1"># Alternative, more efficient way to compute P</span>
<span class="n">P</span> <span class="o">=</span> <span class="n">A</span> <span class="o">/</span> <span class="n">k</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>

<span class="c1"># or even more efficiently</span>
<span class="n">P</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;ij,i-&gt;ij&quot;</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">k</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">NameError</span><span class="g g-Whitespace">                                 </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">line</span> <span class="mi">1</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="n">A</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">get_adjacency_sparse</span><span class="p">()</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="n">k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">degree</span><span class="p">())</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span> <span class="n">n_nodes</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">vcount</span><span class="p">()</span>

<span class="ne">NameError</span>: name &#39;g&#39; is not defined
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Transition probability matrix:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">P</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;YlGnBu&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Each row and column of <span class="math notranslate nohighlight">\(\mathbf{P}\)</span> corresponds to a node, with entries representing the transition probabilities from the row node to the column node.</p>
<p>Now, let us simulate a random walk on this graph. We represent a position of the walker by a vector, <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, with five elements, each of which represents a node. We mark the node that the walker is currently at by <code class="docutils literal notranslate"><span class="pre">1</span></code> and others as <code class="docutils literal notranslate"><span class="pre">0</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Initial position of the walker:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>This vector representation is convenient to get the probabilities of transitions to other nodes from the current node:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{x} \mathbf{P}
\]</div>
<p>which is translated into the following code:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">probs</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">P</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Position of the walker after one step:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">probs</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We can then draw the next node based on the probabilities</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">next_node</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">n_nodes</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">probs</span><span class="p">)</span>
<span class="n">x</span><span class="p">[:]</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># zero out the vector</span>
<span class="n">x</span><span class="p">[</span><span class="n">next_node</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="c1"># set the next node to 1</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Position of the walker after one step:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>By repeating this process, we can simulate the random walk.</p>
<section id="exercise-01">
<h2>Exercise 01<a class="headerlink" href="#exercise-01" title="Link to this heading">#</a></h2>
<p>Write the following function to simulate the random walk for a given number of steps and return the <span class="math notranslate nohighlight">\(x\)</span> for each step.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">random_walk</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">n_steps</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Simulate the random walk on a graph with adjacency matrix A.</span>

<span class="sd">    Args:</span>
<span class="sd">        A (np.ndarray): The adjacency matrix of the graph.</span>
<span class="sd">        x (np.ndarray): The initial position of the walker.</span>
<span class="sd">        n_steps (int): The number of steps to simulate.</span>

<span class="sd">    Returns:</span>
<span class="sd">        np.ndarray: The position of the walker after each step.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Your code here</span>
    <span class="k">pass</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="expected-behavior-of-random-walks">
<h2>Expected behavior of random walks<a class="headerlink" href="#expected-behavior-of-random-walks" title="Link to this heading">#</a></h2>
<p>What is the expected position of the walker after multiple steps? It is easy to compute the expected position of the walker after one step from initial position <span class="math notranslate nohighlight">\(x(0)\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}[x(1)] = x(0) P
\]</div>
<p>where <span class="math notranslate nohighlight">\(x(t)\)</span> is the probability distribution of the walker at time <span class="math notranslate nohighlight">\(t\)</span>. In Python, the expected position of the walker at time <span class="math notranslate nohighlight">\(t=1\)</span> is given by</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x_0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="n">x_1</span> <span class="o">=</span> <span class="n">x_0</span> <span class="o">@</span> <span class="n">P</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Expected position of the walker after one step:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">x_1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>For the second step, the expected position of the walker is given by</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}[x(2)] = \mathbb{E}[x(1) P] = \mathbb{E}[x(0) P] P = x(0) P^2
\]</div>
<p>In other words,</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x_2</span> <span class="o">=</span> <span class="n">x_1</span> <span class="o">@</span> <span class="n">P</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Expected position of the walker after two steps:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">x_2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Following the same argument, the expected position of the walker at time <span class="math notranslate nohighlight">\(t\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}[x(t)] = x(0) P^t
\]</div>
<section id="exercise-02">
<h3>Exercise 02<a class="headerlink" href="#exercise-02" title="Link to this heading">#</a></h3>
<p>Write a function to compute the expected position of the walker at time <span class="math notranslate nohighlight">\(t\)</span> using the above formula:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">expected_position</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">x_0</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the expected position of the walker at time t.</span>

<span class="sd">    Args:</span>
<span class="sd">        A (np.ndarray): The adjacency matrix of the graph.</span>
<span class="sd">        x_0 (np.ndarray): The initial position of the walker.</span>
<span class="sd">        t (int): The number of steps to simulate.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Your code here</span>
    <span class="k">pass</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="exercise-03">
<h3>Exercise 03<a class="headerlink" href="#exercise-03" title="Link to this heading">#</a></h3>
<p>Plot each element of <span class="math notranslate nohighlight">\(x(t)\)</span> as a function of <span class="math notranslate nohighlight">\(t\)</span> for <span class="math notranslate nohighlight">\(t=0,1,2,\ldots, 1000\)</span>. Try different initial positions and compare the results!</p>
<p>Steps:</p>
<ol class="arabic simple">
<li><p>Define the initial position of the walker.</p></li>
<li><p>Compute the expected position of the walker at time <span class="math notranslate nohighlight">\(t\)</span> using the function you wrote above.</p></li>
<li><p>Draw a line for each element of <span class="math notranslate nohighlight">\(x(t)\)</span>, totalling 5 lines.</p></li>
<li><p>Create multiple such plots for different initial positions and compare them.</p></li>
</ol>
</section>
</section>
<section id="community-structure">
<h2>Community structure<a class="headerlink" href="#community-structure" title="Link to this heading">#</a></h2>
<p>Random walks can capture community structure of a network.
To see this, let us consider a network of a ring of cliques.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">networkx</span> <span class="k">as</span> <span class="nn">nx</span>
<span class="kn">import</span> <span class="nn">igraph</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">n_cliques</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">n_nodes_per_clique</span> <span class="o">=</span> <span class="mi">5</span>

<span class="n">G</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">ring_of_cliques</span><span class="p">(</span><span class="n">n_cliques</span><span class="p">,</span> <span class="n">n_nodes_per_clique</span><span class="p">)</span>
<span class="n">g</span> <span class="o">=</span> <span class="n">igraph</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span><span class="o">.</span><span class="n">Adjacency</span><span class="p">(</span><span class="n">nx</span><span class="o">.</span><span class="n">to_numpy_array</span><span class="p">(</span><span class="n">G</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span><span class="o">.</span><span class="n">as_undirected</span><span class="p">()</span>
<span class="n">membership</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_cliques</span><span class="p">),</span> <span class="n">n_nodes_per_clique</span><span class="p">)</span>

<span class="n">color_map</span> <span class="o">=</span> <span class="p">[</span><span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">()[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">membership</span><span class="p">]</span>
<span class="n">igraph</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">vertex_size</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">vertex_color</span><span class="o">=</span><span class="n">color_map</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let us compute the expected position of the walker after 1 to 10 steps.</p>
<p><strong>Compute the transition matrix</strong>:</p>
<div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">sparse</span>

<span class="c1"># Get the adjacency matrix and degree</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">get_adjacency_sparse</span><span class="p">()</span>
<span class="n">k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">degree</span><span class="p">())</span>

<span class="c1"># This is an efficient way to compute the transition matrix</span>
<span class="c1"># using scipy.sparse</span>
<span class="n">P</span> <span class="o">=</span> <span class="n">sparse</span><span class="o">.</span><span class="n">diags</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">k</span><span class="p">)</span> <span class="o">@</span> <span class="n">A</span>
</pre></div>
</div>
</div>
</details>
</div>
<p><strong>Compute the expected position of the walker after 1 to 300 steps</strong>:</p>
<div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x_t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">vcount</span><span class="p">())</span>
<span class="n">x_t</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">x_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">x_t</span><span class="p">]</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">300</span><span class="p">):</span>
    <span class="n">x_t</span> <span class="o">=</span> <span class="n">x_t</span> <span class="o">@</span> <span class="n">P</span>
    <span class="n">x_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x_t</span><span class="p">)</span>
<span class="n">x_list</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x_list</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
</div>
<p><strong>Plot the expected position of the walker at each step</strong>:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cmap</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">(</span><span class="s2">&quot;viridis&quot;</span><span class="p">,</span> <span class="n">as_cmap</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">sns</span><span class="o">.</span><span class="n">set_style</span><span class="p">(</span><span class="s1">&#39;white&#39;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">font_scale</span><span class="o">=</span><span class="mf">1.2</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_style</span><span class="p">(</span><span class="s1">&#39;ticks&#39;</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">10</span><span class="p">),</span> <span class="n">ncols</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">nrows</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>

<span class="n">t_list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">299</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">t_list</span><span class="p">):</span>
    <span class="n">igraph</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">vertex_size</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">vertex_color</span><span class="o">=</span><span class="p">[</span><span class="n">cmap</span><span class="p">(</span><span class="n">x_list</span><span class="p">[</span><span class="n">t</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x_list</span><span class="p">[</span><span class="n">t</span><span class="p">]))</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">vcount</span><span class="p">())],</span> <span class="n">target</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="o">//</span><span class="mi">3</span><span class="p">][</span><span class="n">i</span><span class="o">%</span><span class="k">3</span>])
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="o">//</span><span class="mi">3</span><span class="p">][</span><span class="n">i</span><span class="o">%</span><span class="k">3</span>].set_title(f&quot;$t$ = {t}&quot;, fontsize = 25)
</pre></div>
</div>
</div>
</details>
</div>
<p>where the color of each node represents the probability of the walker being at that node.</p>
<p>An important observation is that the walker spends more time in the clique that it started from and then diffuse to others. Thus, the position of the walker before reaching the steady state tells us the community structure of the network.</p>
</section>
<section id="exercise-04">
<h2>Exercise 04<a class="headerlink" href="#exercise-04" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>Generate a network of 100 nodes with 4 communities using a stochastic block model, with inter-community edge probability <span class="math notranslate nohighlight">\(0.05\)</span> and intra-community edge probability <span class="math notranslate nohighlight">\(0.2\)</span>. Then, compute the expected position of the walker starting from node zero after <span class="math notranslate nohighlight">\(x\)</span> steps. Plot the results for <span class="math notranslate nohighlight">\(x = 0, 5, 10, 1000\)</span>.</p></li>
<li><p>Increase the inter-community edge probability to <span class="math notranslate nohighlight">\(0.15\)</span> and repeat the simulation. Compare the results with the previous simulation.</p></li>
</ol>
</section>
<hr class="docutils" />
<section id="jupytext-formats-md-myst-text-representation-extension-md-format-name-myst-kernelspec-display-name-python-3-language-python-name-python3">
<h2>jupytext:
formats: md:myst
text_representation:
extension: .md
format_name: myst
kernelspec:
display_name: Python 3
language: python
name: python3<a class="headerlink" href="#jupytext-formats-md-myst-text-representation-extension-md-format-name-myst-kernelspec-display-name-python-3-language-python-name-python3" title="Link to this heading">#</a></h2>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="characteristics-of-random-walks">
<h1>Characteristics of Random Walks<a class="headerlink" href="#characteristics-of-random-walks" title="Link to this heading">#</a></h1>
<section id="stationary-state">
<h2>Stationary State<a class="headerlink" href="#stationary-state" title="Link to this heading">#</a></h2>
<p>Let‚Äôs dive into the math behind random walks in a way that‚Äôs easy to understand.</p>
<p>Imagine you‚Äôre at node <span class="math notranslate nohighlight">\(i\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>. You randomly move to a neighboring node <span class="math notranslate nohighlight">\(j\)</span>. The probability of this move, called the transition probability <span class="math notranslate nohighlight">\(p_{ij}\)</span>, is:</p>
<div class="math notranslate nohighlight">
\[
p_{ij} = \frac{A_{ij}}{k_i},
\]</div>
<p>Here, <span class="math notranslate nohighlight">\(A_{ij}\)</span> is an element of the adjacency matrix, and <span class="math notranslate nohighlight">\(k_i\)</span> is the degree of node <span class="math notranslate nohighlight">\(i\)</span>. For a network with <span class="math notranslate nohighlight">\(N\)</span> nodes, we can represent all transition probabilities in a transition probability matrix <span class="math notranslate nohighlight">\(P\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{P} = \begin{pmatrix}
p_{11} &amp; p_{12} &amp; \cdots &amp; p_{1N} \\
p_{21} &amp; p_{22} &amp; \cdots &amp; p_{2N} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
p_{N1} &amp; p_{N2} &amp; \cdots &amp; p_{NN}
\end{pmatrix}
\end{split}\]</div>
<p>This matrix <span class="math notranslate nohighlight">\(P\)</span> encapsulates the entire random walk process. We can use it to calculate the probability of visiting each node after any number of steps. For instance:</p>
<ul class="simple">
<li><p>After one step: <span class="math notranslate nohighlight">\(P_{ij} = p_{ij}\)</span></p></li>
<li><p>After two steps: <span class="math notranslate nohighlight">\(\left(\mathbf{P}^{2}\right)_{ij} = \sum_{k} P_{ik} P_{kj}\)</span></p></li>
<li><p>After <span class="math notranslate nohighlight">\(T\)</span> steps: <span class="math notranslate nohighlight">\(\left(\mathbf{P}^{T}\right)_{ij}\)</span></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Let‚Äôs explore why <span class="math notranslate nohighlight">\(\mathbf{P}^2\)</span> represents the transition probabilities after two steps.</p>
<p>First, recall that <span class="math notranslate nohighlight">\(\mathbf{P}_{ij}\)</span> is the probability of moving from node <span class="math notranslate nohighlight">\(i\)</span> to node <span class="math notranslate nohighlight">\(j\)</span> in one step. Now, consider a two-step walk from <span class="math notranslate nohighlight">\(i\)</span> to <span class="math notranslate nohighlight">\(j\)</span>. We can express this as:</p>
<div class="math notranslate nohighlight">
\[(\mathbf{P}^2)_{ij} = \sum_k \mathbf{P}_{ik} \mathbf{P}_{kj}\]</div>
<p>This equation encapsulates a key idea: to go from <span class="math notranslate nohighlight">\(i\)</span> to <span class="math notranslate nohighlight">\(j\)</span> in two steps, we must pass through some intermediate node <span class="math notranslate nohighlight">\(k\)</span>. Let‚Äôs break this down step by step:</p>
<ol class="arabic simple">
<li><p>The probability of the first step (<span class="math notranslate nohighlight">\(i\)</span> to <span class="math notranslate nohighlight">\(k\)</span>) is <span class="math notranslate nohighlight">\(\mathbf{P}_{ik}\)</span>.</p></li>
<li><p>The probability of the second step (<span class="math notranslate nohighlight">\(k\)</span> to <span class="math notranslate nohighlight">\(j\)</span>) is <span class="math notranslate nohighlight">\(\mathbf{P}_{kj}\)</span>.</p></li>
<li><p>The probability of this specific path (<span class="math notranslate nohighlight">\(i\)</span> ‚Üí <span class="math notranslate nohighlight">\(k\)</span> ‚Üí <span class="math notranslate nohighlight">\(j\)</span>) is the product <span class="math notranslate nohighlight">\(\mathbf{P}_{ik} \mathbf{P}_{kj}\)</span>.</p></li>
<li><p>We sum over all possible intermediate nodes <span class="math notranslate nohighlight">\(k\)</span> to get the total probability.</p></li>
</ol>
<p>Likewise, for three steps, we have:</p>
<div class="math notranslate nohighlight">
\[(\mathbf{P}^3)_{ij} = \sum_k \left( \mathbf{P}\right)^2_{ik} \mathbf{P}_{kj}\]</div>
<p>where:</p>
<ol class="arabic simple">
<li><p>The probability of going from <span class="math notranslate nohighlight">\(i\)</span> to <span class="math notranslate nohighlight">\(k\)</span> in two steps is <span class="math notranslate nohighlight">\(\left( \mathbf{P}\right)^2_{ik}\)</span>.</p></li>
<li><p>The probability of going from <span class="math notranslate nohighlight">\(k\)</span> to <span class="math notranslate nohighlight">\(j\)</span> in one step is <span class="math notranslate nohighlight">\(\mathbf{P}_{kj}\)</span>.</p></li>
<li><p>The probability of this specific path (<span class="math notranslate nohighlight">\(i\)</span> ‚Üí‚Ä¶‚Üí<span class="math notranslate nohighlight">\(k\)</span> ‚Üí <span class="math notranslate nohighlight">\(j\)</span>) is the product <span class="math notranslate nohighlight">\(\left( \mathbf{P}\right)^2_{ik} \mathbf{P}_{kj}\)</span>.</p></li>
<li><p>We sum over all possible intermediate nodes <span class="math notranslate nohighlight">\(k\)</span> to get the total probability.</p></li>
</ol>
<p>And we can extend this reasoning for any number of steps <span class="math notranslate nohighlight">\(t\)</span>.</p>
<p>In summary, for any number of steps <span class="math notranslate nohighlight">\(t\)</span>, <span class="math notranslate nohighlight">\(\left( \mathbf{P}^t \right)_{ij}\)</span> gives the probability of being at node <span class="math notranslate nohighlight">\(j\)</span> after <span class="math notranslate nohighlight">\(t\)</span> steps, starting from node <span class="math notranslate nohighlight">\(i\)</span>.</p>
</div>
<p>As <span class="math notranslate nohighlight">\(T\)</span> becomes very large, the probability distribution of being at each node, <span class="math notranslate nohighlight">\(\mathbf{x}(t)\)</span>, approaches a constant value:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{x}(t+1) =\mathbf{x}(t) \mathbf{P}
\]</div>
<p>This is an eigenvector equation. The solution, given by the Perron-Frobenius theorem, is called the stationary distribution:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{x}(\infty) = \mathbb{\pi}, \; \mathbf{\pi} = [\pi_1, \ldots, \pi_N]
\]</div>
<p>For undirected networks, this stationary distribution always exists and is proportional to the degree of each node:</p>
<div class="math notranslate nohighlight">
\[
\pi_j = \frac{k_j}{\sum_{\ell} k_\ell} \propto k_j
\]</div>
<p>This means the probability of being at node <span class="math notranslate nohighlight">\(j\)</span> in the long run is proportional to the degree of node <span class="math notranslate nohighlight">\(j\)</span>. The normalization ensures that the sum of all probabilities is 1, i.e., <span class="math notranslate nohighlight">\(\sum_{j=1}^N \pi_j = 1\)</span>.</p>
</section>
<section id="experiment">
<h2>Experiment<a class="headerlink" href="#experiment" title="Link to this heading">#</a></h2>
<p>Let us demonstrate the above math by using a small network using Python. Let us consider a small network of 5 nodes, which looks like this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">igraph</span> <span class="k">as</span> <span class="nn">ig</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">edge_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">):</span>
        <span class="n">edge_list</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">))</span>
        <span class="n">edge_list</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">i</span><span class="o">+</span><span class="mi">5</span><span class="p">,</span> <span class="n">j</span><span class="o">+</span><span class="mi">5</span><span class="p">))</span>
<span class="n">edge_list</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">g</span> <span class="o">=</span> <span class="n">ig</span><span class="o">.</span><span class="n">Graph</span><span class="p">(</span><span class="n">edge_list</span><span class="p">)</span>
<span class="n">ig</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">vertex_size</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">vertex_label</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">vcount</span><span class="p">()))</span>
</pre></div>
</div>
</div>
</div>
<p>The transition probability matrix <span class="math notranslate nohighlight">\(P\)</span> is given by:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scipy.sparse</span> <span class="k">as</span> <span class="nn">sparse</span>

<span class="n">A</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">get_adjacency_sparse</span><span class="p">()</span>
<span class="n">deg</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="n">Dinv</span> <span class="o">=</span> <span class="n">sparse</span><span class="o">.</span><span class="n">diags</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">deg</span><span class="p">)</span>
<span class="n">P</span> <span class="o">=</span> <span class="n">Dinv</span> <span class="o">@</span> <span class="n">A</span>
<span class="n">P</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Let us compute the stationary distribution by using the power method.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">vcount</span><span class="p">())</span>
<span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="c1"># Start from node 1</span>
<span class="n">T</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">xt</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">@</span> <span class="n">P</span>
    <span class="n">xt</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">xt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">xt</span><span class="p">)</span> <span class="c1"># Stack the results vertically</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">palette</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">()</span><span class="o">.</span><span class="n">as_hex</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">vcount</span><span class="p">()):</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">),</span> <span class="n">y</span><span class="o">=</span><span class="n">xt</span><span class="p">[:,</span> <span class="n">i</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Node </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">palette</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Time&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Probability&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Stationary distribution of a random walk&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.05</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper left&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>We see that the distributions of the walker converges, and there are three characteristic features in the convergence:</p>
<ol class="arabic simple">
<li><p>The distribution of the walker occilates with a decying amplitude and eventually converges.</p></li>
<li><p>Nodes of the same degree converge to the same stationary probability.</p></li>
<li><p>Nodes with higher degree converge to the higher stationary probability.</p></li>
</ol>
<p>To validate the last two observation, let us compare the stationary distribution of a random walker with the expected stationary distribution, which is proportional to the degree of the nodes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">n_edges</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">deg</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
<span class="n">expected_stationary_dist</span> <span class="o">=</span> <span class="n">deg</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_edges</span><span class="p">)</span>

<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
    <span class="s2">&quot;Expected stationary distribution&quot;</span><span class="p">:</span> <span class="n">expected_stationary_dist</span><span class="p">,</span>
    <span class="s2">&quot;Stationary distribution of a random walk&quot;</span><span class="p">:</span> <span class="n">xt</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="p">})</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{:.4f}</span><span class="s2">&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">set_caption</span><span class="p">(</span><span class="s2">&quot;Comparison of Expected and Observed Stationary Distributions&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">background_gradient</span><span class="p">(</span><span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;cividis&#39;</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="time-to-reach-the-stationary-state">
<h2>Time to reach the stationary state<a class="headerlink" href="#time-to-reach-the-stationary-state" title="Link to this heading">#</a></h2>
<p>Let‚Äôs explore how quickly a random walker reaches its stationary state. The convergence speed is influenced by two main factors: edge density and community structure. In sparse networks, the walker needs more steps to explore the entire network. Additionally, the walker tends to remain within its starting community for some time.</p>
<p>The mixing time, denoted as <span class="math notranslate nohighlight">\(t_{\text{mix}}\)</span>, is defined as the minimum number of steps required for a random walk to get close to the stationary distribution, regardless of the starting point, with the maximum error less than <span class="math notranslate nohighlight">\(\epsilon\)</span>:</p>
<div class="math notranslate nohighlight">
\[t_{\text{mix}} = \min\{t : \max_{{\bf x}(0)} \|{\bf x}(t) - {\bf \pi}\|_{1} \leq \epsilon\}\]</div>
<p>where <span class="math notranslate nohighlight">\(\|{\bf x}(t) - {\bf \pi}\|_{1} = 2\max_{i} |x_i(t) - \pi_i|\)</span> represents the L1 distance between two probability distributions. The choice of <span class="math notranslate nohighlight">\(\epsilon\)</span> is arbitrary.</p>
<p>We know that the distribution of a walker after <span class="math notranslate nohighlight">\(t\)</span> steps is given by:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{x}(t) =  \mathbf{x}(0) \mathbf{P} ^{t}
\]</div>
<p>To find this distribution, we need to compute <span class="math notranslate nohighlight">\(\mathbf{P}^t\)</span>. However, we face a challenge: <span class="math notranslate nohighlight">\(\mathbf{P}\)</span> is not diagonalizable.</p>
<p>A diagonalizable matrix <span class="math notranslate nohighlight">\(\mathbf{S}\)</span> can be written as <span class="math notranslate nohighlight">\(\mathbf{S} = \mathbf{Q} \mathbf{\Lambda} \mathbf{Q}^{-1}\)</span>, where <span class="math notranslate nohighlight">\(\mathbf{\Lambda}\)</span> is a diagonal matrix and <span class="math notranslate nohighlight">\(\mathbf{Q}\)</span> is an orthogonal matrix. Visually, it looks like this:</p>
<p><img alt="" src="../figs/diagonalizable.jpg" /></p>
<p>It is useful because we can then compute the power of the matrix as follows:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{S}^t = \mathbf{Q} \mathbf{\Lambda}^t \mathbf{Q}^{-1}
\]</div>
<p>And it is easy to find <span class="math notranslate nohighlight">\({\bf Q}\)</span> and <span class="math notranslate nohighlight">\({\bf \Lambda}\)</span> by using eigenvalue decomposition if <span class="math notranslate nohighlight">\({\bf S}\)</span> is symmetric and consists only of real values. Namely, the eigenvectors form <span class="math notranslate nohighlight">\({\cal Q}\)</span> and the eigenvalues form the diagonal matrix <span class="math notranslate nohighlight">\({\cal \Lambda}\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Let us demonstrate the above relation by calculating <span class="math notranslate nohighlight">\(\mathbf{S}^2\)</span>.
$$</p>
<div class="amsmath math notranslate nohighlight" id="equation-f5186c87-15bb-43b4-b064-f40a1a34a5e1">
<span class="eqno">()<a class="headerlink" href="#equation-f5186c87-15bb-43b4-b064-f40a1a34a5e1" title="Permalink to this equation">#</a></span>\[\begin{align}
\mathbf{S}^2 &amp;= \mathbf{Q} \mathbf{\Lambda} \mathbf{Q}^{-1} \mathbf{Q} \mathbf{\Lambda} \mathbf{Q}^{-1} \\
&amp;= \mathbf{Q} \mathbf{\Lambda}^2 \mathbf{Q}^{-1}.
\end{align}\]</div>
<p>$<span class="math notranslate nohighlight">\(
(Note that \)</span>\mathbf{Q} \mathbf{Q}^{-1} = {\bf I}$.)</p>
<p><img alt="" src="../figs/diagonalizable-squared.jpg" /></p>
</div>
<p><span class="math notranslate nohighlight">\(\mathbf{P}\)</span> is also diagonalizable but not symmetric like <span class="math notranslate nohighlight">\(\mathbf{\overline A}\)</span> so that we cannot use the above relation directly. So we do a trick by rewriteing <span class="math notranslate nohighlight">\(\mathbf{P}\)</span> as:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{P} = \mathbf{D}^{-1} \mathbf{A} = \mathbf{D}^{-\frac{1}{2}} \overline {\bf A} \mathbf{D}^{\frac{1}{2}}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\overline{\bf A} = \mathbf{D}^{-\frac{1}{2}} \mathbf{A} \mathbf{D}^{-\frac{1}{2}}\)</span> is the normalized adjacency matrix.</p>
<p>The advantage is that <span class="math notranslate nohighlight">\(\overline{\bf A}\)</span> is diagonalizable: <span class="math notranslate nohighlight">\(\overline{\bf A} = \mathbf{Q} \mathbf{\Lambda} \mathbf{Q}^\top\)</span>. Using this, we can compute <span class="math notranslate nohighlight">\(\mathbf{P}^t\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{P}^t = \mathbf{D}^{-\frac{1}{2}} \mathbf{Q} \mathbf{\Lambda}^t \mathbf{Q}^\top \mathbf{D}^{\frac{1}{2}} = \mathbf{Q}_L \mathbf{\Lambda}^t \mathbf{Q}_R ^\top
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{Q}_L = \mathbf{D}^{-\frac{1}{2}} \mathbf{Q}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{Q}_R = \mathbf{D}^{\frac{1}{2}} \mathbf{Q}\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Let us demonstrate the above relation by calculating <span class="math notranslate nohighlight">\(\mathbf{P}^2\)</span>.</p>
<p>$$</p>
<div class="amsmath math notranslate nohighlight" id="equation-2570d4b4-8e47-40ff-92f8-0cbda308e2a5">
<span class="eqno">()<a class="headerlink" href="#equation-2570d4b4-8e47-40ff-92f8-0cbda308e2a5" title="Permalink to this equation">#</a></span>\[\begin{align}
\mathbf{P}^2 &amp;= \mathbf{D}^{-\frac{1}{2}} \overline{\bf A} \mathbf{D}^{\frac{1}{2}} \mathbf{D}^{-\frac{1}{2}} \overline{\bf A} \mathbf{D}^{\frac{1}{2}}\\
&amp;=  \mathbf{D}^{-\frac{1}{2}} \overline{\bf A} ^2 \mathbf{D}^{\frac{1}{2}}\\
&amp;= \mathbf{Q}_L \mathbf{\Lambda}^2 \mathbf{Q}_R ^\top
\end{align}\]</div>
</div>
<p>The probability distribution after <span class="math notranslate nohighlight">\(t\)</span> steps is then:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{x}(t) = \mathbf{x}(0) \mathbf{Q}_L \mathbf{\Lambda}^t \mathbf{Q}_R ^\top
\]</div>
<p>We can rewrite this in a more intuitive form:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{pmatrix}
x_1(t) \\
x_2(t) \\
\vdots \\
x_N(t)
\end{pmatrix}
 =
 \sum_{\ell=1}^N
 \left[
 \lambda_\ell^t
 \begin{pmatrix}
 q^{(L)}_{\ell 1} \\
 q^{(L)}_{\ell 2} \\
 \vdots \\
 q^{(L)}_{\ell N}
 \end{pmatrix}
 \langle\mathbf{q}^{(R)}_{\ell},  \mathbf{x}(0) \rangle
 \right]
\end{split}\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Visualize the above equation by using the following figure.</p>
<p><img alt="" src="../figs/diagonalizable-sum.jpg" /></p>
</div>
<p>The term <span class="math notranslate nohighlight">\(\lambda_\ell^t\)</span> represents the contribution of each eigenvalue to the stationary distribution over time. As <span class="math notranslate nohighlight">\(t\)</span> increases, all terms decay exponentially except for the largest eigenvalue (<span class="math notranslate nohighlight">\(\lambda_1 = 1\)</span>). This explains how the random walk converges to the stationary distribution:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\pi_i = \lim_{t\to\infty} x_i(t) = \begin{pmatrix} q^{(L)}_{1 1} \\ q^{(L)}_{1 2} \\ \vdots \\ q^{(L)}_{1 N} \end{pmatrix} \langle\mathbf{q}^{(R)}_{1},  \mathbf{x}(0) \rangle
\end{split}\]</div>
<p>The second largest eigenvalue primarily determines the convergence speed to the stationary distribution. A larger second eigenvalue leads to slower convergence. Thus, the mixing time is closely related to the second largest eigenvalue.</p>
<p>Levin-Peres-Wilmer theorem states that the mixing time is bounded by the relaxation time as</p>
<div class="math notranslate nohighlight">
\[
t_{\text{mix}} &lt; \tau \log \left( \frac{1}{\epsilon \min_{i} \pi_i} \right), \quad \tau = \frac{1}{\lambda_2}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\lambda_2\)</span> is the second largest eigenvalue of the normalized adjacency matrix. The mixing time is known to be bounded by the relaxation time as</p>
<p>More commonly, it is expressed using the second smallest eigenvalue <span class="math notranslate nohighlight">\(\mu\)</span> of the normalized laplacian matrix as</p>
<div class="math notranslate nohighlight">
\[
t_{\text{mix}} \leq \frac{1}{1-\mu}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mu = 1-\lambda_2\)</span>.</p>
<section id="compute-the-mixing-time">
<h3>Compute the mixing time<a class="headerlink" href="#compute-the-mixing-time" title="Link to this heading">#</a></h3>
<p>Let us demonstrate the above math by using the network of two cliques.</p>
<section id="normalized-adjacency-matrix">
<h4>Normalized Adjacency Matrix<a class="headerlink" href="#normalized-adjacency-matrix" title="Link to this heading">#</a></h4>
<p>First, let us construct the normalized adjacency matrix <span class="math notranslate nohighlight">\(\overline{\bf A}\)</span> of the network.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Dinv_sqrt</span> <span class="o">=</span> <span class="n">sparse</span><span class="o">.</span><span class="n">diags</span><span class="p">(</span><span class="mf">1.0</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">deg</span><span class="p">))</span>
<span class="n">A_norm</span> <span class="o">=</span> <span class="n">Dinv_sqrt</span> <span class="o">@</span> <span class="n">A</span> <span class="o">@</span> <span class="n">Dinv_sqrt</span>
</pre></div>
</div>
</div>
</div>
<p>Next, let us compute the eigenvalues and eigenvectors of the normalized adjacency matrix.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">evals</span><span class="p">,</span> <span class="n">evecs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigh</span><span class="p">(</span><span class="n">A_norm</span><span class="o">.</span><span class="n">toarray</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">evals</span></code> and <code class="docutils literal notranslate"><span class="pre">evecs</span></code> are sorted in descending order of the eigenvalues. <code class="docutils literal notranslate"><span class="pre">evecs[:,</span> <span class="pre">0]</span></code> is the eigenvector corresponding to the largest eigenvalue, which is always 1.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>There is a similar function called <code class="docutils literal notranslate"><span class="pre">np.linalg.eig</span></code> which returns the eigenvalues and eigenvectors. It can be used for any matrices, while <code class="docutils literal notranslate"><span class="pre">np.linalg.eigh</span></code> is specifically for symmetric matrices. <code class="docutils literal notranslate"><span class="pre">np.linalg.eigh</span></code> is faster and more stable and therefore preferred if your matrix is symmetric. <code class="docutils literal notranslate"><span class="pre">np.linalg.eig</span></code> is more susceptible to numerical errors and therefore less stable.</p>
</div>
<p>The eigenvalues and eigenvectors are shown below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
    <span class="s2">&quot;Eigenvalue&quot;</span><span class="p">:</span> <span class="n">evals</span>
<span class="p">})</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">background_gradient</span><span class="p">(</span><span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;cividis&#39;</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">set_caption</span><span class="p">(</span><span class="s2">&quot;Eigenvalues of the normalized adjacency matrix&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
    <span class="s2">&quot;Eigenvector </span><span class="si">%i</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">i</span><span class="p">:</span> <span class="n">evecs</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="p">})</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">background_gradient</span><span class="p">(</span><span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;cividis&#39;</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span><span class="o">.</span><span class="n">set_caption</span><span class="p">(</span><span class="s2">&quot;Eigenvectors of the normalized adjacency matrix&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Notice that the largest eigenvalue is 1, which is always true for a normalized adjacency matrix.
The largest eigenvector (the leftmost one) is associated with the stationary distribution of the random walk.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The sign of the eigenvector is indeterminate, which means we can choose the sign of the eigenvector arbitrarily. In fact, <code class="docutils literal notranslate"><span class="pre">np.linalg.eigh</span></code> returns the eigenvector whose sign can vary for a different run.</p>
</div>
<p>We decompose <span class="math notranslate nohighlight">\(\overline{\bf A}\)</span> as</p>
<div class="math notranslate nohighlight">
\[\overline {\bf A} = {\bf Q}{\bf \Lambda}{\bf Q}^\top\]</div>
<p>where <span class="math notranslate nohighlight">\({\bf Q}\)</span> corresponds to <code class="docutils literal notranslate"><span class="pre">eigvecs</span></code> and <span class="math notranslate nohighlight">\({\bf \Lambda}\)</span> corresponds to <code class="docutils literal notranslate"><span class="pre">np.diag(evals)</span></code> (since <span class="math notranslate nohighlight">\({\bf \Lambda}\)</span> is a diagonal matrix). Let‚Äôs see if this is correct:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">A_norm</span><span class="o">.</span><span class="n">toarray</span><span class="p">())</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">background_gradient</span><span class="p">(</span><span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;cividis&#39;</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span><span class="o">.</span><span class="n">set_caption</span><span class="p">(</span><span class="s2">&quot;Normalized Adjacency Matrix&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">A_norm_reconstructed</span> <span class="o">=</span> <span class="n">evecs</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">evals</span><span class="p">)</span> <span class="o">@</span> <span class="n">evecs</span><span class="o">.</span><span class="n">T</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">A_norm_reconstructed</span><span class="p">)</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">background_gradient</span><span class="p">(</span><span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;cividis&#39;</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span><span class="o">.</span><span class="n">set_caption</span><span class="p">(</span><span class="s2">&quot;Reconstruction of the Normalized Adjacency Matrix&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Notice that the reconstruction is not perfect due to the numerical error, although overall the structure is correct.</p>
</section>
<section id="multi-step-transition-probability">
<h4>Multi-step Transition Probability<a class="headerlink" href="#multi-step-transition-probability" title="Link to this heading">#</a></h4>
<p>Let us first conform whether we can compute the transition probability after <span class="math notranslate nohighlight">\(t\)</span> steps by using the eigenvalues and eigenvectors.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">t</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">x_0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">vcount</span><span class="p">())</span>
<span class="n">x_0</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1"># Compute x_t by using the eigenvalues and eigenvectors</span>
<span class="n">Q_L</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="mf">1.0</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">deg</span><span class="p">))</span> <span class="o">@</span> <span class="n">evecs</span>
<span class="n">Q_R</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">deg</span><span class="p">))</span> <span class="o">@</span> <span class="n">evecs</span>
<span class="n">x_t</span> <span class="o">=</span> <span class="n">x_0</span> <span class="o">@</span> <span class="n">Q_L</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">evals</span><span class="o">**</span><span class="n">t</span><span class="p">)</span> <span class="o">@</span> <span class="n">Q_R</span><span class="o">.</span><span class="n">T</span>

<span class="c1"># Compute x_t by using the power iteration</span>
<span class="n">x_t_power</span> <span class="o">=</span> <span class="n">x_0</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
    <span class="n">x_t_power</span> <span class="o">=</span> <span class="n">x_t_power</span> <span class="o">@</span> <span class="n">P</span>

<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
    <span class="s2">&quot;Eigenvector&quot;</span><span class="p">:</span> <span class="n">x_t</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span>
    <span class="s2">&quot;Power iteration&quot;</span><span class="p">:</span> <span class="n">x_t_power</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="p">})</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">background_gradient</span><span class="p">(</span><span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;cividis&#39;</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span><span class="o">.</span><span class="n">set_caption</span><span class="p">(</span><span class="s2">&quot;Comparison of Eigenvector and Power Iteration&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="relaxation-time-and-mixing-time">
<h4>Relaxation Time and Mixing Time<a class="headerlink" href="#relaxation-time-and-mixing-time" title="Link to this heading">#</a></h4>
<p>Let us measure the relaxation time of the random walk.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">evals</span><span class="p">,</span> <span class="n">evecs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigh</span><span class="p">(</span><span class="n">A_norm</span><span class="o">.</span><span class="n">toarray</span><span class="p">())</span>
<span class="n">lambda_2</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="o">-</span><span class="n">evals</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">tau</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">lambda_2</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The relaxation time of the random walk is </span><span class="si">{</span><span class="n">tau</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
<span class="err">```</span><span class="o">---</span>
<span class="n">jupytext</span><span class="p">:</span>
  <span class="n">formats</span><span class="p">:</span> <span class="n">md</span><span class="p">:</span><span class="n">myst</span>
  <span class="n">text_representation</span><span class="p">:</span>
    <span class="n">extension</span><span class="p">:</span> <span class="o">.</span><span class="n">md</span>
    <span class="n">format_name</span><span class="p">:</span> <span class="n">myst</span>
<span class="n">kernelspec</span><span class="p">:</span>
  <span class="n">display_name</span><span class="p">:</span> <span class="n">Python</span> <span class="mi">3</span>
  <span class="n">language</span><span class="p">:</span> <span class="n">python</span>
  <span class="n">name</span><span class="p">:</span> <span class="n">python3</span>
<span class="o">---</span>

<span class="c1"># Random Walks</span>

<span class="n">Suppose</span> <span class="n">you</span> <span class="n">walk</span> <span class="ow">in</span> <span class="n">a</span> <span class="n">city</span><span class="o">.</span> <span class="n">You</span> <span class="n">are</span> <span class="n">drunk</span> <span class="ow">and</span> <span class="n">your</span> <span class="n">feet</span> <span class="n">have</span> <span class="n">no</span> <span class="n">idea</span> <span class="n">where</span> <span class="n">to</span> <span class="n">go</span><span class="o">.</span> <span class="n">You</span> <span class="n">just</span> <span class="n">take</span> <span class="n">a</span> <span class="n">step</span> <span class="n">wherever</span> <span class="n">your</span> <span class="n">feet</span> <span class="n">take</span> <span class="n">you</span><span class="o">.</span> <span class="n">At</span> <span class="n">every</span> <span class="n">intersection</span><span class="p">,</span> <span class="n">you</span> <span class="n">make</span> <span class="n">a</span> <span class="n">random</span> <span class="n">decision</span> <span class="ow">and</span> <span class="n">take</span> <span class="n">a</span> <span class="n">step</span><span class="o">.</span> <span class="n">This</span> <span class="ow">is</span> <span class="n">the</span> <span class="n">core</span> <span class="n">idea</span> <span class="n">of</span> <span class="n">a</span> <span class="n">random</span> <span class="n">walk</span><span class="o">.</span>

<span class="n">While</span> <span class="n">your</span> <span class="n">feet</span> <span class="n">are</span> <span class="n">taking</span> <span class="n">you</span> <span class="n">to</span> <span class="n">a</span> <span class="n">random</span> <span class="n">street</span><span class="p">,</span> <span class="n">after</span> <span class="n">making</span> <span class="n">many</span> <span class="n">steps</span> <span class="ow">and</span> <span class="n">looking</span> <span class="n">back</span><span class="p">,</span> <span class="n">you</span> <span class="n">will</span> <span class="n">realize</span> <span class="n">that</span> <span class="n">you</span> <span class="n">have</span> <span class="n">been</span> <span class="n">to</span> <span class="n">certain</span> <span class="n">places</span> <span class="n">more</span> <span class="n">frequently</span> <span class="n">than</span> <span class="n">others</span><span class="o">.</span> <span class="n">If</span> <span class="n">you</span> <span class="n">were</span> <span class="n">to</span> <span class="nb">map</span> <span class="n">the</span> <span class="n">frequency</span> <span class="n">of</span> <span class="n">your</span> <span class="n">visits</span> <span class="n">to</span> <span class="n">each</span> <span class="n">street</span><span class="p">,</span> <span class="n">you</span> <span class="n">will</span> <span class="n">end</span> <span class="n">up</span> <span class="k">with</span> <span class="n">a</span> <span class="n">distribution</span> <span class="n">that</span> <span class="n">tells</span> <span class="n">you</span> <span class="n">about</span> <span class="n">salient</span> <span class="n">structure</span> <span class="n">of</span> <span class="n">the</span> <span class="n">street</span> <span class="n">network</span><span class="o">.</span> <span class="n">It</span> <span class="ow">is</span> <span class="n">surprising</span> <span class="n">that</span> <span class="n">this</span> <span class="n">seemingly</span> <span class="n">random</span><span class="p">,</span> <span class="n">brainless</span> <span class="n">behavior</span> <span class="n">can</span> <span class="n">tell</span> <span class="n">us</span> <span class="n">something</span> <span class="n">deep</span> <span class="n">about</span> <span class="n">the</span> <span class="n">structure</span> <span class="n">of</span> <span class="n">the</span> <span class="n">city</span><span class="o">.</span>


<span class="o">&lt;</span><span class="n">img</span> <span class="n">src</span><span class="o">=</span><span class="s2">&quot;../figs/random-walk.png&quot;</span> <span class="n">alt</span><span class="o">=</span><span class="s2">&quot;Random walk on a network&quot;</span> <span class="n">width</span><span class="o">=</span><span class="s2">&quot;50%&quot;</span> <span class="n">style</span><span class="o">=</span><span class="s2">&quot;display: block; margin-left: auto; margin-right: auto;&quot;</span><span class="o">&gt;</span>

<span class="c1">## Random walks in a network</span>

<span class="n">A</span> <span class="n">random</span> <span class="n">walk</span> <span class="ow">in</span> <span class="n">undirected</span> <span class="n">networks</span> <span class="ow">is</span> <span class="n">the</span> <span class="n">following</span> <span class="n">process</span><span class="p">:</span>
<span class="mf">1.</span> <span class="n">Start</span> <span class="n">at</span> <span class="n">a</span> <span class="n">node</span> <span class="err">$</span><span class="n">i</span><span class="err">$</span>
<span class="mf">2.</span> <span class="n">Randomly</span> <span class="n">choose</span> <span class="n">an</span> <span class="n">edge</span> <span class="n">to</span> <span class="n">traverse</span> <span class="n">to</span> <span class="n">a</span> <span class="n">neighbor</span> <span class="n">node</span> <span class="err">$</span><span class="n">j</span><span class="err">$</span>
<span class="mf">3.</span> <span class="n">Repeat</span> <span class="n">step</span> <span class="mi">2</span> <span class="n">until</span> <span class="n">you</span> <span class="n">have</span> <span class="n">taken</span> <span class="err">$</span><span class="n">T</span><span class="err">$</span> <span class="n">steps</span><span class="o">.</span>


<span class="err">```</span><span class="p">{</span><span class="n">figure</span><span class="o">-</span><span class="n">md</span><span class="p">}</span> <span class="n">random</span><span class="o">-</span><span class="n">walk</span><span class="o">-</span><span class="n">example</span>

<span class="o">&lt;</span><span class="n">img</span> <span class="n">src</span><span class="o">=</span><span class="s2">&quot;https://d3i71xaburhd42.cloudfront.net/a56ca795f324f75baab70bb3b49e0544c89e05f7/2-Figure1-1.png&quot;</span> <span class="n">alt</span><span class="o">=</span><span class="s2">&quot;Random walk example&quot;</span> <span class="n">width</span><span class="o">=</span><span class="s2">&quot;100%&quot;</span><span class="o">&gt;</span>

<span class="n">Random</span> <span class="n">walk</span> <span class="n">on</span> <span class="n">a</span> <span class="n">small</span> <span class="n">network</span><span class="o">.</span> <span class="n">The</span> <span class="n">figure</span> <span class="ow">is</span> <span class="n">taken</span> <span class="kn">from</span> <span class="nn">Li</span><span class="p">,</span> <span class="n">Xing</span> <span class="n">et</span> <span class="n">al</span><span class="o">.</span> <span class="err">‚Äú</span><span class="n">Representation</span> <span class="n">Learning</span> <span class="n">of</span> <span class="n">Reconstructed</span> <span class="n">Graphs</span> <span class="n">Using</span> <span class="n">Random</span> <span class="n">Walk</span> <span class="n">Graph</span> <span class="n">Convolutional</span> <span class="n">Network</span><span class="o">.</span><span class="err">‚Äù</span> <span class="n">ArXiv</span> <span class="nb">abs</span><span class="o">/</span><span class="mf">2101.00417</span> <span class="p">(</span><span class="mi">2021</span><span class="p">)</span><span class="o">.</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In case of directed networks, a random walker can only move along the edge direction, and it can be that the random walker is stuck in a so-called ``dead end‚Äô‚Äô that does not have any outgoing edges.</p>
</div>
<p>How does this simple process tell us something about the network structure? To get some insights, let us play with a simple interactive visualization.</p>
<div class="tip admonition">
<p class="admonition-title">Random Walk Simulation</p>
<p>Play with the <a class="reference external" href="https://skojaku.github.io/adv-net-sci/vis/random-walks/index.html?">Random Walk Simulator! üéÆ‚ú®</a> and try to answer the following questions:</p>
<ol class="arabic simple">
<li><p>When the random walker makes many steps, where does it tend to visit most frequently?</p></li>
<li><p>When the walker makes only a few steps, where does it tend to visit?</p></li>
<li><p>Does the behavior of the walker inform us about centrality of the nodes?</p></li>
<li><p>Does the behavior of the walker inform us about communities in the network?</p></li>
</ol>
</div>
</section>
</section>
</section>
<hr class="docutils" />
<section id="id1">
<h2>jupytext:
formats: md:myst
text_representation:
extension: .md
format_name: myst
kernelspec:
display_name: Python 3
language: python
name: python3<a class="headerlink" href="#id1" title="Link to this heading">#</a></h2>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="random-walks-unify-centrality-and-communities">
<h1>Random walks unify centrality and communities<a class="headerlink" href="#random-walks-unify-centrality-and-communities" title="Link to this heading">#</a></h1>
<section id="modularity-interpretation-from-random-walk-perspective">
<h2>Modularity: Interpretation from random walk perspective<a class="headerlink" href="#modularity-interpretation-from-random-walk-perspective" title="Link to this heading">#</a></h2>
<p>Modularity can be intepreted as a random walk perspective. Modularity is given by</p>
<div class="math notranslate nohighlight">
\[
Q = \frac{1}{2m} \sum_{ij} \left( A_{ij} - \frac{d_i d_j}{2m} \right) \delta(c_i, c_j)
\]</div>
<p>where <span class="math notranslate nohighlight">\(m\)</span> is the number of edges in the network, <span class="math notranslate nohighlight">\(A_{ij}\)</span> is the adjacency matrix, <span class="math notranslate nohighlight">\(d_i\)</span> is the degree of node <span class="math notranslate nohighlight">\(i\)</span>, <span class="math notranslate nohighlight">\(c_i\)</span> is the community of node <span class="math notranslate nohighlight">\(i\)</span>, and <span class="math notranslate nohighlight">\(\delta(c_i, c_j)\)</span> is the Kronecker delta function (which is 1 if <span class="math notranslate nohighlight">\(c_i = c_j\)</span> and 0 otherwise).</p>
<p>We can rewrite the modularity using the language of random walks as follows.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
Q &amp;= \sum_{ij} \left(\frac{A_{ij}}{2m}  - \frac{d_i}{2m} \frac{d_j}{2m} \right) \delta(c_i, c_j) \\
&amp;= \sum_{ij} \left(\pi_i P_{ij}  - \pi_i \pi_j \right) \delta(c_i, c_j)
\end{aligned}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\pi_i\)</span> is the stationary distribution of the random walk given by</p>
<div class="math notranslate nohighlight">
\[
\pi_i = \frac{d_i}{2m}
\]</div>
<p>and <span class="math notranslate nohighlight">\(P_{ij}\)</span> is the transition probability between nodes <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Let‚Äôs break down this derivation step by step:</p>
<ol class="arabic">
<li><p>We start with the original modularity formula:</p>
<div class="math notranslate nohighlight">
\[Q = \frac{1}{2m} \sum_{ij} \left( A_{ij} - \frac{d_i d_j}{2m} \right) \delta(c_i, c_j)\]</div>
</li>
<li><p>First, we move the constant <span class="math notranslate nohighlight">\(1/(2m)\)</span> to the inside of the parentheses:</p>
<div class="math notranslate nohighlight">
\[Q = \sum_{ij} \left(\frac{A_{ij}}{2m} - \frac{d_i d_j}{2m^2} \right) \delta(c_i, c_j)\]</div>
</li>
<li><p>Now, we recognize that <span class="math notranslate nohighlight">\(\frac{A_{ij}}{2m}\)</span> can be rewritten as:</p>
<div class="math notranslate nohighlight">
\[\frac{A_{ij}}{2m} = \frac{d_i}{2m} \cdot \frac{A_{ij}}{d_i} = \pi_i P_{ij}\]</div>
</li>
<li><p>We also recognize that <span class="math notranslate nohighlight">\(\frac{d_i}{2m}\)</span> is the stationary distribution of the random walk, which we denote as <span class="math notranslate nohighlight">\(\pi_i\)</span>:</p>
<div class="math notranslate nohighlight">
\[\frac{d_i}{2m} = \pi_i\]</div>
</li>
<li><p>Substituting these into our equation:</p>
<div class="math notranslate nohighlight">
\[Q = \sum_{ij} \left(\pi_i P_{ij} - \pi_i \pi_j \right) \delta(c_i, c_j)\]</div>
</li>
</ol>
</div>
<p>The expression suggests that:</p>
<ol class="arabic simple">
<li><p>The first term, <span class="math notranslate nohighlight">\(\pi_i P_{ij} \delta(c_i, c_j)\)</span>, represents the probability that a walker is at node <span class="math notranslate nohighlight">\(i\)</span> and moves to node <span class="math notranslate nohighlight">\(j\)</span> within the same community <strong>by one step</strong>.</p></li>
<li><p>The second term, <span class="math notranslate nohighlight">\(\pi_i \pi_j\)</span>, represents the probability that a walker is at node <span class="math notranslate nohighlight">\(i\)</span> and moves to another node <span class="math notranslate nohighlight">\(j\)</span> within the same community <strong>after long steps</strong>.</p></li>
</ol>
<p>In summary, modularity compares short-term and long-term random walk probabilities. High modularity indicates that a random walker is more likely to stay within the same community after one step than after many steps.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Building on this perspective from random walks, Delvenne et al. <a class="footnote-reference brackets" href="#footcite-delvenne2010stability" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a> extends the modularity by comparing multi-step and long-step transition probabilities of a random walk. This approach, known as ‚ÄúMarkov stability‚Äù, shows that the number of steps acts as a ‚Äúresolution parameter‚Äù that determines the scale of detectable communities.</p>
</div>
</section>
<section id="pagerank-interpretation-from-random-walk-perspective">
<h2>PageRank: Interpretation from random walk perspective<a class="headerlink" href="#pagerank-interpretation-from-random-walk-perspective" title="Link to this heading">#</a></h2>
<p>PageRank can be interpreted from a random walk perspective:</p>
<div class="math notranslate nohighlight">
\[
c_i = (1-\beta) \sum_j P_{ji} c_j + \beta \cdot \frac{1}{N}
\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(c_i\)</span> is the PageRank of node <span class="math notranslate nohighlight">\(i\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P_{ji}\)</span> is the transition probability from node <span class="math notranslate nohighlight">\(j\)</span> to node <span class="math notranslate nohighlight">\(i\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\beta\)</span> is the teleportation probability</p></li>
<li><p><span class="math notranslate nohighlight">\(N\)</span> is the total number of nodes</p></li>
</ul>
<p>This equation represents a random walk where:</p>
<ol class="arabic simple">
<li><p>With probability <span class="math notranslate nohighlight">\((1-\beta)\)</span>, the walker follows a link to the next node.</p></li>
<li><p>With probability <span class="math notranslate nohighlight">\(\beta\)</span>, the walker <em>teleports</em> to a random node in the network.</p></li>
</ol>
<p>The PageRank <span class="math notranslate nohighlight">\(c_i\)</span> is the stationary distribution of this random walk, representing the long-term probability of finding the walker at node <span class="math notranslate nohighlight">\(i\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This sounds odd at first glance. But it makes sense when you think about what PageRank was invented for, i.e., Web search. It characterizes a web surfer as a random walker that chooses the next page by randomly jumping to a random page with probability <span class="math notranslate nohighlight">\(\beta\)</span> or by following a link to a page with probability <span class="math notranslate nohighlight">\(1-\beta\)</span>. The web page with the largest PageRank means that the page is most likely to be visited by this random web surfer.</p>
</div>
<div class="docutils container" id="id3">
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="footcite-delvenne2010stability" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">1</a><span class="fn-bracket">]</span></span>
<p>J-C Delvenne, Sophia¬†N Yaliraki, and Mauricio Barahona. Stability of graph communities across time scales. <em>Proceedings of the national academy of sciences</em>, 107(29):12755‚Äì12760, 2010.</p>
</aside>
</aside>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "skojaku/adv-net-sci",
            ref: "gh-pages",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Ladder Lottery</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-01">Exercise 01</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#expected-behavior-of-random-walks">Expected behavior of random walks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-02">Exercise 02</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-03">Exercise 03</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#community-structure">Community structure</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-04">Exercise 04</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#jupytext-formats-md-myst-text-representation-extension-md-format-name-myst-kernelspec-display-name-python-3-language-python-name-python3">jupytext:
formats: md:myst
text_representation:
extension: .md
format_name: myst
kernelspec:
display_name: Python 3
language: python
name: python3</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#characteristics-of-random-walks">Characteristics of Random Walks</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stationary-state">Stationary State</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#experiment">Experiment</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#time-to-reach-the-stationary-state">Time to reach the stationary state</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#compute-the-mixing-time">Compute the mixing time</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#normalized-adjacency-matrix">Normalized Adjacency Matrix</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-step-transition-probability">Multi-step Transition Probability</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#relaxation-time-and-mixing-time">Relaxation Time and Mixing Time</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">jupytext:
formats: md:myst
text_representation:
extension: .md
format_name: myst
kernelspec:
display_name: Python 3
language: python
name: python3</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#random-walks-unify-centrality-and-communities">Random walks unify centrality and communities</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#modularity-interpretation-from-random-walk-perspective">Modularity: Interpretation from random walk perspective</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pagerank-interpretation-from-random-walk-perspective">PageRank: Interpretation from random walk perspective</a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Sadamori Kojaku
</p>

  </div>
  
  <div class="footer-item">
    

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>