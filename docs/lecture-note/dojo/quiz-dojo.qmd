---
title: Quiz Dojo
filters:
    - marimo-team/marimo
search: false
title-block-banner: ../figs/dojo.jpeg
author: ""
date: ""
---
<style>

#title-block-header {
  margin-block-end: 1rem;
  position: relative;
  margin-top: -1px;
  height: 300px;
}


.quarto-title-banner {
  margin-block-end: 2rem;
  position: relative;
  height: 100%;
}

</style>

[â† Back to Main Page](../index.qmd)

This is an experimental AI-powered quiz practice feature to help you prepare for exams.

::: {.callout-note collapse="true"}
## Instructions & Important Information

**ðŸŒ Access Requirements**: This feature is only available within the Binghamton University campus network. If you're off-campus, please connect to the Binghamton VPN first. Find the VPN setup guide [here](https://www.binghamton.edu/its/about/teams/operations-infrastructure/network_administration/connecting_from_off_campus-ssl-vpn/index.html).

**ðŸš€ Getting Started**:

1. **API Key**: Enter the API key provided by your instructor (contact them if you don't have one)
2. **Select Module**: Choose the course module you want to practice on
3. **Choose Mode**:
   - **Q&A Mode**: Have conversations about concepts and get explanations
   - **Quiz Mode**: Get quizzed with interactive questions and receive feedback
   - **Adversarial Mode**: Create challenging questions to expose AI weaknesses and judge its responses

**ðŸ“š How It Works**: The AI has access to all course materials for the selected module and will provide contextually accurate responses and questions based on the official content.

**âš ï¸ Important Disclaimer**: Always doubt and verify AI responses. The AI may generate plausible-sounding but incorrect answers. Cross-check all information with official course materials, textbooks, or your instructor. Think critically about every response and use this tool as a practice aid, not as an authoritative source.
:::

::: {.callout-warning collapse="true"}
## Troubleshooting: Connection Issues

**ðŸš¨ Common Problem**: If you see errors like "Connection failed", "URLError", or "timeout" when trying to use the quiz, this usually means you're accessing from outside the campus network.

**ðŸ’¡ Solution**: Connect to the Binghamton University VPN first, then try again.

**ðŸ”§ Steps to Fix**:

1. **Connect to BU VPN**: Use the official Binghamton University VPN client
2. **Verify Connection**: Make sure your VPN shows "Connected" status
3. **Refresh & Retry**: Reload this page and try submitting a question again
4. **Still Having Issues?**: Contact your instructor for assistance

**ðŸ“ VPN Resources**:
- [Binghamton University VPN Setup Guide](https://www.binghamton.edu/its/services/network-communications/vpn/)
- Contact BU ITS Help Desk: (607) 777-6420

**âš™ï¸ Technical Details**: The LLM API endpoint is only accessible from within the campus network for security reasons. Off-campus access requires VPN connection to authenticate your location.
:::

```python {.marimo}
import marimo as mo

api_key_holder = mo.ui.text(
    label="Enter the API key",
    placeholder="API key",
    value="",
)

# Module selector
module_options = {
    "intro": "Introduction",
    "m01-euler_tour": "Module 1: Euler Tour",
    "m02-small-world": "Module 2: Small World",
    "m03-robustness": "Module 3: Robustness",
    "m04-friendship-paradox": "Module 4: Friendship Paradox",
    "m05-clustering": "Module 5: Clustering",
    "m06-centrality": "Module 6: Centrality",
    "m07-random-walks": "Module 7: Random Walks",
    "m08-embedding": "Module 8: Embedding",
    "m09-graph-neural-networks": "Module 9: Graph Neural Networks"
}

module_selector = mo.ui.dropdown(
    options=module_options,
    value="intro",
    label="Select module to practice"
)

# Mode selector
mode_selector = mo.ui.radio(
    options=["Q&A Mode", "Quiz Mode", "Adversarial Mode"],
    value="Q&A Mode",
    label="Select interaction mode"
)

mo.vstack([api_key_holder, module_selector, mode_selector])
```

```python {.marimo}

# Function to get example message for selected mode
def get_mode_example():
    selected_mode = mode_selector.value if hasattr(mode_selector, 'value') else "Q&A Mode"

    if selected_mode == "Q&A Mode":
        return "**Prompt Example:** *Help me understand the main ideas this module*"
    elif selected_mode == "Quiz Mode":
        return "**Prompt Example:** *Ask me a free-form question about this module*"
    elif selected_mode == "Adversarial Mode":
        return "**Prompt Example:** *Can a disconnected graph have an Euler circuit?*"
    else:
        return ""

# Display the example message
mo.md(get_mode_example())
```

```python {.marimo}
import urllib.request
import urllib.parse
import json
import os
from typing import List, Dict, Any


def read_module_content(module_name: str) -> Dict[str, str]:
    """Read all markdown files from the selected module via GitHub raw URLs"""
    import urllib.request

    # GitHub repository details
    github_user = "skojaku"
    github_repo = "adv-net-sci"
    github_branch = "main"

    # Define base filenames (without extensions)
    module_files = {
        "intro": ["why-networks", "setup"],
        "m01-euler_tour": ["00-preparation", "01-concepts"],
        "m02-small-world": ["00-preparation", "01-concepts", "03-exercises"],
        "m03-robustness": ["00-preparation", "01-concepts", "03-exercises", "04-appendix"],
        "m04-friendship-paradox": ["00-preparation", "01-concepts", "03-exercises"],
        "m05-clustering": ["00-preparation"],
        "m06-centrality": ["00-preparation", "01-concepts", "03-exercises"],
        "m07-random-walks": ["00-preparation", "03-exercises"],
        "m08-embedding": ["00-preparation", "01-concepts", "03-exercises", "04-appendix"],
        "m09-graph-neural-networks": ["00-preparation", "01-concepts", "03-exercises"]
    }

    # Get files to fetch for this module
    base_filenames = module_files.get(module_name, [])

    content = {}

    # Build base raw URL
    base_raw_url = f"https://raw.githubusercontent.com/{github_user}/{github_repo}/{github_branch}/docs/lecture-note"

    if module_name == "intro":
        module_path = f"{base_raw_url}/intro"
    else:
        module_path = f"{base_raw_url}/{module_name}"

    # Fetch each file, trying both .qmd and .md extensions
    for base_filename in base_filenames:
        file_content = None
        used_filename = None
        
        # Try both extensions
        for ext in [".qmd", ".md"]:
            filename = base_filename + ext
            file_url = f"{module_path}/{filename}"

            try:
                req = urllib.request.Request(file_url)
                req.add_header('User-Agent', 'quiz-dojo-marimo-app')

                with urllib.request.urlopen(req, timeout=10) as response:
                    file_content = response.read().decode('utf-8')
                    used_filename = filename
                    break  # Successfully found file, stop trying extensions

            except urllib.error.HTTPError as e:
                if e.code == 404:
                    # File doesn't exist with this extension, try next
                    continue
                else:
                    # Other HTTP error, record and stop trying
                    content[base_filename + ".md"] = f"Error fetching {filename}: HTTP {e.code}"
                    break
            except Exception as e:
                # Other error, record and stop trying
                content[base_filename + ".md"] = f"Error fetching {filename}: {str(e)}"
                break
        
        # Store the content if we found the file
        if file_content and used_filename:
            content[used_filename] = file_content
        elif base_filename + ".md" not in content:  # Only add error if we haven't already
            content[base_filename + ".md"] = f"File not found: tried {base_filename}.qmd and {base_filename}.md"

    if not content:
        return {"error": f"No content could be loaded for module '{module_name}'. Tried files: {base_filenames}"}

    return content


def format_module_context(content_dict: Dict[str, str], module_name: str) -> str:
    """Format module content for LLM context"""
    if "error" in content_dict:
        return f"Error loading module content: {content_dict['error']}"

    context = f"=== COURSE MODULE CONTEXT: {module_name.upper().replace('-', ' ')} ===\n\n"

    # Order files by importance
    file_order = ['00-preparation.md', '01-concepts.md', '03-exercises.md']

    # Add ordered files first
    for filename in file_order:
        if filename in content_dict:
            context += f"--- {filename.replace('.md', '').replace('-', ' ').title()} ---\n"
            context += content_dict[filename] + "\n\n"

    # Add remaining files
    for filename, content_text in content_dict.items():
        if filename not in file_order:
            context += f"--- {filename.replace('.md', '').replace('-', ' ').title()} ---\n"
            context += content_text + "\n\n"

    context += "=== END MODULE CONTEXT ===\n\n"
    return context


def custom_llm_api(messages, config, module_context=None, mode="Q&A Mode") -> str:
    """
    Custom LLM function that calls an OpenAI-compatible API endpoint
    """
    # Your API configuration
    base_url = "https://chat.binghamton.edu/api"
    api_key = api_key_holder.value

    # Use different models based on mode
    if mode == "Adversarial Mode":
        model = "llama3.2:latest"
    else:
        model = "gemma3:27b"

    # Use config parameters if available
    temperature = getattr(config, "temperature", 0.1)
    max_tokens = getattr(config, "max_tokens", 5000)

    # Convert marimo ChatMessage objects to OpenAI format
    openai_messages = []

    # Add module context as system message if provided
    if module_context:
        if mode == "Adversarial Mode":
            system_prompt = f"""You are an AI assistant answering questions about an Advanced Network Science course. You have been provided with the module content below. Your role is to answer questions posed by the user, who will then evaluate whether your answers are correct.

{module_context}

Instructions for Adversarial Mode:
- Answer questions based on the module content provided above
- Try to be helpful but don't be overly cautious - give definitive answers even when uncertain
- You may occasionally make mistakes, especially with:
  - Complex mathematical details
  - Edge cases or boundary conditions
  - Subtle distinctions between similar concepts
  - Specific numerical values or exact formulas
- Don't hedge too much with phrases like "I think" or "probably"
- Use proper syntax highlighting for code: ```python for Python, ```r for R, etc.
- When including mathematical formulas, use LaTeX notation: $inline math$ or $$display math$$
- The user is testing your knowledge and will provide corrections if needed"""
        elif mode == "Quiz Mode":
            system_prompt = f"""You are a quiz creator and evaluator for an Advanced Network Science course. You have been provided with the complete content for the selected module below. Your primary role is to create challenging quiz questions and evaluate student responses.

{module_context}

Instructions for Quiz Mode:
- STOP! READ THIS CAREFULLY: You must ask EXACTLY ONE question and STOP.
- DO NOT create multiple questions (like 1., 2., 3.)
- DO NOT provide answer keys or solutions
- DO NOT give the answer in your response
- NEVER use phrases like "Here are some questions" or "Question 1, Question 2"

REQUIRED FORMAT:
1. Ask ONE single question
2. Say "Please provide your answer, and I'll give you feedback."
3. STOP and wait for user response

After user responds, THEN provide feedback with âœ… or âŒ
- DEFAULT to free-form, open-ended questions that require explanation and critical thinking
- Only use multiple choice if specifically requested by the user
- When providing code examples, use proper syntax highlighting: ```python for Python, ```r for R, etc.
- When including mathematical formulas, use LaTeX notation: $inline math$ or $$display math$$
- Focus on interactive learning through single question-answer cycles

EXAMPLE CORRECT BEHAVIOR:
User: "Ask me a question about Euler paths"
You: "What is the difference between an Euler path and an Euler circuit? Please provide your answer, and I'll give you feedback!"

EXAMPLE INCORRECT BEHAVIOR (ABSOLUTELY DO NOT DO THIS):
User: "Ask me a question about Euler paths"
You: "Here are some quiz questions: 1) What is an Euler path? 2) What is an Euler circuit? ANSWER KEY: 1) A path that visits every edge exactly once..."

THIS IS WRONG! You violated EVERY rule:
- Multiple questions (1, 2)
- Provided answer key
- Did not wait for user response

CORRECT VERSION:
You: "What is an Euler path? Please provide your answer, and I'll give you feedback!\""""
        else:  # Q&A Mode
            system_prompt = f"""You are a helpful teaching assistant for an Advanced Network Science course. You have been provided with the complete content for the selected module below. Use this content to answer questions accurately and help students understand the material.

{module_context}

Instructions for Q&A Mode:
- ALWAYS build intuitive understanding FIRST before any technical details
- Use analogies, real-world examples, and simple language to explain concepts
- Keep responses conversational and focused - typically 2-4 sentences
- Use conversational tone like talking to a study buddy
- Only add mathematical formulas or technical details AFTER the intuitive explanation is clear
- When students ask for "more detail" or "rigorous explanation":
  1. First strengthen the intuitive explanation with better analogies/examples
  2. Then add technical/mathematical details (formulas, precise definitions)
- Ask ONE focused follow-up question to continue dialogue
- When providing code examples, use proper syntax highlighting: ```python for Python, ```r for R, etc.
- When including mathematical formulas, use LaTeX notation: $inline math$ or $$display math$$
- Think: "Can a non-expert understand the core idea from my explanation?"

EXAMPLES:
Student: "What is a small world network?"
Good response: "Think of your friend group - you have tight circles of friends, but through just a few connections you can reach anyone in the world (like 6 degrees of separation). That's a small world network! Want to explore how this works?"

Student: "Can you explain small world networks in detail?"
Good response: "Imagine a neighborhood where everyone knows their immediate neighbors well (high clustering), but there are a few 'shortcut' connections to distant neighborhoods that make the whole city feel small. In network terms, this means we keep strong local clustering while maintaining short paths globally through these strategic long-range connections. Mathematically, we measure this as high clustering coefficient $C$ but low average path length $L$."

Bad response: [Starting with formulas or technical definitions without building intuition first]"""

        openai_messages.append({
            "role": "system",
            "content": system_prompt
        })

    for msg in messages:
        # Handle both dict and ChatMessage objects
        if hasattr(msg, 'role') and hasattr(msg, 'content'):
            openai_messages.append({
                "role": msg.role,
                "content": msg.content
            })
        elif isinstance(msg, dict) and 'role' in msg and 'content' in msg:
            openai_messages.append({
                "role": msg["role"],
                "content": msg["content"]
            })

    payload = {
        "model": model,
        "messages": openai_messages,
        "temperature": temperature,
        "max_tokens": max_tokens,
        "stream": False
    }

    try:
        # Prepare the request
        url = f"{base_url}/chat/completions"
        data = json.dumps(payload).encode('utf-8')

        req = urllib.request.Request(
            url,
            data=data,
            headers={
                "Content-Type": "application/json",
                "Authorization": f"Bearer {api_key}"
            }
        )

        # Make the request
        with urllib.request.urlopen(req, timeout=30) as response:
            result = json.loads(response.read().decode('utf-8'))

            # Extract the assistant's response
            if "choices" in result and len(result["choices"]) > 0:
                return result["choices"][0]["message"]["content"]
            else:
                return "Error: No response from LLM. Make sure you have entered the correct API key."

    except urllib.error.URLError as e:
        error_msg = str(e)
        if "timed out" in error_msg.lower() or "connection" in error_msg.lower():
            return "ðŸš¨ **Connection Error**: Cannot reach the API server. This usually happens when you're off-campus.\n\n**ðŸ’¡ Solution**: Please connect to the Binghamton University VPN and try again.\n\n**ðŸ”— VPN Setup**: https://www.binghamton.edu/its/about/teams/operations-infrastructure/network_administration/connecting_from_off_campus-ssl-vpn/index.html"
        else:
            return f"Error: Connection failed - {error_msg}"
    except json.JSONDecodeError:
        return "Error: Invalid JSON response from server. Please try again or contact your instructor if the issue persists."
    except Exception as e:
        error_msg = str(e)
        if "timeout" in error_msg.lower() or "connection" in error_msg.lower():
            return "ðŸš¨ **Network Timeout**: The connection to the API server timed out. This often indicates you're accessing from off-campus.\n\n**ðŸ’¡ Solution**: Connect to the Binghamton University VPN and try again."
        else:
            return f"Error: {error_msg}"

def llm_wrapper(messages, config):
    """
    Wrapper function for marimo chat with module context
    """
    # Get the selected module
    selected_module = module_selector.value

    # Debug: Add some error checking
    if not selected_module:
        return custom_llm_api(messages, config)

    # Make sure we're using the correct module key
    # In case the dropdown returns the display name, map it back to the key
    module_key_map = {v: k for k, v in module_options.items()}

    if selected_module in module_key_map:
        # It's a display name, get the key
        module_key = module_key_map[selected_module]
    elif selected_module in module_options:
        # It's already a key
        module_key = selected_module
    else:
        # Unknown module
        return custom_llm_api(messages, config)

    # Read module content and format context
    module_content = read_module_content(module_key)
    module_context = format_module_context(module_content, module_key)

    # Get the selected mode
    selected_mode = mode_selector.value if hasattr(mode_selector, 'value') else "Q&A Mode"

    # Pass context and mode to custom_llm_api
    return custom_llm_api(messages, config, module_context, selected_mode)

# Module-specific prompts based on selection
def get_module_prompts():
    selected = module_selector.value
    if not selected:
        return [
            "Hello! How can you help me today?",
            "Select a module above to get started"
        ]

    # Get the correct module key (same logic as llm_wrapper)
    module_key_map = {v: k for k, v in module_options.items()}

    if selected in module_key_map:
        module_key = module_key_map[selected]
        module_display = selected
    elif selected in module_options:
        module_key = selected
        module_display = module_options[selected]
    else:
        return ["Error: Unknown module selected"]

    # Get the selected mode
    selected_mode = mode_selector.value if hasattr(mode_selector, 'value') else "Q&A Mode"

    if selected_mode == "Adversarial Mode":
        base_prompts = [
            f"ðŸŽ¯ Test the AI: Ask about edge cases in {module_display}",
            f"ðŸŽ¯ Challenge: Ask for exact numerical values or formulas",
            f"ðŸŽ¯ Trick question: Mix up similar concepts from {module_display}",
            f"ðŸŽ¯ Advanced: Ask about mathematical proofs or derivations",
            f"ðŸŽ¯ Precision test: Ask for specific parameter values or constants",
            f"ðŸŽ¯ Boundary testing: When does {{algorithm}} fail or break down?",
            f"ðŸŽ¯ Adversarial: Create a misleading or ambiguous question about {{concept}}",
            f"ðŸŽ¯ Counter-example: Ask about exceptions to general rules in {module_display}",
            f"ðŸŽ¯ Gotcha: Test common misconceptions about {{topic}}",
        ]
    elif selected_mode == "Quiz Mode":
        base_prompts = [
            f"Ask me a free-form question about {module_display}",
            f"Give me an open-ended question for {module_display}",
            f"Test me with a problem-solving question from {module_display}",
            "Quiz me on a key concept from this module",
            "Ask me a question about {{concept}} and wait for my answer",
            "Test my understanding of {{algorithm}}",
            "Give me a challenging question from this module",
            "Ask me a multiple choice question about {{topic}}",
            "Check my answer to your previous question",
        ]
    else:  # Q&A Mode
        base_prompts = [
            f"Let's discuss the key concepts in {module_display}",
            f"Help me understand the main ideas in {module_display}",
            "Walk me through a practice problem from this module",
            "Can you explain {{concept}} and help me think through it?",
            "I'm curious about {{algorithm}} - can we explore how it works?",
            "What real-world applications should I know about from this module?",
            "I have a question about this module...",
        ]

    # Add module-specific prompts
    if selected_mode == "Adversarial Mode":
        module_specific = {
            "intro": [
                "ðŸŽ¯ Trick: Are all networks the same type of mathematical object?",
                "ðŸŽ¯ Edge case: What's the smallest possible network?"
            ],
            "m01-euler_tour": [
                "ðŸŽ¯ Gotcha: Can a disconnected graph have an Euler circuit?",
                "ðŸŽ¯ Precision: What's the exact degree requirement for Euler paths?"
            ],
            "m02-small-world": [
                "ðŸŽ¯ Challenge: What's the exact clustering coefficient formula?",
                "ðŸŽ¯ Boundary: When does the small-world model break down?"
            ],
            "m03-robustness": [
                "ðŸŽ¯ Counter: Give an example where targeted attacks fail",
                "ðŸŽ¯ Edge case: What happens with single-node networks?"
            ],
            "m04-friendship-paradox": [
                "ðŸŽ¯ Trick: Does friendship paradox apply to directed graphs?",
                "ðŸŽ¯ Exception: When might the friendship paradox not hold?"
            ],
            "m05-clustering": [
                "ðŸŽ¯ Advanced: Prove the modularity optimization is NP-hard",
                "ðŸŽ¯ Gotcha: Can modularity be negative?"
            ],
            "m06-centrality": [
                "ðŸŽ¯ Precision: What's the exact eigenvector centrality formula?",
                "ðŸŽ¯ Edge case: Centrality in disconnected graphs?"
            ],
            "m07-random-walks": [
                "ðŸŽ¯ Challenge: Prove PageRank convergence conditions",
                "ðŸŽ¯ Boundary: When do random walks not converge?"
            ],
            "m08-embedding": [
                "ðŸŽ¯ Technical: What's the exact spectral embedding algorithm?",
                "ðŸŽ¯ Limitation: When do embeddings lose information?"
            ],
            "m09-graph-neural-networks": [
                "ðŸŽ¯ Deep: Derive the graph convolution backpropagation",
                "ðŸŽ¯ Failure: When do GNNs perform poorly?"
            ]
        }
    elif selected_mode == "Quiz Mode":
        module_specific = {
            "intro": [
                "Quiz me on why we study networks",
                "Ask me about real-world network examples"
            ],
            "m01-euler_tour": [
                "Test me on the Seven Bridges of KÃ¶nigsberg problem",
                "Quiz me on determining Euler paths"
            ],
            "m02-small-world": [
                "Ask me about the small-world phenomenon",
                "Test me on the Milgram experiment"
            ],
            "m03-robustness": [
                "Quiz me on measuring network robustness",
                "Ask me to compare random vs targeted attacks"
            ],
            "m04-friendship-paradox": [
                "Test me on the friendship paradox",
                "Quiz me on degree centrality and the friendship paradox"
            ],
            "m05-clustering": [
                "Ask me about community detection",
                "Test me on modularity in clustering"
            ],
            "m06-centrality": [
                "Quiz me on comparing centrality measures",
                "Ask me when to use different centrality measures"
            ],
            "m07-random-walks": [
                "Test me on random walks on networks",
                "Quiz me on the PageRank algorithm"
            ],
            "m08-embedding": [
                "Ask me about network embeddings",
                "Test me on spectral vs neural embeddings"
            ],
            "m09-graph-neural-networks": [
                "Quiz me on Graph Neural Networks",
                "Ask me about graph convolution operations"
            ]
        }
    else:  # Q&A Mode
        module_specific = {
            "intro": [
                "Help me understand why we study networks",
                "Let's explore some real-world network examples together"
            ],
            "m01-euler_tour": [
                "Can we walk through the Seven Bridges of KÃ¶nigsberg problem?",
                "I'm confused about Euler paths - can you help me understand?"
            ],
            "m02-small-world": [
                "What's interesting about the small-world phenomenon?",
                "Tell me about the Milgram experiment and what we learned"
            ],
            "m03-robustness": [
                "How do we actually measure if a network is robust?",
                "What's the difference between random and targeted attacks?"
            ],
            "m04-friendship-paradox": [
                "The friendship paradox sounds confusing - can you explain it?",
                "How does degree centrality connect to the friendship paradox?"
            ],
            "m05-clustering": [
                "What's the big deal about community detection?",
                "Help me understand modularity and why it matters"
            ],
            "m06-centrality": [
                "I'm trying to understand different centrality measures",
                "When should I use betweenness vs eigenvector centrality?"
            ],
            "m07-random-walks": [
                "How do random walks actually work on networks?",
                "Can you walk me through how PageRank works?"
            ],
            "m08-embedding": [
                "What are network embeddings and why do we need them?",
                "Help me compare spectral vs neural embedding approaches"
            ],
            "m09-graph-neural-networks": [
                "I'm curious about how Graph Neural Networks work",
                "Can you explain graph convolution operations?"
            ]
        }

    if module_key in module_specific:
        base_prompts.extend(module_specific[module_key])

    return base_prompts

# Create the chat interface
chat = mo.ui.chat(
    llm_wrapper,
    prompts=get_module_prompts(),
    show_configuration_controls=True,
    allow_attachments=False
)

# Display module status and chat
selected_module_name = module_options.get(module_selector.value, "None") if module_selector.value else "None"
selected_mode = mode_selector.value if hasattr(mode_selector, 'value') else "Q&A Mode"

# Show which model is being used
current_model = "llama3.2:latest" if selected_mode == "Adversarial Mode" else "gemma3:27b"
status_text = f"**Selected Module:** {selected_module_name} | **Mode:** {selected_mode} | **Model:** {current_model}"

mo.vstack([
    mo.md(status_text),
    chat
])
```