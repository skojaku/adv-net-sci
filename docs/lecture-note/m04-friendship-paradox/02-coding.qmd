---
title: "Analyzing Degree Distributions and the Friendship Paradox"
jupyter: advnetsci
execute:
    enabled: true
---

<a target="_blank" href="https://colab.research.google.com/github/skojaku/adv-net-sci/blob/main/notebooks/exercise-m04-friendship-paradox.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>

![](https://barabasi.com/img/6/159.png)

Understanding degree distribution is the first key step to understand networks! In this coding session, we'll learn how to compute, analyze, and visualize degree distributions effectively using practical examples.

## Visualization basics

To learn the basics of data visualization, please take a [pen and paper exercise](./pen-and-paper/exercise.pdf).

## Coding exercise

[Exercise: Plotting degree distribution](https://github.com/skojaku/adv-net-sci/blob/main/notebooks/exercise-m04-friendship-paradox.ipynb)



### Computing degree distribution

(The following content includes the answer to the exercise. So please do the exercise first before reading the following content.)

Let's compute the degree distribution of a network using practical implementation. We'll create a BarabÃ¡si-Albert network with $N=10,000$ nodes and $m=1$ edge per node to demonstrate the techniques.

```{python}
import igraph
g = igraph.Graph.Barabasi(n = 10000, m = 1) # Create a BarabÃ¡si-Albert network
A = g.get_adjacency() # Get the adjacency matrix
```

Compute the degree of each node by summing the elements of the adjacency matrix along the rows.

```{python}
import numpy as np
deg = np.sum(A, axis=1)
deg = deg.flatten()
```

The degree distribution $p(k)$ can be computed by counting the number of nodes with each degree and dividing by the total number of nodes.

```{python}
p_deg = np.bincount(deg) / len(deg)
```

Let us plot the degree distribution. This is not as trivial as you might think... ğŸ¤”

```{python}
import seaborn as sns
import matplotlib.pyplot as plt

ax = sns.lineplot(x=np.arange(len(p_deg)), y=p_deg)
ax.set_xlabel('Degree')
ax.set_ylabel('Probability')
```

While it clearly shows that most nodes have small degree, it does not show the tail of the distribution clearly, and often it is this tail that is of great interest (e.g., hub nodes). To show the tail of the distribution more clearly, we can use a log-log plot.

```{python}
ax = sns.lineplot(x=np.arange(len(p_deg)), y=p_deg)
ax.set_xscale('log')
ax.set_yscale('log')
ax.set_ylim(np.min(p_deg[p_deg>0])*0.01, None)
ax.set_xlabel('Degree')
ax.set_ylabel('Probability')
```

We see fluctuations for large degree nodes because of the small number of nodes with large degree.
One can use "binning" to smooth the plot. Binning involves grouping the data into bins and calculating the fraction of data within each bin. However, selecting an appropriate bin size can be challenging, and even with a well-chosen bin size, some information may be lost.

A more convenient way is to use the complementary cumulative distribution function (CCDF).
The CCDF at degree $k$ is the probability that a randomly chosen node has degree $k'$ greater than $k$ ($k' > k$).  For a visual comparison of CCDF and PDF, see Figure 3 in {footcite}`newman2005power` or [the arxiv version](https://arxiv.org/pdf/cond-mat/0412004)

$$
\text{CCDF}(k) = P(k' > k) = \sum_{k'=k+1}^\infty p(k')
$$

The CCDF offers several computational advantages: it's a monotonically decreasing function of $k$ that provides smooth visualization, encompasses the full information of $p(k)$ (since taking the derivative of CCDF gives $p(k)$), and can be plotted as a smooth curve on a log-log scale without requiring binning decisions.

```{python}
ccdf_deg = 1 - np.cumsum(p_deg)[:-1] # 1 - CDF (cumulative distribution function).
# The last element is excluded because it is always 1, resulting in CCDF=0, which cannot be plotted on a log-log scale.

ax = sns.lineplot(x=np.arange(len(ccdf_deg)), y=ccdf_deg)
ax.set_xscale('log')
ax.set_yscale('log')
ax.set_xlabel('Degree')
ax.set_ylabel('CCDF')
```

```{python}
:tags: [remove-cell]
from myst_nb import glue

cdf_deg = np.cumsum(p_deg)
fig, ax = plt.subplots(figsize=(3,3))
ax = sns.lineplot(x=np.arange(len(cdf_deg)), y=cdf_deg, ax = ax)
ax.set_xscale('log')
ax.set_yscale('log')
ax.set_xlabel('Degree')
ax.set_ylabel('CDF')
glue("cdf_fig", fig, display=False)
```
::: {.callout-note}
CCDF (complementary cumulative distribution function) is used instead of CDF (cumulative distribution function) because it highlights the tail of the distribution better in a log-log plot. A log scale expands small values and compresses large values. In a CDF, large degree nodes have values close to 1, compressing the tail. In a CCDF, large degree nodes have small values, making the tail more visible.
```{glue} cdf_fig
:align: center
```
:::

**Interpreting the CCDF slope:** A **steep slope** indicates a more homogeneous degree distribution where nodes have similar degrees, while a **flat slope** indicates a more heterogeneous degree distribution with a wide range of degrees.

The CCDF in a log-log plot provides a convenient visual summary of the degree distribution, with the slope providing a measure of the heterogeneity.


## Degree distribution of a friend

Continuing from the previous page, we will now consider the degree distribution of a friend of a node.

There are two ways to sample a friend of a node.
1. Sample a node uniformly at random and then sample a friend of the node.
2. Sample a *friendship* (i.e., edge) uniformly at random and then sample an end node of the edge.

Let us focus on the second case and leave the first case for interested students as an exercise.
In the second case, we sample an edge from the network.
This sampling is biased towards nodes with many edges, i.e., a person with $d$ edges is $d$ times more likely to be sampled than someone with 1 edge.
Thus, the degree distribution $p'(k)$ of a friend is given by

$$
p' (k) = C \cdot k \cdot p(k)
$$
The additional term $k$ reflects the fact that a person with $k$ friends is $k$ times more likely to be sampled than someone with 1 friend.
Term $C$ is the normalization constant that ensures the sum of probabilities $p'(k)$ over all $k$ is 1, which can be easily computed as follows:

$$
C = \frac{1}{\sum_{k} k \cdot p(k)} = \frac{1}{\langle k \rangle}
$$

where $\langle k \rangle$ is the average degree of the network. Substituting $C$ into $p'(k)$, we get:

$$
p' (k) = \frac{k}{\langle k \rangle} p(k)
$$

This is the degree distribution of a friend, and it is easy to verify that the average degree of a friend is given by

$$
\langle k' \rangle = \sum_{k} k \cdot p'(k) = \sum_{k} k \cdot \frac{k}{\langle k \rangle} p(k) = \frac{\langle k^2 \rangle}{\langle k \rangle}
$$

which is always larger than $\langle k \rangle$:

$$
\langle k' \rangle = \frac{\langle k^2 \rangle}{\langle k \rangle} \geq \langle k \rangle
$$

with equality only if every node has the same degree. This is a proof of the friendship paradox ğŸ˜‰!


::: {.callout-note}
The distribution $p'(k)$ is related to *the excess degree distribution* given by

$$
q(k) = \frac{k + 1}{\langle k \rangle} p(k+1)
$$

The term *excess* comes from the fact that the distribution represents the number of additional connections a randomly chosen friend has, beyond the connection that led to their selection. It excludes the link to the focal node and focuses on the remaining connections of the selected friend.
:::

::: {.callout-note}
The friend's degree, $\frac{\langle k^2 \rangle}{\langle k \rangle}$, concides with a term in Molloy-Reed condition:

$$

\frac{\langle k^2 \rangle}{\langle k \rangle} >2

$$

which is a condition for the existence of a giant component in a network. The Molloy-Reed condition states that the average degree of a node's friends must be at least 2 (the inequality is strict because the transition from a small component to a giant component is discontinuous). If a friend has only one edge, you and your friend form an isolated component. If a friend has two edges on average, your friend is a friend of someone else, and that someone else is also friend of another someone else and so on, forming a giant component.

:::

## Plotting degree distribution of a friend

Let us compare the degree distribution of a node and its friend.
We first get the edges in the network, from which we sample a friend.

```{python}
from scipy import sparse
src, trg, _ = sparse.find(A)
```
The `sparse.find(A)` function returns the source node, target node, and edge weight of each edge. Here, `src` contains the source nodes, `trg` contains the target nodes, and we use `_` to ignore the edge weight values since we only need the source and target nodes for our friendship paradox analysis.

Now, let us get the degree of each friend

```{python}
deg_friend = deg[src]
p_deg_friend = np.bincount(deg_friend) / len(deg_friend)
```

The CCDF of the degree distributions of a node and a friend can be computed by:

```{python}
ccdf_deg = 1 - np.cumsum(p_deg)[:-1]
ccdf_deg_friend = 1 - np.cumsum(p_deg_friend)[:-1]
```
and plotted by:

```{python}
import seaborn as sns
import matplotlib.pyplot as plt


ax = sns.lineplot(x=np.arange(len(ccdf_deg)), y=ccdf_deg, label='Node')
ax = sns.lineplot(x=np.arange(len(ccdf_deg)), y=ccdf_deg_friend, label='Friend', ax = ax)
ax.set_xscale('log')
ax.set_yscale('log')
ax.set_xlabel('Degree')
ax.set_ylabel('CCDF')
ax.legend(frameon = False)
```

The slope of the CCDF of a friend is flatter than that of a node, indicating that the degree distribution of a friend is biased towards higher degrees.

## Exercise 01 ğŸ‹ï¸â€â™€ï¸ğŸ’ªğŸ§ 

Let's implement a complete friendship paradox analysis from scratch:

1. Create different types of networks (ErdÅ‘sâ€“RÃ©nyi, BarabÃ¡si-Albert, Watts-Strogatz) with the same number of nodes.
2. For each network, compute both the degree distribution of nodes and their friends.
3. Calculate the average degree for nodes vs. friends to verify the friendship paradox.
4. Create comparative CCDF plots to visualize the difference.

```{python}
# Your implementation here
```

## Exercise 02 ğŸ‹ï¸â€â™€ï¸ğŸ’ªğŸ§  

Apply the friendship paradox to real data:

1. Select a network dataset from [Netzschleuder](https://networks.skewed.de/) (choose one with < 5000 nodes for efficiency).
2. Load the data and create the network.
3. Implement the friendship paradox analysis.
4. Determine how much stronger the effect is in real networks compared to random graphs.

**Hint:** Use `pandas` to load the CSV data and extract source/target columns for the edge list.

```{python}
# Your implementation here
```

## Comparative Network Analysis

Let's compare how the friendship paradox manifests across different network types:

```{python}
import igraph
import numpy as np
import matplotlib.pyplot as plt

# Create different network types
n_nodes = 1000

networks = {
    "ErdÅ‘sâ€“RÃ©nyi": igraph.Graph.Erdos_Renyi(n_nodes, 0.01),
    "BarabÃ¡si-Albert": igraph.Graph.Barabasi(n_nodes, m=5),
    "Watts-Strogatz": igraph.Graph.Watts_Strogatz(dim=1, size=n_nodes, nei=10, p=0.1),
}

print("Friendship Paradox Analysis:")
print("-" * 60)
print(f"{'Network':<15} {'Avg Node Deg':<12} {'Avg Friend Deg':<15} {'Paradox Ratio':<12}")
print("-" * 60)

for name, g in networks.items():
    # Node degrees
    degrees = np.array(g.degree())
    avg_node_deg = np.mean(degrees)
    
    # Friend degrees (edge-based sampling)
    A = g.get_adjacency()
    from scipy import sparse
    src, trg, _ = sparse.find(A)
    friend_degrees = degrees[src]
    avg_friend_deg = np.mean(friend_degrees)
    
    ratio = avg_friend_deg / avg_node_deg
    
    print(f"{name:<15} {avg_node_deg:<12.2f} {avg_friend_deg:<15.2f} {ratio:<12.2f}")
```

::: {.column-margin}
**Understanding the results:** A ratio greater than 1 confirms the friendship paradox is present. Higher ratios indicate stronger paradox effects, and different network structures show varying paradox strengths due to their degree distribution properties.
:::

This analysis reveals how network structure influences the strength of the friendship paradox, with scale-free networks (BarabÃ¡si-Albert) typically showing the strongest effect due to their highly skewed degree distributions.