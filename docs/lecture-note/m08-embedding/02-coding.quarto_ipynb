{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Network Embedding Concepts\"\n",
        "jupyter: advnetsci\n",
        "execute:\n",
        "    enabled: true\n",
        "---\n",
        "\n",
        "In this section, we implement the embedding methods discussed in the [concepts section](./01-concepts.md).\n",
        "\n",
        "## Data Preparation\n",
        "\n",
        "We will use the karate club network throughout this notebook."
      ],
      "id": "8898bd4b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import igraph\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the karate club network\n",
        "g = igraph.Graph.Famous(\"Zachary\")\n",
        "A = g.get_adjacency_sparse()\n",
        "\n",
        "# Get community labels (Mr. Hi = 0, Officer = 1)\n",
        "labels = np.array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
        "g.vs[\"label\"] = labels\n",
        "\n",
        "# Visualize the network\n",
        "palette = sns.color_palette().as_hex()\n",
        "igraph.plot(g, vertex_color=[palette[label] for label in labels], bbox=(300, 300))"
      ],
      "id": "b7c4281c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Spectral Embedding\n",
        "\n",
        "### Example: Spectral Embedding with Adjacency Matrix\n",
        "\n",
        "Let us demonstrate spectral embedding with the karate club network."
      ],
      "id": "c542ec72"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Convert to dense array for eigendecomposition\n",
        "A_dense = A.toarray()"
      ],
      "id": "588efeae",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Compute the spectral decomposition\n",
        "eigvals, eigvecs = np.linalg.eig(A_dense)\n",
        "\n",
        "# Find the top d eigenvectors\n",
        "d = 2\n",
        "sorted_indices = np.argsort(eigvals)[::-1][:d]\n",
        "eigvals = eigvals[sorted_indices]\n",
        "eigvecs = eigvecs[:, sorted_indices]\n",
        "\n",
        "# Plot the results\n",
        "fig, ax = plt.subplots(figsize=(7, 5))\n",
        "sns.scatterplot(x = eigvecs[:, 0], y = eigvecs[:, 1], hue=labels, ax=ax)\n",
        "ax.set_title('Spectral Embedding')\n",
        "ax.set_xlabel('Eigenvector 1')\n",
        "ax.set_ylabel('Eigenvector 2')\n",
        "plt.show()"
      ],
      "id": "bd49034a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Interestingly, the first eigenvector corresponds to the eigencentrality of the network, representing the centrality of the nodes.\n",
        "The second eigenvector captures the community structure of the network, clearly separating the two communities in the network.\n",
        "\n",
        "### Example: Modularity Embedding\n",
        "\n",
        "We can use the modularity matrix to generate a low-dimensional embedding of the network."
      ],
      "id": "8ce00658"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "deg = np.sum(A_dense, axis=1)\n",
        "m = np.sum(deg) / 2\n",
        "Q = A_dense - np.outer(deg, deg) / (2 * m)\n",
        "Q/= 2*m\n",
        "\n",
        "eigvals, eigvecs = np.linalg.eig(Q)\n",
        "\n",
        "# Sort the eigenvalues and eigenvectors\n",
        "sorted_indices = np.argsort(-eigvals)[:d]\n",
        "eigvals = eigvals[sorted_indices]\n",
        "eigvecs = eigvecs[:, sorted_indices]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(7, 5))\n",
        "sns.scatterplot(x = eigvecs[:, 0], y = eigvecs[:, 1], hue=labels, ax=ax)\n",
        "ax.set_title('Modularity Embedding')\n",
        "ax.set_xlabel('Eigenvector 1')\n",
        "ax.set_ylabel('Eigenvector 2')\n",
        "plt.show()"
      ],
      "id": "1801dc3b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example: Laplacian Eigenmap\n",
        "\n",
        "Let us first compute the Laplacian matrix and its eigenvectors."
      ],
      "id": "77f0e825"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "D = np.diag(np.sum(A_dense, axis=1))\n",
        "L = D - A_dense\n",
        "\n",
        "eigvals, eigvecs = np.linalg.eig(L)\n",
        "\n",
        "# Sort the eigenvalues and eigenvectors\n",
        "sorted_indices = np.argsort(eigvals)[1:d+1]  # Exclude the first eigenvector\n",
        "eigvals = eigvals[sorted_indices]\n",
        "eigvecs = eigvecs[:, sorted_indices]\n",
        "\n",
        "# Plot the results\n",
        "fig, ax = plt.subplots(figsize=(7, 5))\n",
        "sns.scatterplot(x = eigvecs[:, 0], y = eigvecs[:, 1], hue=labels, ax=ax)\n",
        "ax.set_title('Laplacian Eigenmap')\n",
        "ax.set_xlabel('Eigenvector 2')\n",
        "ax.set_ylabel('Eigenvector 3')\n",
        "plt.show()"
      ],
      "id": "fb19ac42",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Neural Embedding with word2vec\n",
        "\n",
        "### Example: word2vec with Text\n",
        "\n",
        "To showcase the effectiveness of word2vec, let's walk through an example using the `gensim` library."
      ],
      "id": "b5b862e2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import gensim\n",
        "import gensim.downloader\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Load pre-trained word2vec model from Google News\n",
        "model = gensim.downloader.load('word2vec-google-news-300')"
      ],
      "id": "4cf4225a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our first example is to find the words most similar to *king*."
      ],
      "id": "a283af34"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Example usage\n",
        "word = \"king\"\n",
        "similar_words = model.most_similar(word)\n",
        "print(f\"Words most similar to '{word}':\")\n",
        "for similar_word, similarity in similar_words:\n",
        "    print(f\"{similar_word}: {similarity:.4f}\")"
      ],
      "id": "ec6dc463",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A cool (yet controversial) application of word embeddings is analogy solving. Let us consider the following puzzle:\n",
        "\n",
        "> *man* is to *woman* as *king* is to ___ ?\n",
        "\n",
        "We can use word embeddings to solve this puzzle."
      ],
      "id": "0d287ff0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# We solve the puzzle by\n",
        "#\n",
        "#  vec(king) - vec(man) + vec(woman)\n",
        "#\n",
        "# To solve this, we use the model.most_similar function, with positive words being \"king\" and \"woman\" (additive), and negative words being \"man\" (subtractive).\n",
        "#\n",
        "model.most_similar(positive=['woman', \"king\"], negative=['man'], topn=5)"
      ],
      "id": "fc11fad1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The last example is to visualize the word embeddings."
      ],
      "id": "0bc7b157"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "countries = ['Germany', 'France', 'Italy', 'Spain', 'Portugal', 'Greece']\n",
        "capital_words = ['Berlin', 'Paris', 'Rome', 'Madrid', 'Lisbon', 'Athens']\n",
        "\n",
        "# Get the word embeddings for the countries and capitals\n",
        "country_embeddings = np.array([model[country] for country in countries])\n",
        "capital_embeddings = np.array([model[capital] for capital in capital_words])\n",
        "\n",
        "# Compute the PCA\n",
        "pca = PCA(n_components=2)\n",
        "embeddings = np.vstack([country_embeddings, capital_embeddings])\n",
        "embeddings_pca = pca.fit_transform(embeddings)\n",
        "\n",
        "# Create a DataFrame for seaborn\n",
        "df = pd.DataFrame(embeddings_pca, columns=['PC1', 'PC2'])\n",
        "df['Label'] = countries + capital_words\n",
        "df['Type'] = ['Country'] * len(countries) + ['Capital'] * len(capital_words)\n",
        "\n",
        "# Plot the data\n",
        "plt.figure(figsize=(12, 10))\n",
        "\n",
        "# Create a scatter plot with seaborn\n",
        "scatter_plot = sns.scatterplot(data=df, x='PC1', y='PC2', hue='Type', style='Type', s=200, palette='deep', markers=['o', 's'])\n",
        "\n",
        "# Annotate the points\n",
        "for i in range(len(df)):\n",
        "    plt.text(df['PC1'][i], df['PC2'][i] + 0.08, df['Label'][i], fontsize=12, ha='center', va='bottom',\n",
        "             bbox=dict(facecolor='white', edgecolor='none', alpha=0.8))\n",
        "\n",
        "# Draw arrows between countries and capitals\n",
        "for i in range(len(countries)):\n",
        "    plt.arrow(df['PC1'][i], df['PC2'][i], df['PC1'][i + len(countries)] - df['PC1'][i], df['PC2'][i + len(countries)] - df['PC2'][i],\n",
        "              color='gray', alpha=0.6, linewidth=1.5, head_width=0.02, head_length=0.03)\n",
        "\n",
        "plt.legend(title='Type', title_fontsize='13', fontsize='11')\n",
        "plt.title('PCA of Country and Capital Word Embeddings', fontsize=16)\n",
        "plt.xlabel('Principal Component 1', fontsize=14)\n",
        "plt.ylabel('Principal Component 2', fontsize=14)\n",
        "ax = plt.gca()\n",
        "ax.set_axis_off()"
      ],
      "id": "08ba4a2f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that word2vec places the words representing countries close to each other and so do the words representing their capitals. The country-capital relationship is also roughly preserved, e.g., *Germany*-*Berlin* vector is roughly parallel to *France*-*Paris* vector.\n",
        "\n",
        "## Graph Embedding with word2vec\n",
        "\n",
        "### DeepWalk: Learning Network Embeddings via Random Walks\n",
        "\n",
        "DeepWalk treats random walks on a graph as \"sentences\" and applies word2vec to learn node embeddings. The key insight is that nodes appearing in similar contexts (neighborhoods) should have similar embeddings.\n",
        "\n",
        "::: {.column-margin}\n",
        "DeepWalk was introduced by Perozzi et al. (2014) and was one of the first methods to successfully apply natural language processing techniques to graph embedding.\n",
        ":::\n",
        "\n",
        "#### Step 1: Generate Random Walks\n",
        "\n",
        "The first step is to generate training data for word2vec. We do this by sampling random walks from the network. Each random walk is like a \"sentence\" where nodes are \"words\".\n",
        "\n",
        "Let's implement a function to sample random walks. A random walk starts at a node and repeatedly moves to a random neighbor until it reaches the desired length."
      ],
      "id": "387b158d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def random_walk(net, start_node, walk_length):\n",
        "    \"\"\"\n",
        "    Generate a random walk starting from start_node.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    net : sparse matrix\n",
        "        Adjacency matrix of the network\n",
        "    start_node : int\n",
        "        Starting node for the walk\n",
        "    walk_length : int\n",
        "        Length of the walk\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    walk : list\n",
        "        List of node indices representing the random walk\n",
        "    \"\"\"\n",
        "    walk = [start_node]\n",
        "\n",
        "    while len(walk) < walk_length:\n",
        "        cur = walk[-1]\n",
        "        cur_nbrs = list(net[cur].indices)\n",
        "\n",
        "        if len(cur_nbrs) > 0:\n",
        "            # Randomly choose one of the neighbors\n",
        "            walk.append(np.random.choice(cur_nbrs))\n",
        "        else:\n",
        "            # Dead end - terminate the walk\n",
        "            break\n",
        "\n",
        "    return walk"
      ],
      "id": "c04567e6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.column-margin}\n",
        "In practice, we generate multiple walks per node to ensure each node appears in various contexts, which helps the model learn better representations.\n",
        ":::\n",
        "\n",
        "Now we generate multiple random walks starting from each node. We'll use 10 walks per node, each of length 50."
      ],
      "id": "d1e47ecd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "n_nodes = g.vcount()\n",
        "n_walkers_per_node = 10\n",
        "walk_length = 50\n",
        "\n",
        "walks = []\n",
        "for i in range(n_nodes):\n",
        "    for _ in range(n_walkers_per_node):\n",
        "        walks.append(random_walk(A, i, walk_length))\n",
        "\n",
        "print(f\"Generated {len(walks)} random walks\")\n",
        "print(f\"Example walk: {walks[0][:10]}...\")  # Show first 10 nodes of first walk"
      ],
      "id": "dfd2ee13",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 2: Train the Word2Vec Model\n",
        "\n",
        "Now we feed the random walks to the word2vec model. The model will learn to predict which nodes appear together in the same walk, similar to how it learns which words appear together in sentences."
      ],
      "id": "426ec36e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "model = Word2Vec(\n",
        "    walks,\n",
        "    vector_size=32,   # Dimension of the embedding vectors\n",
        "    window=3,         # Maximum distance between current and predicted node\n",
        "    min_count=1,      # Minimum frequency for a node to be included\n",
        "    sg=1,             # Use skip-gram model (vs CBOW)\n",
        "    hs=1              # Use hierarchical softmax for training\n",
        ")"
      ],
      "id": "855f36d8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.column-margin}\n",
        "**Key Parameters:**\n",
        "\n",
        "- `vector_size`: Higher dimensions capture more information but require more data and computation.\n",
        "- `window`: Larger windows capture broader context but may dilute local structure.\n",
        "- `sg=1`: Skip-gram predicts context from target. It works better for small datasets.\n",
        "- `hs=1`: Hierarchical softmax is faster than negative sampling for small vocabularies.\n",
        ":::\n",
        "\n",
        "The `window` parameter is crucial. For a random walk `[0, 1, 2, 3, 4, 5, 6, 7]`, when `window=3`, the context of node 2 includes nodes `[0, 1, 3, 4, 5]` - all nodes within distance 3.\n",
        "\n",
        "#### Step 3: Extract Node Embeddings\n",
        "\n",
        "After training, we can extract the learned embeddings for each node from the model."
      ],
      "id": "4c3cd39b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Extract embeddings for all nodes\n",
        "embedding = np.array([model.wv[i] for i in range(n_nodes)])\n",
        "\n",
        "print(f\"Embedding matrix shape: {embedding.shape}\")\n",
        "print(f\"First node embedding (first 5 dimensions): {embedding[0][:5]}\")"
      ],
      "id": "9badd1ac",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 4: Visualize Embeddings\n",
        "\n",
        "Let's visualize the learned embeddings in 2D using UMAP (Uniform Manifold Approximation and Projection). UMAP reduces the 32-dimensional embeddings to 2D while preserving the local structure.\n",
        "\n",
        "::: {.column-margin}\n",
        "UMAP is a dimensionality reduction technique that preserves both local and global structure better than t-SNE, making it ideal for visualizing high-dimensional embeddings.\n",
        ":::"
      ],
      "id": "e1ead7d7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "import umap\n",
        "from bokeh.plotting import figure, show\n",
        "from bokeh.io import output_notebook\n",
        "from bokeh.models import ColumnDataSource, HoverTool\n",
        "\n",
        "# Reduce embeddings to 2D\n",
        "reducer = umap.UMAP(n_components=2, random_state=42, n_neighbors=15, metric=\"cosine\")\n",
        "xy = reducer.fit_transform(embedding)\n",
        "\n",
        "output_notebook()\n",
        "\n",
        "# Calculate node degrees for visualization\n",
        "degrees = A.sum(axis=1).A1\n",
        "\n",
        "# Create interactive plot\n",
        "source = ColumnDataSource(data=dict(\n",
        "    x=xy[:, 0],\n",
        "    y=xy[:, 1],\n",
        "    size=np.sqrt(degrees / np.max(degrees)) * 30,\n",
        "    community=[palette[label] for label in g.vs[\"label\"]]\n",
        "))\n",
        "\n",
        "p = figure(title=\"DeepWalk Node Embeddings (UMAP projection)\", x_axis_label=\"UMAP 1\", y_axis_label=\"UMAP 2\")\n",
        "p.scatter('x', 'y', size='size', source=source, line_color=\"black\", color=\"community\")\n",
        "\n",
        "show(p)"
      ],
      "id": "96879a16",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice how nodes from the same community (shown in the same color) tend to cluster together in the embedding space. This demonstrates that DeepWalk successfully captures the community structure.\n",
        "\n",
        "#### Step 5: Clustering with K-means\n",
        "\n",
        "One practical application of node embeddings is clustering. While we have dedicated community detection methods like modularity maximization, embeddings allow us to use general machine learning algorithms like K-means.\n",
        "\n",
        "::: {.column-margin}\n",
        "The advantage of using embeddings is that we can leverage the rich ecosystem of machine learning tools designed for vector data.\n",
        ":::\n",
        "\n",
        "Let's implement K-means clustering with automatic selection of the number of clusters using the silhouette score."
      ],
      "id": "8433a6f4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "def find_optimal_clusters(embedding, n_clusters_range=(2, 10)):\n",
        "    \"\"\"\n",
        "    Find the optimal number of clusters using silhouette score.\n",
        "\n",
        "    The silhouette score measures how well each node fits within its cluster\n",
        "    compared to other clusters. Scores range from -1 to 1, where higher is better.\n",
        "    \"\"\"\n",
        "    silhouette_scores = []\n",
        "\n",
        "    for n_clusters in range(*n_clusters_range):\n",
        "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "        cluster_labels = kmeans.fit_predict(embedding)\n",
        "        score = silhouette_score(embedding, cluster_labels)\n",
        "        silhouette_scores.append((n_clusters, score))\n",
        "        print(f\"k={n_clusters}: silhouette score = {score:.3f}\")\n",
        "\n",
        "    # Select the number of clusters with highest silhouette score\n",
        "    optimal_k = max(silhouette_scores, key=lambda x: x[1])[0]\n",
        "    print(f\"\\nOptimal number of clusters: {optimal_k}\")\n",
        "\n",
        "    # Perform final clustering with optimal k\n",
        "    kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
        "    return kmeans.fit_predict(embedding)\n",
        "\n",
        "# Find clusters\n",
        "cluster_labels = find_optimal_clusters(embedding)"
      ],
      "id": "ef6ebbe2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.column-margin}\n",
        "The silhouette score measures both cohesion (how close nodes are within their cluster) and separation (how far clusters are from each other).\n",
        ":::\n",
        "\n",
        "Now let's visualize the discovered clusters on the network:"
      ],
      "id": "701aa902"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Visualize the clustering results\n",
        "cmap = sns.color_palette().as_hex()\n",
        "igraph.plot(\n",
        "    g,\n",
        "    vertex_color=[cmap[label] for label in cluster_labels],\n",
        "    bbox=(500, 500),\n",
        "    vertex_size=20\n",
        ")"
      ],
      "id": "db3b62b9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The K-means algorithm successfully identifies community structure using only the learned embeddings, demonstrating that DeepWalk captures meaningful structural properties of the network.\n",
        "\n",
        "### Node2vec: Flexible Graph Embeddings\n",
        "\n",
        "Node2vec extends DeepWalk by introducing a biased random walk strategy. Instead of uniformly choosing the next node, node2vec uses two parameters, $p$ and $q$, to control the exploration strategy:\n",
        "\n",
        "- **Return parameter $p$**: Controls the likelihood of returning to the previous node\n",
        "- **In-out parameter $q$**: Controls whether the walk explores locally (BFS-like) or ventures further (DFS-like)\n",
        "\n",
        "::: {.column-margin}\n",
        "Node2vec was introduced by Grover and Leskovec (2016). The biased walk allows it to learn embeddings that capture different structural properties depending on the task.\n",
        ":::\n",
        "\n",
        "#### Step 1: Implement Biased Random Walk\n",
        "\n",
        "The key innovation in node2vec is the biased random walk. Let's implement it step by step."
      ],
      "id": "755d3a57"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def node2vec_random_walk(net, start_node, walk_length, p, q):\n",
        "    \"\"\"\n",
        "    Generate a biased random walk for node2vec.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    net : sparse matrix\n",
        "        Adjacency matrix of the network\n",
        "    start_node : int\n",
        "        Starting node for the walk\n",
        "    walk_length : int\n",
        "        Length of the walk\n",
        "    p : float\n",
        "        Return parameter (controls likelihood of returning to previous node)\n",
        "    q : float\n",
        "        In-out parameter (controls BFS vs DFS behavior)\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    walk : list\n",
        "        List of node indices representing the biased random walk\n",
        "    \"\"\"\n",
        "    walk = [start_node]\n",
        "\n",
        "    while len(walk) < walk_length:\n",
        "        cur = walk[-1]\n",
        "        cur_nbrs = list(net[cur].indices)\n",
        "\n",
        "        if len(cur_nbrs) > 0:\n",
        "            if len(walk) == 1:\n",
        "                # First step: uniform random choice\n",
        "                walk.append(np.random.choice(cur_nbrs))\n",
        "            else:\n",
        "                # Subsequent steps: biased choice based on p and q\n",
        "                prev = walk[-2]\n",
        "                next_node = biased_choice(net, cur_nbrs, prev, p, q)\n",
        "                walk.append(next_node)\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return walk\n",
        "\n",
        "def biased_choice(net, neighbors, prev, p, q):\n",
        "    \"\"\"\n",
        "    Choose the next node with bias controlled by p and q.\n",
        "\n",
        "    The transition probability is:\n",
        "    - 1/p if returning to the previous node\n",
        "    - 1   if moving to a neighbor of the previous node (distance 1)\n",
        "    - 1/q if moving away from the previous node (distance 2)\n",
        "    \"\"\"\n",
        "    unnormalized_probs = []\n",
        "\n",
        "    for neighbor in neighbors:\n",
        "        if neighbor == prev:\n",
        "            # Returning to previous node\n",
        "            unnormalized_probs.append(1 / p)\n",
        "        elif neighbor in net[prev].indices:\n",
        "            # Moving to a common neighbor (BFS-like)\n",
        "            unnormalized_probs.append(1.0)\n",
        "        else:\n",
        "            # Moving away from previous node (DFS-like)\n",
        "            unnormalized_probs.append(1 / q)\n",
        "\n",
        "    # Normalize probabilities\n",
        "    norm_const = sum(unnormalized_probs)\n",
        "    normalized_probs = [prob / norm_const for prob in unnormalized_probs]\n",
        "\n",
        "    # Sample next node\n",
        "    return np.random.choice(neighbors, p=normalized_probs)"
      ],
      "id": "698db46c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.column-margin}\n",
        "**Understanding p and q:**\n",
        "\n",
        "- Small $p$ ($p < 1$): Encourages returning to previous node, leading to local exploration\n",
        "- Large $q$ ($q > 1$): Discourages moving away, resulting in BFS-like behavior\n",
        "- Small $q$ ($q < 1$): Encourages exploration, resulting in DFS-like behavior\n",
        ":::\n",
        "\n",
        "#### Step 2: Generate Walks and Train Model\n",
        "\n",
        "Now let's generate biased random walks and train the word2vec model. We'll use $p=1$ and $q=0.1$, which encourages outward exploration (DFS-like behavior) to capture community structure."
      ],
      "id": "b1307d92"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Generate biased random walks\n",
        "p = 1.0   # Return parameter\n",
        "q = 0.1   # In-out parameter (q < 1 means DFS-like)\n",
        "\n",
        "walks_node2vec = []\n",
        "for i in range(n_nodes):\n",
        "    for _ in range(n_walkers_per_node):\n",
        "        walks_node2vec.append(node2vec_random_walk(A, i, walk_length, p, q))\n",
        "\n",
        "print(f\"Generated {len(walks_node2vec)} biased random walks\")\n",
        "print(f\"Example walk: {walks_node2vec[0][:10]}...\")"
      ],
      "id": "8ef78d41",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.column-margin}\n",
        "With $q=0.1$, the walk is 10 times more likely to explore distant nodes than return to the immediate neighborhood, encouraging discovery of global community structure.\n",
        ":::\n",
        "\n",
        "Train the word2vec model on the biased walks:"
      ],
      "id": "ba5bc4c3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Train node2vec model\n",
        "model_node2vec = Word2Vec(\n",
        "    walks_node2vec,\n",
        "    vector_size=32,\n",
        "    window=3,\n",
        "    min_count=1,\n",
        "    sg=1,\n",
        "    hs=1\n",
        ")\n",
        "\n",
        "# Extract embeddings\n",
        "embedding_node2vec = np.array([model_node2vec.wv[i] for i in range(n_nodes)])\n",
        "print(f\"Node2vec embedding shape: {embedding_node2vec.shape}\")"
      ],
      "id": "3802a6fb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 3: Visualize Node2vec Embeddings\n",
        "\n",
        "Let's visualize the node2vec embeddings and compare them with DeepWalk."
      ],
      "id": "2d2d87c8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "\n",
        "# Reduce node2vec embeddings to 2D\n",
        "reducer_n2v = umap.UMAP(n_components=2, random_state=42, n_neighbors=15, metric=\"cosine\")\n",
        "xy_n2v = reducer_n2v.fit_transform(embedding_node2vec)\n",
        "\n",
        "output_notebook()\n",
        "\n",
        "degrees = A.sum(axis=1).A1\n",
        "\n",
        "source_n2v = ColumnDataSource(data=dict(\n",
        "    x=xy_n2v[:, 0],\n",
        "    y=xy_n2v[:, 1],\n",
        "    size=np.sqrt(degrees / np.max(degrees)) * 30,\n",
        "    community=[palette[label] for label in g.vs[\"label\"]],\n",
        "    name = [str(i) for i in range(n_nodes)]\n",
        "))\n",
        "\n",
        "p_n2v = figure(title=\"Node2vec Embeddings (UMAP projection)\", x_axis_label=\"UMAP 1\", y_axis_label=\"UMAP 2\")\n",
        "p_n2v.scatter('x', 'y', size='size', source=source_n2v, line_color=\"black\", color=\"community\")\n",
        "\n",
        "hover = HoverTool()\n",
        "hover.tooltips = [\n",
        "    (\"Node\", \"@name\"),\n",
        "    (\"Community\", \"@community\")\n",
        "]\n",
        "p_n2v.add_tools(hover)\n",
        "\n",
        "show(p_n2v)"
      ],
      "id": "b6b4f6d6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice how the node2vec embeddings with $q=0.1$ (DFS-like exploration) create even more distinct community clusters compared to DeepWalk. This is because the biased walk explores the community structure more thoroughly.\n",
        "\n",
        "#### Step 4: Clustering Analysis\n",
        "\n",
        "Let's apply K-means clustering to the node2vec embeddings:"
      ],
      "id": "ff57d2b0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Find optimal clusters for node2vec embeddings\n",
        "cluster_labels_n2v = find_optimal_clusters(embedding_node2vec)\n",
        "\n",
        "# Visualize the clustering results\n",
        "igraph.plot(\n",
        "    g,\n",
        "    vertex_color=[palette[label] for label in cluster_labels_n2v],\n",
        "    bbox=(500, 500),\n",
        "    vertex_size=20,\n",
        "    vertex_label=[str(i) for i in range(n_nodes)]\n",
        ")"
      ],
      "id": "dd96eae0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By tuning the $p$ and $q$ parameters, node2vec can adapt to different network analysis tasks - from community detection (small $q$) to role discovery (large $q$)."
      ],
      "id": "25278e2c"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "advnetsci",
      "language": "python",
      "display_name": "Python (advnetsci)",
      "path": "/Users/skojaku-admin/Library/Jupyter/kernels/advnetsci"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}