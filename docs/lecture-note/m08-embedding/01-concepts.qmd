---
title: "Network Embedding Concepts"
jupyter: advnetsci
execute:
    enabled: true
---

## What is Network Embedding?

Networks are high-dimensional discrete data that can be difficult to analyze with traditional machine learning methods that assume continuous and smooth data. Network embedding is a technique to represent networks in low-dimensional continuous spaces, enabling us to apply standard machine learning algorithms.

The goal of network embedding is to map each node in a network to a point in a low-dimensional space (typically $\mathbb{R}^d$ where $d \ll N$) while preserving important structural properties of the network.

## Spectral Embedding

### Network Compression Approach

Let us approach spectral embedding from the perspective of network compression.
Suppose we have an adjacency matrix $\mathbf{A}$ of a network.
The adjacency matrix is high-dimensional data, i.e., a matrix of size $N \times N$ for a network of $N$ nodes.
We want to compress it into a lower-dimensional matrix $\mathbf{U}$ of size $N \times d$ for a user-defined small integer $d < N$.
A good $\mathbf{U}$ should preserve the network structure and thus can reconstruct the original data $\mathbf{A}$ as closely as possible.
This leads to the following optimization problem:

$$
\min_{\mathbf{U}} J(\mathbf{U}),\quad J(\mathbf{U}) = \| \mathbf{A} - \mathbf{U}\mathbf{U}^\top \|_F^2
$$

where:

1. $\mathbf{U}\mathbf{U}^\top$ is the outer product of $\mathbf{U}$ and represents the reconstructed network.
2. $\|\cdot\|_F$ is the Frobenius norm, which is the sum of the squares of the elements in the matrix.
3. $J(\mathbf{U})$ is the loss function that measures the difference between the original network $\mathbf{A}$ and the reconstructed network $\mathbf{U}\mathbf{U}^\top$.

By minimizing the Frobenius norm with respect to $\mathbf{U}$, we obtain the best low-dimensional embedding of the network.

### Spectral Decomposition Solution

Consider the spectral decomposition of $\mathbf{A}$:

$$
\mathbf{A} = \sum_{i=1}^N \lambda_i \mathbf{u}_i \mathbf{u}_i^\top
$$

where $\lambda_i$ are eigenvalues (weights) and $\mathbf{u}_i$ are eigenvectors. Each term $\lambda_i \mathbf{u}_i \mathbf{u}_i^\top$ is a rank-one matrix that captures a part of the network's structure. The larger the weight $\lambda_i$, the more important that term is in describing the network.

To compress the network, we can select the $d$ terms with the largest weights $\lambda_i$. By combining the corresponding $\mathbf{u}_i$ vectors into a matrix $\mathbf{U}$, we obtain a good low-dimensional embedding of the network.

![](../figs/spectral-decomposition.jpg)

For a formal proof, please refer to the [Appendix section](./04-appendix.md).

### Modularity Embedding

In a similar vein, we can use the modularity matrix to generate a low-dimensional embedding of the network.
Let us define the modularity matrix $\mathbf{Q}$ as follows:

$$
Q_{ij} = \frac{1}{2m}A_{ij} - \frac{k_i k_j}{4m^2}
$$

where $k_i$ is the degree of node $i$, and $m$ is the number of edges in the network.

We then compute the eigenvectors of $\mathbf{Q}$ and use them to embed the network into a low-dimensional space just as we did for the adjacency matrix. The modularity embedding can be used to bipartition the network into two communities using a simple algorithm: group nodes with the same sign of the second eigenvector [@newman2006modularity].

### Laplacian Eigenmap

Laplacian Eigenmap [@belkin2003laplacian] is another approach to compress a network into a low-dimensional space. The fundamental idea behind this method is to position connected nodes close to each other in the low-dimensional space. This approach leads to the following optimization problem:

$$
\min_{\mathbf{U}} J_{LE}(\mathbf{U}),\quad J_{LE}(\mathbf{U}) = \frac{1}{2}\sum_{i,j} A_{ij} \| u_i - u_j \|^2
$$

::: {.column-margin}
**Derivation steps:**

Starting from $J_{LE}(\mathbf{U})$, we expand:

$$
\begin{aligned}
&= \frac{1}{2}\sum_{i,j} A_{ij} \left( \| u_i \|^2 - 2 u_i^\top u_j + \| u_j \|^2 \right) \\
&= \sum_{i} k_i \| u_i \|^2 - \sum_{i,j} A_{ij} u_i^\top u_j\\
&= \sum_{i,j} L_{ij} u_i^\top u_j
\end{aligned}
$$

where $L_{ij} = k_i$ if $i=j$ and $L_{ij} = -A_{ij}$ otherwise.

In matrix form: $J_{LE}(\mathbf{U}) = \text{Tr}(\mathbf{U}^\top \mathbf{L} \mathbf{U})$

See [Appendix](./04-appendix.md) for full details.
:::

The solution minimizes distances between connected nodes. Through algebraic manipulation (see margin), we can rewrite the objective as $J_{LE}(\mathbf{U}) = \text{Tr}(\mathbf{U}^\top \mathbf{L} \mathbf{U})$, where $\mathbf{L}$ is the graph Laplacian matrix:

$$
L_{ij} = \begin{cases}
k_i & \text{if } i = j \\
-A_{ij} & \text{if } i \neq j
\end{cases}
$$

By taking the derivative and setting it to zero:

$$
\frac{\partial J_{LE}}{\partial \mathbf{U}} = 0 \implies \mathbf{L} \mathbf{U} = \lambda \mathbf{U}
$$

**Solution**: The $d$ eigenvectors associated with the $d$ smallest eigenvalues of the Laplacian matrix $\mathbf{L}$.

::: {.column-margin}
The smallest eigenvalue is always zero (with eigenvector of all ones). In practice, compute $d+1$ smallest eigenvectors and discard the trivial one.
:::

## Neural Embedding

### Introduction to word2vec

Neural embedding methods leverage neural network architectures to learn node representations. Before discussing graph-specific methods, we first introduce *word2vec*, which forms the foundation for many neural graph embedding techniques.

word2vec is a neural network model that learns word embeddings in a continuous vector space. It was introduced by Tomas Mikolov and his colleagues at Google in 2013 [@mikolov2013distributed].

### How word2vec Works

word2vec learns word meanings from context, following the linguistic principle: "You shall know a word by the company it keeps" [@church1988word].

::: {.column-margin}
This phrase means a person is similar to those they spend time with. It comes from Aesop's fable *The Ass and his Purchaser* (500s B.C.): a man brings an ass to his farm on trial. The ass immediately seeks out the laziest, greediest ass in the herd. The man returns the ass, knowing it will be lazy and greedy based on the company it chose.

<iframe width="100%" height="200" src="https://www.youtube.com/embed/gQddtTdmG_8?si=x8DUQnll2Rnj8qkn" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
:::

**The Core Idea**: Given a target word, predict its surrounding context words within a fixed window. For example, in:

> The quick brown fox jumps over a lazy dog

The context of *fox* (with window size 2) includes: *quick*, *brown*, *jumps*, *over*.

::: {.column-margin}
The window size determines how far we look around the target word. A window size of 2 means we consider 2 words before and 2 words after.
:::

**Why This Works**: Words appearing in similar contexts get similar embeddings. Consider:

- "The quick brown **fox** jumps over the fence"
- "The quick brown **dog** runs over the fence"
- "The **student** studies in the library"

Both *fox* and *dog* appear with words like "quick," "brown," and "jumps/runs," so they'll have similar embeddings. But *student* appears in completely different contexts (like "studies," "library"), so its embedding will be far from *fox* and *dog*. This is how word2vec captures semantic similarity without explicit supervision.

**Two Training Approaches**:

- **Skip-gram**: Given a target word → predict context words (we'll use this)
- **CBOW**: Given context words → predict target word

::: {.column-margin}
Skip-gram works better for small datasets and rare words. CBOW is faster to train.
:::

**Network Architecture**: word2vec uses a simple 3-layer neural network:

![](../figs/word2vec.png){width="70%" fig-align="center"}

- **Input layer** ($N$ neurons): One-hot encoding of the target word
- **Hidden layer** ($d$ neurons, $d \ll N$): The learned word embedding
- **Output layer** ($N$ neurons): Probability distribution over context words (via softmax)

The hidden layer activations become the word embeddings—dense, low-dimensional vectors that capture semantic relationships.

::: {.column-margin}
For a visual walkthrough of word2vec, see [The Illustrated Word2vec](https://jalammar.github.io/illustrated-word2vec/) by Jay Alammar.
:::

### Key Technical Components

#### The Computational Challenge

To predict context word $w_c$ given target word $w_t$, we compute:

$$
P(w_c | w_t) = \frac{\exp(\mathbf{v}_{w_c} \cdot \mathbf{v}_{w_t})}{\sum_{w \in V} \exp(\mathbf{v}_w \cdot \mathbf{v}_{w_t})}
$$

::: {.column-margin}
This is the softmax function—it converts dot products into probabilities that sum to 1.
:::

**The problem**: The denominator sums over all vocabulary words (e.g., 100,000+ terms), making training prohibitively slow.

#### Two Solutions

**Hierarchical Softmax**: Organizes vocabulary as a binary tree with words at leaves. Computing probability becomes traversing root-to-leaf paths, reducing complexity from $O(|V|)$ to $O(\log |V|)$.

![](https://lh5.googleusercontent.com/proxy/_omrC8G6quTl2SGarwFe57qzbIs-PtGkEA5yODFE5I0Ny2IHGiJwsUhMrcuUqg5o-R2nD9hkgMuZsQJKoCggP29zXtj-Vz-X8BE)

**Negative Sampling**: Instead of normalizing over all words, sample a few "negative" (non-context) words. Contrast the target's true context with these random words. This approximates the full softmax much more efficiently.

::: {.column-margin}
Typically, 5-20 negative samples are enough. The model learns to distinguish true context words from random words.
:::

### What's Special About word2vec?

With word2vec, words are represented as dense vectors, enabling us to explore their relationships using simple linear algebra. This is in contrast to traditional natural language processing (NLP) methods, such as bag-of-words and topic modeling, which represent words as discrete units or high-dimensional vectors.

![](https://miro.medium.com/v2/resize:fit:678/1*5F4TXdFYwqi-BWTToQPIfg.jpeg)

word2vec embeddings can capture semantic relationships, such as analogies (e.g., *man* is to *woman* as *king* is to *queen*) and can visualize relationships between concepts (e.g., countries and their capitals form parallel vectors in the embedding space).

## Graph Embedding with word2vec

How can we apply word2vec to graph data? There is a critical challenge: word2vec takes sequences of words as input, while graph data are discrete and unordered. A solution to fill this gap is *random walk*, which transforms graph data into a sequence of nodes. Once we have a sequence of nodes, we can treat it as a sequence of words and apply word2vec.

::: {.column-margin}
Random walks create "sentences" from graphs: each walk is a sequence of nodes, just like a sentence is a sequence of words.
:::

### DeepWalk

![](https://dt5vp8kor0orz.cloudfront.net/7c56c256b9fbf06693da47737ac57fae803a5a4f/1-Figure1-1.png)

DeepWalk is one of the pioneering works to apply word2vec to graph data [@perozzi2014deepwalk]. It views the nodes as words and the random walks on the graph as sentences, and applies word2vec to learn the node embeddings.

More specifically, the method contains the following steps:

1. Sample multiple random walks from the graph.
2. Treat the random walks as sentences and feed them to word2vec to learn the node embeddings.

DeepWalk typically uses the skip-gram model with hierarchical softmax for efficient training.

### node2vec

node2vec [@grover2016node2vec] extends DeepWalk with **biased random walks** controlled by two parameters:

$$
P(v_{t+1} = x | v_t = v, v_{t-1} = t) \propto
\begin{cases}
\frac{1}{p} & \text{if } d(t,x) = 0 \text{ (return to previous)} \\
1 & \text{if } d(t,x) = 1 \text{ (close neighbor)} \\
\frac{1}{q} & \text{if } d(t,x) = 2 \text{ (explore further)} \\
\end{cases}
$$

where $d(t,x)$ is the shortest path distance from the previous node $t$ to candidate node $x$.

**Controlling Exploration**:

- Low $p$ → return bias (local revisiting)
- Low $q$ → outward bias (exploration)
- High $q$ → inward bias (stay local)

**Two Exploration Strategies**:

![](https://www.researchgate.net/publication/354654762/figure/fig3/AS:1069013035655173@1631883977008/A-biased-random-walk-procedure-of-node2vec-B-BFS-and-DFS-search-strategies-from-node-u.png)

- **BFS-like** (low $q$): Explore immediate neighborhoods → captures **community structure**
- **DFS-like** (high $q$): Explore deep paths → captures **structural roles**

![](https://miro.medium.com/v2/resize:fit:1138/format:webp/1*nCyF5jFSU5uJVdAPdf-0HA.png)

**Technical Note**: node2vec uses **negative sampling** instead of hierarchical softmax, which affects embedding characteristics [@kojaku2021neurips; @dyer2014notes].

### LINE

LINE [@tang2015line] is another pioneering work to learn node embeddings by directly optimizing the graph structure. It is equivalent to node2vec with $p=1$, $q=1$, and window size 1.

## Comparing Spectral and Neural Embedding Approaches

Both approaches have complementary strengths:

### Analytical Tractability

**Spectral methods**: Fully analyzable using linear algebra. We can prove optimality—e.g., adjacency and Laplacian spectral embeddings are optimal for community detection in stochastic block models [@nadakuditi2012graph].

**Neural methods**: Less transparent, but recent work establishes equivalences to spectral methods under specific conditions [@qiu2018network; @kojaku2023network]. Surprisingly, DeepWalk, node2vec, and LINE are also provably optimal for the stochastic block model.

### Scalability

**Spectral methods**: Computationally expensive (eigendecomposition scales poorly). Randomized algorithms like `TruncatedSVD` (scikit-learn) help, but can be numerically unstable depending on the eigenvalue spectrum.

**Neural methods**: More stable and scalable for large networks. Training via stochastic gradient descent handles millions of nodes efficiently.

### Flexibility

**Neural methods**: Highly adaptable. Can modify loss functions to capture different notions of proximity. Example: Poincaré embeddings [@nickel2017poincare] use hyperbolic geometry to better represent hierarchical structures.

![](https://pbs.twimg.com/media/DUUj0sxU8AACV50.jpg)

**Implementation Note**: Many libraries take shortcuts that deviate from original papers:

- [fastnode2vec](https://github.com/louisabraham/fastnode2vec): Fast but uses uniform negative sampling (degrades quality)
- [pytorch-geometric](https://github.com/pyg-team/pytorch_geometric): Popular GNN library with similar limitation
- [gnn-tools](https://github.com/skojaku/gnn-tools): Experimental implementations
- [graphvec](https://github.com/skojaku/graphvec): Lightweight embedding collection

## Exercises

- [✍️ Pen and paper exercises](pen-and-paper/exercise.pdf)
