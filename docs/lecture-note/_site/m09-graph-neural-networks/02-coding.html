<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Sadamori Kojaku">
<meta name="dcterms.date" content="2025-07-27">

<title>Coding: Graph Neural Networks Implementation</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../m09-graph-neural-networks/03-exercises.html" rel="next">
<link href="../m09-graph-neural-networks/01-concepts.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-0348920b7671f696dc9078d39bff215e.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-e6dc204ec8b52f55243daf2cac742210.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "|"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../assets/custom.css">
</head>

<body class="nav-sidebar docked nav-fixed fullcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../logo.jpg" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Advanced Topics in Network Science</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-course" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Course</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-course">    
        <li>
    <a class="dropdown-item" href="../course/welcome.html">
 <span class="dropdown-text">Welcome</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../course/about.html">
 <span class="dropdown-text">About</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../course/discord.html">
 <span class="dropdown-text">Discord</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../course/minidora-usage.html">
 <span class="dropdown-text">Minidora</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../course/setup.html">
 <span class="dropdown-text">Setup</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-intro" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Intro</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-intro">    
        <li>
    <a class="dropdown-item" href="../intro/why-networks.html">
 <span class="dropdown-text">Why Networks?</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-foundations" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Foundations</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-foundations">    
        <li class="dropdown-header">─── M01: Euler Path ───</li>
        <li>
    <a class="dropdown-item" href="../m01-euler_tour/01-concepts.html">
 <span class="dropdown-text">Concepts</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m01-euler_tour/02-coding.html">
 <span class="dropdown-text">Coding</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m01-euler_tour/03-exercises.html">
 <span class="dropdown-text">Exercises</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m01-euler_tour/04-advanced.html">
 <span class="dropdown-text">Advanced</span></a>
  </li>  
        <li class="dropdown-header">─── M02: Small World ───</li>
        <li>
    <a class="dropdown-item" href="../m02-small-world/01-concepts.html">
 <span class="dropdown-text">Concepts</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m02-small-world/02-coding.html">
 <span class="dropdown-text">Coding</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m02-small-world/03-exercises.html">
 <span class="dropdown-text">Exercises</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m02-small-world/04-appendix.html">
 <span class="dropdown-text">Appendix</span></a>
  </li>  
        <li class="dropdown-header">─── M03: Robustness ───</li>
        <li>
    <a class="dropdown-item" href="../m03-robustness/01-concepts.html">
 <span class="dropdown-text">Concepts</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m03-robustness/02-coding.html">
 <span class="dropdown-text">Coding</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m03-robustness/03-exercises.html">
 <span class="dropdown-text">Exercises</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m03-robustness/04-appendix.html">
 <span class="dropdown-text">Appendix</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-core-topics" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Core Topics</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-core-topics">    
        <li class="dropdown-header">─── M04: Friendship Paradox ───</li>
        <li>
    <a class="dropdown-item" href="../m04-node-degree/01-concepts.html">
 <span class="dropdown-text">Concepts</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m04-node-degree/02-coding.html">
 <span class="dropdown-text">Coding</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m04-node-degree/03-exercises.html">
 <span class="dropdown-text">Exercises</span></a>
  </li>  
        <li class="dropdown-header">─── M05: Clustering ───</li>
        <li>
    <a class="dropdown-item" href="../m05-clustering/01-concepts.html">
 <span class="dropdown-text">Concepts</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m05-clustering/02-coding.html">
 <span class="dropdown-text">Coding</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m05-clustering/03-exercises.html">
 <span class="dropdown-text">Exercises</span></a>
  </li>  
        <li class="dropdown-header">─── M06: Centrality ───</li>
        <li>
    <a class="dropdown-item" href="../m06-centrality/01-concepts.html">
 <span class="dropdown-text">Concepts</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m06-centrality/02-coding.html">
 <span class="dropdown-text">Coding</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m06-centrality/03-exercises.html">
 <span class="dropdown-text">Exercises</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-advanced-topics" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Advanced Topics</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-advanced-topics">    
        <li class="dropdown-header">─── M07: Random Walks ───</li>
        <li>
    <a class="dropdown-item" href="../m07-random-walks/01-concepts.html">
 <span class="dropdown-text">Concepts</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m07-random-walks/02-coding.html">
 <span class="dropdown-text">Coding</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m07-random-walks/03-exercises.html">
 <span class="dropdown-text">Exercises</span></a>
  </li>  
        <li class="dropdown-header">─── M08: Embedding ───</li>
        <li>
    <a class="dropdown-item" href="../m08-embedding/01-concepts.html">
 <span class="dropdown-text">Concepts</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m08-embedding/02-coding.html">
 <span class="dropdown-text">Coding</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m08-embedding/03-exercises.html">
 <span class="dropdown-text">Exercises</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m08-embedding/04-appendix.html">
 <span class="dropdown-text">Appendix</span></a>
  </li>  
        <li class="dropdown-header">─── M09: Graph Neural Networks ───</li>
        <li>
    <a class="dropdown-item" href="../m09-graph-neural-networks/01-concepts.html">
 <span class="dropdown-text">Concepts</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m09-graph-neural-networks/02-coding.html">
 <span class="dropdown-text">Coding</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m09-graph-neural-networks/03-exercises.html">
 <span class="dropdown-text">Exercises</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m09-graph-neural-networks/04-appendix.html">
 <span class="dropdown-text">Appendix</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../m09-graph-neural-networks/01-concepts.html">M09: Graph Neural Networks</a></li><li class="breadcrumb-item"><a href="../m09-graph-neural-networks/02-coding.html">Advanced Topics in Network Science</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false">
 <span class="menu-text">Course Information</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course/welcome.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course/about.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About us</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course/discord.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Discord</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course/minidora-usage.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Using Minidora</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course/how-to-submit-assignment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">How to submit assignment</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">Introduction</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../intro/why-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Networks</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false">
 <span class="menu-text">M01: Euler Path</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m01-euler_tour/01-concepts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">A Stroll, Seven Bridges, and a Mathematical Revolution</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m01-euler_tour/02-coding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Coding Networks in Python</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m01-euler_tour/03-exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exercises</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m01-euler_tour/04-advanced.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Advanced: Sparse Matrices for Large-Scale Networks</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false">
 <span class="menu-text">M02: Small World</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m02-small-world/01-concepts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Core Concepts</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m02-small-world/02-coding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Efficient Network Representation and Computing Paths</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m02-small-world/03-exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exercises and Assignments</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m02-small-world/04-appendix.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Appendix - Brief Introduction to igraph</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false">
 <span class="menu-text">M03: Robustness</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m03-robustness/01-concepts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Core Concepts</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m03-robustness/02-coding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Coding - Network Robustness Analysis</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m03-robustness/03-exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exercises and Assignments</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m03-robustness/04-appendix.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exercises and Assignments</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false">
 <span class="menu-text">M04: Friendship Paradox</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-node-degree/01-concepts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Core Concepts</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-node-degree/02-coding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Visualizing Degree Distributions in Python</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-node-degree/03-exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exercises</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false">
 <span class="menu-text">M05: Clustering</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m05-clustering/01-concepts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Core Concepts</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m05-clustering/02-coding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Clustering Algorithms and Implementation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m05-clustering/03-exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exercises and Assignments</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="false">
 <span class="menu-text">M06: Centrality</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m06-centrality/01-concepts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Advanced Topics in Network Science</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m06-centrality/02-coding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Advanced Topics in Network Science</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m06-centrality/03-exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Advanced Topics in Network Science</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false">
 <span class="menu-text">M07: Random Walks</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m07-random-walks/01-concepts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Advanced Topics in Network Science</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m07-random-walks/02-coding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Advanced Topics in Network Science</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m07-random-walks/03-exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Advanced Topics in Network Science</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="false">
 <span class="menu-text">M08: Embedding</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m08-embedding/01-concepts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Advanced Topics in Network Science</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m08-embedding/02-coding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Advanced Topics in Network Science</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m08-embedding/03-exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Advanced Topics in Network Science</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m08-embedding/04-appendix.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Advanced Topics in Network Science</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="true">
 <span class="menu-text">M09: Graph Neural Networks</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-11" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m09-graph-neural-networks/01-concepts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Advanced Topics in Network Science</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m09-graph-neural-networks/02-coding.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Advanced Topics in Network Science</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m09-graph-neural-networks/03-exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Advanced Topics in Network Science</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m09-graph-neural-networks/04-appendix.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Advanced Topics in Network Science</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../m09-graph-neural-networks/01-concepts.html">M09: Graph Neural Networks</a></li><li class="breadcrumb-item"><a href="../m09-graph-neural-networks/02-coding.html">Advanced Topics in Network Science</a></li></ol></nav>
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Coding: Graph Neural Networks Implementation</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Sadamori Kojaku </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">July 27, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="preliminaries-image-processing" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="preliminaries-image-processing"><span class="header-section-number">1</span> Preliminaries: Image Processing</h2>
<p>Graph Neural Networks are a type of neural network for graph data. node2vec and deepwalk stem from the idea of language modeling. In this module, we will focus on another branch of graph neural networks that stem from image processing.</p>
<section id="edge-detection-problem-in-image-processing" class="level3">
<h3 class="anchored" data-anchor-id="edge-detection-problem-in-image-processing">Edge Detection Problem in Image Processing</h3>
<p>Edge detection is a classical problem in image processing. The goal is to identify the boundaries of objects in an image.</p>
<p><img src="https://media.geeksforgeeks.org/wp-content/uploads/20240616211411/Screenshot-(85).webp" class="img-fluid"></p>
<p>To approach the problem, let us first remind that an image is a matrix of pixels. Each pixel has RGB values, each of which represents the intensity of red, green, and blue color. To simplify the problem, we focus on grayscale images, in which each pixel has only one value representing the brightness. In this case, an image can be represented as a 2D matrix, where each element in the matrix represents the brightness of a pixel.</p>
<p><img src="https://ai.stanford.edu/~syyeung/cvweb/Pictures1/imagematrix.png" class="img-fluid"></p>
</section>
<section id="an-example" class="level3">
<h3 class="anchored" data-anchor-id="an-example">An example</h3>
<p>Human eyes are very sensitive to brightness changes. An edge in an image appears when there is a <em>significant brightness change between adjacent pixels</em>. To be more concrete, let’s consider a small example consisting of 6x6 pixels, with a vertical line from the top to the bottom, where the brightness is higher than the neighboring pixels. This is an edge we want to detect.</p>
<p><span class="math display">
X = \begin{bmatrix}
10 &amp; 10 &amp; 80 &amp; 10 &amp; 10 &amp; 10 \\
10 &amp; 10 &amp; 80 &amp; 10 &amp; 10 &amp; 10 \\
10 &amp; 10 &amp; 80 &amp; 10 &amp; 10 &amp; 10 \\
10 &amp; 10 &amp; 80 &amp; 10 &amp; 10 &amp; 10 \\
10 &amp; 10 &amp; 80 &amp; 10 &amp; 10 &amp; 10 \\
10 &amp; 10 &amp; 80 &amp; 10 &amp; 10 &amp; 10
\end{bmatrix}
</span></p>
<p>Let’s zoom on the pixel at (3, 3) and its surrounding pixels.</p>
<p><span class="math display">
Z = \begin{bmatrix}
10 &amp; 80 &amp; 10 \\
\textcolor{blue}{10} &amp; \textcolor{red}{80} &amp; \textcolor{purple}{10} \\
10 &amp; 80 &amp; 10
\end{bmatrix}
</span></p>
<p>where the central pixel is highlighted in red. Since we are interested in the edge which is a sudden change in brightness along the horizontal direction, we take a derivative at the central pixel by</p>
<p><span class="math display">
\nabla Z_{22} = \textcolor{blue}{Z_{2,1}} - \textcolor{purple}{Z_{2,3}}
</span></p>
<p>Following the same process, we can compute the derivative at all pixels, which gives us the (horizontal) derivative of the image.</p>
<p><span class="math display">
\begin{bmatrix}
- &amp; -70 &amp; 0 &amp; 70 &amp; 0 &amp; - \\
- &amp; -70 &amp; 0 &amp; 70 &amp; 0 &amp; - \\
- &amp; -70 &amp; 0 &amp; 70 &amp; 0 &amp; - \\
- &amp; -70 &amp; 0 &amp; 70 &amp; 0 &amp; - \\
- &amp; -70 &amp; 0 &amp; 70 &amp; 0 &amp; -
\end{bmatrix}
</span></p>
<p>The symbol <code>-</code> indicates that the derivative is not defined because one of the neighboring pixels is out of the image boundary. We observe that the derivative is high at the edge and low elsewhere. This is a simple but effective way to detect edges in an image.</p>
<p>We can consider a derivative operator along the vertical direction that computes the difference between the vertical neighboring pixels.</p>
<p><span class="math display">
\nabla Z_{22} = Z_{1,2} - Z_{3,2}
</span></p>
<p>And, when applied to the entire image, the result is</p>
<p><span class="math display">
\begin{bmatrix}
- &amp; - &amp; - &amp; - &amp; -  &amp; - \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
- &amp; - &amp; - &amp; - &amp; - &amp; -
\end{bmatrix}
</span></p>
<p>The all entries are zero, meaning that there is no edge in the vertical direction.</p>
<p>We can combine the horizontal and vertical derivatives to get the gradient of the image. For example,</p>
<p><span class="math display">
\nabla Z_{22} = Z_{12} - Z_{32} + Z_{21} - Z_{23}
</span></p>
<p>When applied to the entire image, the result is the same as the horizontal derivative.</p>
</section>
<section id="convolution" class="level3">
<h3 class="anchored" data-anchor-id="convolution">Convolution</h3>
<p>We observe that there is a repeated pattern in the derivative computation: we are taking addition and subtraction of neighbiring pixels. This motivates us to generalize the operation to a more general form.</p>
<p><span class="math display">
\nabla Z_{22} = \sum_{i=-1}^1 \sum_{j=-1}^1 K_{h-(i+1),w-(j+1)} Z_{2+i, 2+j}
</span></p>
<p>where <span class="math inline">K</span> is a <span class="math inline">3 \times 3</span> matrix, and <span class="math inline">w=h=3</span> represent the width and height of the kernel.</p>
<p><span class="math display">
K_{\text{horizontal}} = \begin{bmatrix}
0 &amp; 0 &amp; 0 \\
-1 &amp; 0 &amp; 1 \\
0 &amp; 0 &amp; 0
\end{bmatrix},\quad
K_{\text{vertical}} = \begin{bmatrix}
0 &amp; -1 &amp; 0 \\
0 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0
\end{bmatrix}
</span></p>
<p>The operation of <span class="math inline">K</span> on the image is called <em>convolution</em>, and <span class="math inline">K</span> is called the <em>kernel</em> or <em>filter</em>. More generally, the convolution of a kernel <span class="math inline">K</span> and an image <span class="math inline">X</span> is defined as</p>
<p><span class="math display">
Y_{ij} = \sum_{p}\sum_{q} K_{pq} X_{i+p-\frac{h+1}{2}, j+q-\frac{w+1}{2}}
</span></p>
<p>where <span class="math inline">h</span> and <span class="math inline">w</span> are the height and width of the kernel, respectively.</p>
</section>
</section>
<section id="from-image-to-graph" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="from-image-to-graph"><span class="header-section-number">2</span> From Image to Graph</h2>
<section id="analogy-between-image-and-graph-data" class="level3">
<h3 class="anchored" data-anchor-id="analogy-between-image-and-graph-data">Analogy between image and graph data</h3>
<p>We can think of a convolution of an image from the perspective of networks. In the convolution of an image, a pixel is convolved with its <em>neighbors</em>. We can regard each pixel as a node, and each node is connected to its neighboring nodes (pixels) that are involved in the convolution.</p>
<p><img src="https://av-eks-lekhak.s3.amazonaws.com/media/__sized__/article_images/conv_graph-thumbnail_webp-600x300.webp" class="img-fluid"></p>
<p>Building on this analogy, we can extend the idea of convolution to general graph data. Each node has a pixel value(s) (e.g., feature vector), which is convolved with the values of its neighbors in the graph. This is the key idea of graph convolutional networks. But, there is a key difference: while the number of neighbors for an image is homogeneous, the number of neighbors for a node in a graph can be heterogeneous. Each pixel has the same number of neighbors (except for the boundary pixels), but nodes in a graph can have very different numbers of neighbors. This makes it non-trivial to define the “kernel” for graph convolution.</p>
</section>
<section id="spectral-filter-on-graphs" class="level3">
<h3 class="anchored" data-anchor-id="spectral-filter-on-graphs">Spectral filter on graphs</h3>
<p>Just like we can define a convolution on images in the frequency domain, we can also define a ‘’frequency domain’’ for graphs.</p>
<p>Consider a network of <span class="math inline">N</span> nodes, where each node has a feature variable <span class="math inline">{\mathbf x}_i \in \mathbb{R}</span>. We are interested in:</p>
<p><span class="math display">
J = \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N A_{ij}(x_i - x_j)^2,
</span></p>
<p>where <span class="math inline">A_{ij}</span> is the adjacency matrix of the graph. The quantity <span class="math inline">J</span> represents <em>the total variation</em> of <span class="math inline">x</span> between connected nodes; a small <span class="math inline">J</span> means that connected nodes have similar <span class="math inline">x</span> (low variation; low frequency), while a large <span class="math inline">J</span> means that connected nodes have very different <span class="math inline">x</span> (high variation; high frequency).</p>
<p>We can rewrite <span class="math inline">J</span> as</p>
<p><span class="math display">
J = \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N A_{ij}(x_i - x_j)^2 = {\bf x}^\top {\bf L} {\bf x},
</span></p>
<p>where <span class="math inline">{\bf L}</span> is the Laplacian matrix of the graph given by</p>
<p><span class="math display">
L_{ij} = \begin{cases}
-1 &amp; \text{if } i \text{ and } j \text{ are connected} \\
k_i &amp; \text{if } i = j \\
0 &amp; \text{otherwise}
\end{cases}.
</span></p>
<p>and <span class="math inline">{\bf x} = [x_1,x_2,\ldots, x_N]^\top</span> is a column vector of feature variables.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Detailed derivation">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Detailed derivation
</div>
</div>
<div class="callout-body-container callout-body">
<p>:tag: note :class: dropdown</p>
<p>The above derivation shows that the total variation of <span class="math inline">x</span> between connected nodes is proportional to <span class="math inline">{\bf x}^\top {\bf L} {\bf x}</span>.</p>
<p><span class="math display">
\begin{aligned}
J &amp;= \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N A_{ij}(x_i - x_j)^2 \\
&amp;= \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N \underbrace{A_{ij}\left( x_i^2 +x_j^2\right)}_{\text{symmetric}} - \sum_{i=1}^N\sum_{j=1}^N A_{ij}x_ix_j \\
&amp;= \sum_{i=1}^Nx_i^2\underbrace{\sum_{j=1}^N A_{ij}}_{\text{degree of node } i, k_i} - \sum_{i=1}^N\sum_{j=1}^N A_{ij}x_ix_j \\
&amp;= \sum_{i=1}^Nx_i^2 k_i - \sum_{i=1}^N\sum_{j=1}^N A_{ij}x_ix_j \\
&amp;= \underbrace{[x_1,x_2,\ldots, x_N]}_{{\bf x}} \underbrace{\begin{bmatrix} k_1 &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; k_2 &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \cdots &amp; k_N \end{bmatrix}}_{{\bf D}} \underbrace{\begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_N \end{bmatrix}}_{{\bf x}} - 2\underbrace{\sum_{i=1}^N\sum_{j=1}^N A_{ij}}_{{\bf x}^\top {\mathbf A} {\bf x}} {\bf x} \\
&amp;= {\bf x}^\top {\bf D} {\bf x} - {\bf x}^\top {\mathbf A} {\bf x} \\
&amp;= {\bf x}^\top {\bf L} {\bf x},
\end{aligned}
</span></p>
</div>
</div>
<p>Let us showcase the analogy between the Fourier transform and the Laplacian matrix. In the Fourier transform, a signal is decomposed into sinusoidal basis functions. Similarly, for a graph, we can decompose the variation <span class="math inline">J</span> into eigenvector bases.</p>
<p><span class="math display">
J = \sum_{i=1}^N \lambda_i  {\bf x}^\top {\mathbf u}_i {\mathbf u}_i^\top {\bf x} = \sum_{i=1}^N \lambda_i  ||{\bf x}^\top {\mathbf u}_i||^2.
</span></p>
<p>where <span class="math inline">{\mathbf u}_i</span> is the eigenvector corresponding to the eigenvalue <span class="math inline">\lambda_i</span>. - The term <span class="math inline">({\bf x}^\top {\mathbf u}_i)</span> is a dot-product between the feature vector <span class="math inline">{\bf x}</span> and the eigenvector <span class="math inline">{\mathbf u}_i</span>, which measures how much <span class="math inline">{\bf x}</span> <em>coheres</em> with eigenvector <span class="math inline">{\mathbf u}_i</span>, similar to how Fourier coefficients measure coherency with sinusoids. - Each <span class="math inline">||{\bf x}^\top {\mathbf u}_i||^2</span> is the ‘’strength’’ of <span class="math inline">{\bf x}</span> with respect to the eigenvector <span class="math inline">{\mathbf u}_i</span>, and the total variation <span class="math inline">J</span> is a weighted sum of these strengths.</p>
<p>Some eigenvectors correspond to low-frequency components, while others correspond to high-frequency components. For example, the total variation <span class="math inline">J</span> for an eigenvector <span class="math inline">{\mathbf u}_i</span> is given by</p>
<p><span class="math display">
J = \frac{1}{2} \sum_{j}\sum_{\ell} A_{j\ell}(u_{ij} - u_{i\ell})^2 = {\mathbf u}_i^\top {\mathbf L} {\mathbf u}_i = \lambda_i.
</span></p>
<p>This equation provides key insight into the meaning of eigenvalues:</p>
<ol type="1">
<li>For an eigenvector <span class="math inline">{\mathbf u}_i</span>, its eigenvalue <span class="math inline">\lambda_i</span> measures the total variation for <span class="math inline">{\mathbf u}_i</span>.</li>
<li>Large eigenvalues mean large differences between neighbors (high frequency), while small eigenvalues mean small differences (low frequency).</li>
</ol>
<p>Thus, if <span class="math inline">{\bf x}</span> aligns well with <span class="math inline">{\mathbf u}_i</span> with a large <span class="math inline">\lambda_i</span>, then <span class="math inline">{\bf x}</span> has a strong high-frequency component; if <span class="math inline">{\bf x}</span> aligns well with <span class="math inline">{\mathbf u}_i</span> with a small <span class="math inline">\lambda_i</span>, then <span class="math inline">{\bf x}</span> has strong low-frequency component.</p>
</section>
<section id="spectral-filtering" class="level3">
<h3 class="anchored" data-anchor-id="spectral-filtering">Spectral Filtering</h3>
<p>Eigenvalues <span class="math inline">\lambda_i</span> can be thought of as a <em>filter</em> that controls which frequency components pass through. Instead of using the filter associated with the Laplacian matrix, we can design a filter <span class="math inline">h(\lambda_i)</span> to control which frequency components pass through. This leads to the idea of <em>spectral filtering</em>. Two common filters are:</p>
<ol type="1">
<li><strong>Low-pass Filter</strong>: <span class="math display">h_{\text{low}}(\lambda) = \frac{1}{1 + \alpha\lambda}</span>
<ul>
<li>Preserves low frequencies (small λ)</li>
<li>Suppresses high frequencies (large λ)</li>
<li>Results in smoother signals</li>
</ul></li>
<li><strong>High-pass Filter</strong>: <span class="math display">h_{\text{high}}(\lambda) = \frac{\alpha\lambda}{1 + \alpha\lambda}</span>
<ul>
<li>Preserves high frequencies</li>
<li>Suppresses low frequencies</li>
<li>Emphasizes differences between neighbors</li>
</ul></li>
</ol>
<div id="bdb7d78e" class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>:tags: [remove<span class="op">-</span><span class="bu">input</span>]</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>sns.set_context(<span class="st">"talk"</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>lambdas <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">100</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>h_low <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> alpha <span class="op">*</span> lambdas)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>h_high <span class="op">=</span> (alpha <span class="op">*</span> lambdas) <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> alpha <span class="op">*</span> lambdas)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>sns.lineplot(x<span class="op">=</span>lambdas, y<span class="op">=</span>h_low, label<span class="op">=</span><span class="st">"Low-pass filter"</span>, ax<span class="op">=</span>axes[<span class="dv">0</span>])</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].legend(frameon<span class="op">=</span><span class="va">False</span>).remove()</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>sns.lineplot(x<span class="op">=</span>lambdas, y<span class="op">=</span>h_high, label<span class="op">=</span><span class="st">"High-pass filter"</span>, ax<span class="op">=</span>axes[<span class="dv">1</span>])</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].legend(frameon<span class="op">=</span><span class="va">False</span>).remove()</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">"Low-pass filter"</span>)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">"High-pass filter"</span>)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>fig.text(<span class="fl">0.5</span>, <span class="fl">0.01</span>, <span class="st">"Eigenvalue $</span><span class="er">\</span><span class="st">lambda$"</span>, ha<span class="op">=</span><span class="st">"center"</span>)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_ylabel(<span class="st">"Filter response $h(</span><span class="er">\</span><span class="st">lambda)$"</span>)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>sns.despine()</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="example" class="level3">
<h3 class="anchored" data-anchor-id="example">Example</h3>
<p>Let us showcase the idea of spectral filtering with a simple example with the karate club network.</p>
<div id="b2905ced" class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>:tags: [remove<span class="op">-</span><span class="bu">input</span>]</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> igraph <span class="im">as</span> ig</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> sparse</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib <span class="im">as</span> mpl</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>G <span class="op">=</span> ig.Graph.Famous(<span class="st">"Zachary"</span>)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> G.get_adjacency_sparse()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We will first compute the laplacian matrix and its eigendecomposition.</p>
<div id="7853c03e" class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute Laplacian matrix</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>deg <span class="op">=</span> np.array(A.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>)).reshape(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>D <span class="op">=</span> sparse.diags(deg)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>L <span class="op">=</span> D <span class="op">-</span> A</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute eigendecomposition</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>evals, evecs <span class="op">=</span> np.linalg.eigh(L.toarray())</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Sort eigenvalues and eigenvectors</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>order <span class="op">=</span> np.argsort(evals)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>evals <span class="op">=</span> evals[order]</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>evecs <span class="op">=</span> evecs[:, order]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now, let’s create a low-pass and high-pass filter.</p>
<div id="ef823b44" class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>L_low <span class="op">=</span> evecs <span class="op">@</span> np.diag(<span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> alpha <span class="op">*</span> evals)) <span class="op">@</span> evecs.T</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>L_high <span class="op">=</span> evecs <span class="op">@</span> np.diag(alpha <span class="op">*</span> evals <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> alpha <span class="op">*</span> evals)) <span class="op">@</span> evecs.T</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Size of low-pass filter:"</span>, L_low.shape)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Size of high-pass filter:"</span>, L_high.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Notice that the high-pass filter and low-pass filter are matrices of the same size as the adjacency matrix <span class="math inline">A</span>, which defines a ‘convolution’ on the graph as follows:</p>
<p><span class="math display">
{\bf x}' = {\bf L}_{\text{low}} {\bf x} \quad \text{or} \quad {\bf x}' = {\bf L}_{\text{high}} {\bf x}.
</span></p>
<p>where <span class="math inline">{\bf L}_{\text{low}}</span> and <span class="math inline">{\bf L}_{\text{high}}</span> are the low-pass and high-pass filters, respectively, and <span class="math inline">{\bf x}'</span> is the convolved feature vector.</p>
<p>Now, let’s see how these filters work. Our first example is a random feature vector.</p>
<div id="82998e91" class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Random feature vector</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.random.randn(A.shape[<span class="dv">0</span>], <span class="dv">1</span>)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Convolve with low-pass filter</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>x_low <span class="op">=</span> L_low <span class="op">@</span> x</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Convolve with high-pass filter</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>x_high <span class="op">=</span> L_high <span class="op">@</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let us visualize the results.</p>
<div id="1b852b91" class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>:tags: [hide<span class="op">-</span><span class="bu">input</span>]</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">5</span>))</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>palette <span class="op">=</span> sns.color_palette(<span class="st">"viridis"</span>, as_cmap<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>norm <span class="op">=</span> mpl.colors.Normalize(vmin<span class="op">=-</span><span class="fl">0.3</span>, vmax<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Original</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>values <span class="op">=</span> x.reshape(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>values <span class="op">/=</span> np.linalg.norm(values)</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>ig.plot(G, vertex_color<span class="op">=</span>[palette(norm(x)) <span class="cf">for</span> x <span class="kw">in</span> values], bbox<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">500</span>, <span class="dv">500</span>), vertex_size<span class="op">=</span><span class="dv">20</span>, target<span class="op">=</span>axes[<span class="dv">0</span>])</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">"Original"</span>)</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Low-pass filter applied</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>values <span class="op">=</span> L_low <span class="op">@</span> x</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>values <span class="op">/=</span> np.linalg.norm(values)</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>values <span class="op">=</span> values.reshape(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>ig.plot(G, vertex_color<span class="op">=</span>[palette(norm(x)) <span class="cf">for</span> x <span class="kw">in</span> values], bbox<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">500</span>, <span class="dv">500</span>), vertex_size<span class="op">=</span><span class="dv">20</span>, target<span class="op">=</span>axes[<span class="dv">1</span>])</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">"Low-pass filter"</span>)</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a><span class="co"># High-pass filter applied</span></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>values <span class="op">=</span> L_high <span class="op">@</span> x</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>values <span class="op">/=</span> np.linalg.norm(values)</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>values <span class="op">=</span> values.reshape(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>ig.plot(G, vertex_color<span class="op">=</span>[palette(norm(x)) <span class="cf">for</span> x <span class="kw">in</span> values], bbox<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">500</span>, <span class="dv">500</span>), vertex_size<span class="op">=</span><span class="dv">20</span>, target<span class="op">=</span>axes[<span class="dv">2</span>])</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].set_title(<span class="st">"High-pass filter"</span>)</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We observe that the low-pass filter results in smoother <span class="math inline">{\bf x}</span> between connected nodes (i.e., neighboring nodes have similar <span class="math inline">{\bf x}</span>). The original <span class="math inline">{\bf x}</span> and <span class="math inline">{\bf x}'_{\text{low}}</span> are very similar because random variables are high-frequency components. In contrast, when we apply the high-pass filter, <span class="math inline">{\bf x}'_{\text{high}}</span> is similar to <span class="math inline">{\bf x}</span> because the high-frequency components are not filtered.</p>
<p>Let’s now use an eigenvector as our feature vector <span class="math inline">{\bf x}</span>.</p>
<div id="24438618" class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>:tags: [hide<span class="op">-</span><span class="bu">input</span>]</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>eigen_centrality <span class="op">=</span> np.array(G.eigenvector_centrality()).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>low_pass_eigen <span class="op">=</span> L_low <span class="op">@</span> eigen_centrality</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>high_pass_eigen <span class="op">=</span> L_high <span class="op">@</span> eigen_centrality</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">5</span>))</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>palette <span class="op">=</span> sns.color_palette(<span class="st">"viridis"</span>, as_cmap<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>norm <span class="op">=</span> mpl.colors.Normalize(vmin<span class="op">=-</span><span class="dv">0</span>, vmax<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>values <span class="op">=</span> eigen_centrality.reshape(<span class="op">-</span><span class="dv">1</span>)<span class="co"># high_pass_random.reshape(-1)</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>values <span class="op">/=</span> np.linalg.norm(values)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>values <span class="op">=</span> values.reshape(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>ig.plot(G, vertex_color<span class="op">=</span>[palette(norm(x)) <span class="cf">for</span> x <span class="kw">in</span> values], bbox<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">500</span>, <span class="dv">500</span>), vertex_size<span class="op">=</span><span class="dv">20</span>, target<span class="op">=</span>axes[<span class="dv">0</span>])</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">"Original"</span>)</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>values <span class="op">=</span> low_pass_eigen.reshape(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>values <span class="op">/=</span> np.linalg.norm(values)</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>values <span class="op">=</span> values.reshape(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>ig.plot(G, vertex_color<span class="op">=</span>[palette(norm(x)) <span class="cf">for</span> x <span class="kw">in</span> values], bbox<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">500</span>, <span class="dv">500</span>), vertex_size<span class="op">=</span><span class="dv">20</span>, target<span class="op">=</span>axes[<span class="dv">1</span>])</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">"Low-pass filter"</span>)</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>values <span class="op">=</span> high_pass_eigen.reshape(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>values <span class="op">/=</span> np.linalg.norm(values)</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>ig.plot(G, vertex_color<span class="op">=</span>[palette(norm(x)) <span class="cf">for</span> x <span class="kw">in</span> values], bbox<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">500</span>, <span class="dv">500</span>), vertex_size<span class="op">=</span><span class="dv">20</span>, target<span class="op">=</span>axes[<span class="dv">2</span>])</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].set_title(<span class="st">"High-pass filter"</span>)</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The high-pass filter increases the contrast of the eigenvector centrality, emphasizing the differences between nodes. On the other hand, the low-pass filter smooths out the eigenvector centrality.</p>
</section>
</section>
<section id="graph-convolutional-networks" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="graph-convolutional-networks"><span class="header-section-number">3</span> Graph Convolutional Networks</h2>
<p>We have seen that spectral filters give us a principled way to think about “convolution” on irregular graph structures, and controlling the frequency components brings out different aspects of the data. We now go one step further: instead of designing filters by hand, we can learn them from data for specific tasks.</p>
<section id="spectral-graph-convolutional-networks" class="level3">
<h3 class="anchored" data-anchor-id="spectral-graph-convolutional-networks">Spectral Graph Convolutional Networks</h3>
<p>A simplest form of learnable spectral filter is given by</p>
<p><span class="math display">
{\bf L}_{\text{learn}} = \sum_{k=1}^K \theta_k {\mathbf u}_k {\mathbf u}_k^\top,
</span></p>
<p>where <span class="math inline">{\mathbf u}_k</span> are the eigenvectors and <span class="math inline">\theta_k</span> are the learnable parameters. The variable <span class="math inline">K</span> is the number of eigenvectors used (i.e., the rank of the filter). The weight <span class="math inline">\theta_k</span> is learned to maximize the performance of the task at hand.</p>
<p>Building on this idea, {footcite}<code>bruna2014spectral</code> added a nonlinearity to the filter and proposed a spectral convolutional neural network (GCN) by</p>
<p><span class="math display">
{\bf x}^{(\ell+1)} = h\left( L_{\text{learn}} {\bf x}^{(\ell)}\right),
</span></p>
<p>where <span class="math inline">h</span> is an activation function, and <span class="math inline">{\bf x}^{(\ell)}</span> is the feature vector of the <span class="math inline">\ell</span>-th convolution. They further extend this idea to convolve on multidimensional feature vectors, <span class="math inline">{\bf X} \in \mathbb{R}^{N \times f_{\text{in}}}</span> to produce new feature vectors of different dimensionality, <span class="math inline">{\bf X}' \in \mathbb{R}^{N \times f_{\text{out}}}</span>.</p>
<p><span class="math display">
\begin{aligned}
{\bf X}^{(\ell+1)}_i &amp;= h\left( \sum_j L_{\text{learn}}^{(i,j)} {\bf X}^{(\ell)}_j\right),\quad \text{where} \quad L^{(i,j)}_{\text{learn}} = \sum_{k=1}^K \theta_{k, (i,j)} {\mathbf u}_k {\mathbf u}_k^\top,
\end{aligned}
</span></p>
<p>Notice that the learnable filter <span class="math inline">L_{\text{learn}}^{(i,j)}</span> is defined for each pair of input <span class="math inline">i</span> and output <span class="math inline">j</span> dimensions.</p>
<pre class="{note}"><code>Many GCNs simple when it comes to implementation despite the complicated formula. And this is one of my ways to learn GNNs. Check out the [Appendix for the Python implementation](appendix.md).
</code></pre>
</section>
<section id="from-spectral-to-spatial" class="level3">
<h3 class="anchored" data-anchor-id="from-spectral-to-spatial">From Spectral to Spatial</h3>
<p>Spectral GCNs are mathematically elegant but have two main limitations: 1. <strong>Computational Limitation</strong>: Computing the spectra of the Laplacian is expensive <span class="math inline">{\cal O}(N^3)</span> and prohibitive for large graphs 2. <strong>Spatial Locality</strong>: The learned filters are not spatially localized. A node can be influenced by all other nodes in the graph.</p>
<p>These two limitations motivate the development of spatial GCNs.</p>
</section>
<section id="chebnet" class="level3">
<h3 class="anchored" data-anchor-id="chebnet">ChebNet</h3>
<p>ChebNet {footcite}<code>defferrard2016convolutional</code> is one of the earliest spatial GCNs that bridges the gap between spectral and spatial domains. The key idea is to leverage Chebyshev polynomials to approximate <span class="math inline">{\bf L}_{\text{learn}}</span> by</p>
<p><span class="math display">
{\bf L}_{\text{learn}} \approx \sum_{k=0}^{K-1} \theta_k T_k(\tilde{{\bf L}}), \quad \text{where} \quad \tilde{{\bf L}} = \frac{2}{\lambda_{\text{max}}}{\bf L} - {\bf I},
</span></p>
<p>where <span class="math inline">\tilde{{\bf L}}</span> is the scaled and normalized Laplacian matrix in order to have eigenvalues in the range of <span class="math inline">[-1,1]</span>. The Chebyshev polynomials <span class="math inline">T_k(\tilde{{\bf L}})</span> transforms the eigenvalues <span class="math inline">\tilde{{\bf L}}</span> to the following recursively:</p>
<p><span class="math display">
\begin{aligned}
T_0(\tilde{{\bf L}}) &amp;= {\bf I} \\
T_1(\tilde{{\bf L}}) &amp;= \tilde{{\bf L}} \\
T_k(\tilde{{\bf L}}) &amp;= 2\tilde{{\bf L}} T_{k-1}(\tilde{{\bf L}}) - T_{k-2}(\tilde{{\bf L}})
\end{aligned}
</span></p>
<p>We then replace <span class="math inline">{\bf L}_{\text{learn}}</span> in the original spectral GCN with the Chebyshev polynomial approximation:</p>
<p><span class="math display">
{\bf x}^{(\ell+1)} = h\left( \sum_{k=0}^{K-1} \theta_k T_k(\tilde{{\bf L}}){\bf x}^{(\ell)}\right),
</span></p>
<p>where: - <span class="math inline">T_k(\tilde{{\bf L}})</span> applies the k-th Chebyshev polynomial to the scaled Laplacian matrix - <span class="math inline">\theta_k</span> are the learnable parameters - K is the order of the polynomial (typically small, e.g., K=3)</p>
</section>
<section id="graph-convolutional-networks-by-kipf-and-welling" class="level3">
<h3 class="anchored" data-anchor-id="graph-convolutional-networks-by-kipf-and-welling">Graph Convolutional Networks by Kipf and Welling</h3>
<p>While ChebNet offers a principled way to approximate spectral convolutions, Kipf and Welling (2017) {footcite}<code>kipf2017semi</code> proposed an even simpler and highly effective variant called <strong>Graph Convolutional Networks (GCN)</strong>.</p>
<section id="first-order-approximation" class="level5">
<h5 class="anchored" data-anchor-id="first-order-approximation">First-order Approximation</h5>
<p>The key departure is to use the first-order approximation of the Chebyshev polynomials.</p>
<p><span class="math display">
g_{\theta'} * x \approx \theta'_0x + \theta'_1(L - I_N)x = \theta'_0x - \theta'_1D^{-\frac{1}{2}}AD^{-\frac{1}{2}}x
</span></p>
<p>This is crude approximation but it leads to a much simpler form, leaving only two learnable parameters, instead of <span class="math inline">K</span> parameters in the original ChebNet.</p>
<p>Additionally, they further simplify the formula by using the same <span class="math inline">\theta</span> for both remaining parameters (i.e., <span class="math inline">\theta_0 = \theta</span> and <span class="math inline">\theta_1 = -\theta</span>). The result is the following convolutional filter:</p>
<p><span class="math display">
g_{\theta} * x \approx \theta(I_N + D^{-\frac{1}{2}}AD^{-\frac{1}{2}})x
</span></p>
<p>While this is a very simple filter, one can stack multiple layers of convolutions to perform high-order graph convolutions.</p>
</section>
<section id="deep-gcns-can-suffer-from-over-smoothing" class="level5">
<h5 class="anchored" data-anchor-id="deep-gcns-can-suffer-from-over-smoothing">Deep GCNs can suffer from over-smoothing</h5>
<p>GCN models can be deep, and when they are too deep, they start suffering from an ill-posed problem called <em>gradient vanishing/exploding</em>, where the gradients of the loss function becomes too small or too large to update the model parameters. It is a common problem in deep learning.</p>
<p>To facilitate the training of deep GCNs, the authors introduce a very simple trick called <em>renormalization</em>. The idea is to add self-connections to the graph:</p>
<p><span class="math display">
\tilde{A} = A + I_N, \quad \text{and} \quad \tilde{D}_{ii} = \sum_j \tilde{A}_{ij}
</span></p>
<p>And use <span class="math inline">\tilde{A}</span> and <span class="math inline">\tilde{D}</span> to form the convolutional filter.</p>
<p>Altogether, this leads to the following layer-wise propagation rule:</p>
<p><span class="math display">X^{(\ell+1)} = \sigma(\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}X^{(\ell)}W^{(\ell)})</span></p>
<p>where: - <span class="math inline">X^{(\ell)}</span> is the matrix of node features at layer <span class="math inline">\ell</span> - <span class="math inline">W^{(\ell)}</span> is the layer’s trainable weight matrix - <span class="math inline">\sigma</span> is a nonlinear activation function (e.g., ReLU)</p>
<p>These simplifications offer several advantages: - <strong>Efficiency</strong>: Linear complexity in number of edges - <strong>Localization</strong>: Each layer only aggregates information from immediate neighbors - <strong>Depth</strong>: Fewer parameters allow building deeper models - <strong>Performance</strong>: Despite (or perhaps due to) its simplicity, it often outperforms more complex models</p>
<div class="callout callout-style-default callout-note callout-titled" title="Exercise">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise
</div>
</div>
<div class="callout-body-container callout-body">
<p>:class: note</p>
<p>Let’s implement a simple GCN model for node classification. <a href="../../../notebooks/exercise-m09-graph-neural-net.ipynb">Coding Exercise</a></p>
</div>
</div>
</section>
</section>
</section>
<section id="popular-graph-neural-networks" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="popular-graph-neural-networks"><span class="header-section-number">4</span> Popular Graph Neural Networks</h2>
<p>In this section, we will introduce three popular GNNs: GraphSAGE, Graph Attention Networks (GAT), and Graph Isomorphism Network (GIN).</p>
<section id="graphsage-sample-and-aggregate" class="level3">
<h3 class="anchored" data-anchor-id="graphsage-sample-and-aggregate">GraphSAGE: Sample and Aggregate</h3>
<p>GraphSAGE {footcite}<code>hamilton2017graphsage</code> introduced a different GCN that can be <strong><em>generalized to unseen nodes</em></strong> (they called it “inductive”). While previous approaches like ChebNet and GCN operate on the entire graph, GraphSAGE proposes an inductive framework that generates embeddings by sampling and aggregating features from a node’s neighborhood.</p>
<p><img src="https://theaisummer.com/static/02e23adc75fe68e5dd249a94f3c1e8cc/c483d/graphsage.png" class="img-fluid"></p>
</section>
<section id="key-ideas" class="level3">
<h3 class="anchored" data-anchor-id="key-ideas">Key Ideas</h3>
<p>GraphSAGE involves two key ideas: (1) sampling and (2) aggregation.</p>
<section id="neighborhood-sampling" class="level5">
<h5 class="anchored" data-anchor-id="neighborhood-sampling">Neighborhood Sampling</h5>
<p>The key idea is the <em>neighborhood sampling</em>. Instead of using all neighbors, GraphSAGE samples a fixed-size set of neighbors for each node. This controls memory complexity, a key limitation of the previous GNNs.</p>
<p>Another key advantage of neighborhood sampling is that it enables GraphSAGE to handle dynamic, growing networks. Consider a citation network where new papers (nodes) are continuously added. Traditional GCNs would need to recompute filters for the entire network with each new addition. In contrast, GraphSAGE can immediately generate embeddings for new nodes by simply sampling their neighbors, without any retraining or recomputation.</p>
</section>
<section id="aggregation" class="level5">
<h5 class="anchored" data-anchor-id="aggregation">Aggregation</h5>
<p>Another key idea is the <em>aggregation</em>. GraphSAGE makes a distinction between self-information and neighborhood information. While previous GNNs treat them equally and aggregate them, GraphSAGE treats them differently. Specifically, GraphSAGE introduces an additional step: it concatenates the self-information and the neighborhood information as the input of the convolution.</p>
<p><span class="math display">
Z_v = \text{CONCAT}(X_v, X_{\mathcal{N}(v)})
</span></p>
<p>where <span class="math inline">X_v</span> is the feature of the node itself and <span class="math inline">X_{\mathcal{N}(v)}</span> is the aggregation of the features of its neighbors. GraphSAGE introduces different ways to aggregate information from neighbors:</p>
<p><span class="math display">X_{\mathcal{N}(v)} = \text{AGGREGATE}_k(\{X_u, \forall u \in \mathcal{N}(v)\})</span></p>
<p>Common aggregation functions include: - Mean aggregator: <span class="math inline">\text{AGGREGATE} = \text{mean}(\{h_u, \forall u \in \mathcal{N}(v)\})</span> - Max-pooling: <span class="math inline">\text{AGGREGATE} = \max(\{\sigma(W_{\text{pool}}h_u + b), \forall u \in \mathcal{N}(v)\})</span> - LSTM aggregator: Apply LSTM to randomly permuted neighbors</p>
<p>The concatenated feature <span class="math inline">Z_v</span> is normalized by the L2 norm.</p>
<p><span class="math display">
\hat{Z}_v = \frac{Z_v}{\|Z_v\|_2}
</span></p>
<p>and then fed into the convolution.</p>
<p><span class="math display">
X_v^k = \sigma(W^k \hat{Z}_v + b^k)
</span></p>
</section>
</section>
<section id="graph-attention-networks-gat-differentiate-individual-neighbors" class="level3">
<h3 class="anchored" data-anchor-id="graph-attention-networks-gat-differentiate-individual-neighbors">Graph Attention Networks (GAT): Differentiate Individual Neighbors</h3>
<p>A key innovation of GraphSAGE is to treat the self and neighborhood information differently. But should all neighbors be treated equally? Graph Attention Networks (GAT) address this by letting the model learn which neighbors to pay attention to.</p>
</section>
<section id="attention-mechanism" class="level3">
<h3 class="anchored" data-anchor-id="attention-mechanism">Attention Mechanism</h3>
<p><img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-08_at_7.55.32_PM_vkdDcDx.png" class="img-fluid"></p>
<p>The core idea is beautifully simple: instead of using fixed weights like GCN, let’s learn attention weights <span class="math inline">\alpha_{ij}</span> that determine how much node <span class="math inline">i</span> should attend to node <span class="math inline">j</span>. These weights are computed dynamically based on node features:</p>
<p><span class="math display">
\alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k \in \mathcal{N}(i)} \exp(e_{ik})}
</span></p>
<p>where <span class="math inline">e_{ij}</span> represents the importance of the edge between node <span class="math inline">i</span> and node <span class="math inline">j</span>. Variable <span class="math inline">e_{ij}</span> is a <em>learnable</em> parameter and can be negative, and the exponential function is applied to transform it to a non-negative value, with the normalization term <span class="math inline">\sum_{k \in \mathcal{N}(i)} \exp(e_{ik})</span> to ensure the weights sum to 1.</p>
<p>How to compute <span class="math inline">e_{ij}</span>? One simple choice is to use a neural network with a shared weight matrix <span class="math inline">W</span> and a LeakyReLU activation function. Specifically:</p>
<ol type="1">
<li>Let’s focus on computing <span class="math inline">e_{ij}</span> for node <span class="math inline">i</span> and its neighbor <span class="math inline">j</span>.</li>
<li>We use a shared weight matrix <span class="math inline">W</span> to transform the features of node <span class="math inline">i</span> and <span class="math inline">j</span>. <span class="math display">
\mathbf{\tilde h}_i  = \mathbf{h}_i, \quad \mathbf{\tilde h}_j  = W\mathbf{h}_j
</span></li>
<li>We concatenate the transformed features and apply a LeakyReLU activation function.</li>
</ol>
<p><span class="math display">
e_{ij} = \text{LeakyReLU}(\mathbf{a}^T[\mathbf{\tilde h}_i, \mathbf{\tilde h}_j])
</span></p>
<p>where <span class="math inline">\mathbf{a}</span> is a trainable parameter vector that sums the two transformed features.</p>
<p>Once we have these attention weights, the node update is straightforward - just a weighted sum of neighbor features:</p>
<p><span class="math display">\mathbf{h}'_i = \sigma\left(\sum_{j \in \mathcal{N}(i) \cup \{i\}} \alpha_{ij}{\bf W}_{\text{feature}}\mathbf{h}_j\right)</span></p>
<p>where <span class="math inline">{\bf W}_{\text{feature}}</span> is a trainable weight matrix. To stabilize training, GAT uses multiple attention heads and concatenates their outputs:</p>
<p><span class="math display">\mathbf{h}'_i = \parallel_{k=1}^K \sigma\left(\sum_{j \in \mathcal{N}(i) \cup \{i\}} \alpha_{ij}^k{\bf W}^k_{\text{feature}}\mathbf{h}_j\right)</span></p>
</section>
<section id="graph-isomorphism-network-gin-differentiate-the-aggregation" class="level3">
<h3 class="anchored" data-anchor-id="graph-isomorphism-network-gin-differentiate-the-aggregation">Graph Isomorphism Network (GIN): Differentiate the Aggregation</h3>
<p>Graph Isomorphism Networks (GIN) is another popular GNN that born out of a question: what is the maximum discriminative power achievable by Graph Neural Networks? The answer lies in its theoretical connection to <strong>the Weisfeiler-Lehman (WL) test</strong>, a powerful algorithm for graph isomorphism testing.</p>
</section>
<section id="weisfeiler-lehman-test" class="level3">
<h3 class="anchored" data-anchor-id="weisfeiler-lehman-test">Weisfeiler-Lehman Test</h3>
<p>Are two graphs structurally identical? Graph isomorphism testing determines if two graphs are structurally identical, with applications in graph classification, clustering, and other tasks.</p>
<p><img src="https://i.sstatic.net/j5sGu.png" class="img-fluid"></p>
<p>While the general problem has no known polynomial-time solution, the WL test is an efficient heuristic that works well in practice. The WL test iteratively refines node labels by hashing the multiset of neighboring labels</p>
<p><img src="../figs/weisfeiler-lehman-test.jpg" class="img-fluid"></p>
<p>The WL test works as follows:</p>
<ol type="1">
<li>Assign all nodes the same initial label.</li>
<li>For each node, collect the labels of all its neighbors and <em>aggregate them</em> into a hash (e.g., new label). For example, the top node gets {0} from its neighbors, resulting in a collection {0,0}. A new label is created via a hash function <span class="math inline">h</span> that maps {0, {0, 0}} to a new label 1.</li>
<li>Repeat the process for a fixed number of iterations or until convergence.</li>
</ol>
<p>Here is the implementation of the WL test in Python:</p>
<div id="fb269e77" class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>:tags: [hide<span class="op">-</span><span class="bu">input</span>]</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> sparse</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> weisfeiler_lehman_test(A, num_iterations):</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    n_nodes <span class="op">=</span> A.shape[<span class="dv">0</span>]</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    labels <span class="op">=</span> np.zeros(n_nodes, dtype<span class="op">=</span><span class="bu">int</span>)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    color_map <span class="op">=</span> {}</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    hash_fn <span class="op">=</span> <span class="kw">lambda</span> x: color_map.setdefault(x, <span class="bu">len</span>(color_map))</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_iterations):</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Go through each node</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>        labels_old <span class="op">=</span> labels.copy()</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_nodes):</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Collect the labels of all neighbors</span></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>            neighbors <span class="op">=</span> A[i].nonzero()[<span class="dv">1</span>]</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>            neighbor_labels <span class="op">=</span> labels_old[neighbors]</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Count the frequency of each label</span></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>            unique, counts <span class="op">=</span> np.unique(neighbor_labels, return_counts<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Create a hash key by converting the frequency dictionary to a string</span></span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>            hash_key <span class="op">=</span> <span class="bu">str</span>({unique[j]: counts[j] <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(unique))})</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Create a new label by hashing the frequency dictionary</span></span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>            label <span class="op">=</span> hash_fn(hash_key)</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>            labels[i] <span class="op">=</span> label</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Check convergence</span></span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>        unique, counts <span class="op">=</span> np.unique(labels, return_counts<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a>        unique_old, counts_old <span class="op">=</span> np.unique(labels_old, return_counts<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> np.array_equal(np.sort(counts), np.sort(counts_old)):</span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> labels</span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a>edge_list <span class="op">=</span> [(<span class="dv">0</span>, <span class="dv">1</span>), (<span class="dv">1</span>, <span class="dv">2</span>), (<span class="dv">2</span>, <span class="dv">0</span>), (<span class="dv">3</span>, <span class="dv">4</span>), (<span class="dv">4</span>, <span class="dv">5</span>), (<span class="dv">5</span>, <span class="dv">3</span>)]</span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> sparse.csr_matrix(</span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a>    ([<span class="dv">1</span>] <span class="op">*</span> <span class="bu">len</span>(edge_list), ([e[<span class="dv">0</span>] <span class="cf">for</span> e <span class="kw">in</span> edge_list], [e[<span class="dv">1</span>] <span class="cf">for</span> e <span class="kw">in</span> edge_list])),</span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a>    shape<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">6</span>),</span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> A <span class="op">+</span> A.T</span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a>A.sort_indices()</span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-48"><a href="#cb9-48" aria-hidden="true" tabindex="-1"></a>weisfeiler_lehman_test(A, A.shape[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>After these iterations: - Nodes with the same label are structurally identical, meaning that they are indistinguishable unless we label them differently. - Two graphs are structurally identical if and only if they have the same node labels after the WL test.</p>
<p>The WL test is a heuristic and can fail on some graphs. For example, it cannot distinguish regular graphs with the same number of nodes and edges.</p>
<pre class="{note}"><code>The WL test above is called the 1-WL test. There are higher-order WL tests that can distinguish more graphs, which are the basis of advanced GNNs.
Check out [this note](https://www.moldesk.net/blog/weisfeiler-lehman-isomorphism-test/)</code></pre>
</section>
<section id="gin" class="level3">
<h3 class="anchored" data-anchor-id="gin">GIN</h3>
<p>GIN {footcite}<code>xu2018how</code> is a GNN that is based on the WL test. The key idea is to focus on the parallel between the WL test and the GNN update rule. - In the WL test, we iteratively collect the labels of neighbors and aggregate them through a <em>hash function</em>. - In the GraphSAGE and GAT, the labels are the nodes’ features, and the aggregation is some arithmetic operations such as mean or max.</p>
<p>The key difference is that the hash function in the WL test always distinguishes different sets of neighbors’ labels, while the aggregation in GraphSAGE and GAT does not always do so. For example, if all nodes have the same feature (e.g., all 1), the aggregation by the mean or max will result in the same value for all nodes, whereas the hash function in the WL test can still distinguish different sets of neighbors’ labels by <em>the count of each label</em>.</p>
<p>The resulting convolution update rule is:</p>
<p><span class="math display">
h_v^{(k+1)} = \text{MLP}^{(k)}\left((1 + \epsilon^{(k)}) \cdot h_v^{(k)} + \sum_{u \in \mathcal{N}(v)} h_u^{(k)}\right)
</span></p>
<p>where <span class="math inline">\text{MLP}^{(k)}</span> is a multi-layer perceptron (MLP) with <span class="math inline">k</span> layers, and <span class="math inline">\epsilon^{(k)}</span> is a fixed or trainable parameter.</p>
<pre class="{footbibliography}"><code></code></pre>


<!-- -->

</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/skojaku\.github\.io\/adv-net-sci\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../m09-graph-neural-networks/01-concepts.html" class="pagination-link" aria-label="Advanced Topics in Network Science">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Advanced Topics in Network Science</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../m09-graph-neural-networks/03-exercises.html" class="pagination-link" aria-label="Advanced Topics in Network Science">
        <span class="nav-page-text">Advanced Topics in Network Science</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb12" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="fu"># Coding: Graph Neural Networks Implementation</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="fu">## Preliminaries: Image Processing</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>Graph Neural Networks are a type of neural network for graph data. node2vec and deepwalk stem from the idea of language modeling.</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>In this module, we will focus on another branch of graph neural networks that stem from image processing.</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="fu">### Edge Detection Problem in Image Processing</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>Edge detection is a classical problem in image processing. The goal is to identify the boundaries of objects in an image.</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>![](https://media.geeksforgeeks.org/wp-content/uploads/20240616211411/Screenshot-(85).webp)</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>To approach the problem, let us first remind that an image is a matrix of pixels. Each pixel has RGB values, each of which represents the intensity of red, green, and blue color. To simplify the problem, we focus on grayscale images, in which each pixel has only one value representing the brightness. In this case, an image can be represented as a 2D matrix, where each element in the matrix represents the brightness of a pixel.</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a><span class="al">![](https://ai.stanford.edu/~syyeung/cvweb/Pictures1/imagematrix.png)</span></span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a><span class="fu">### An example</span></span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>Human eyes are very sensitive to brightness changes. An edge in an image appears when there is a *significant brightness change between adjacent pixels*. To be more concrete, let's consider a small example consisting of 6x6 pixels, with a vertical line from the top to the bottom, where the brightness is higher than the neighboring pixels. This is an edge we want to detect.</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>X = \begin{bmatrix}</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>10 &amp; 10 &amp; 80 &amp; 10 &amp; 10 &amp; 10 <span class="sc">\\</span></span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a>10 &amp; 10 &amp; 80 &amp; 10 &amp; 10 &amp; 10 <span class="sc">\\</span></span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a>10 &amp; 10 &amp; 80 &amp; 10 &amp; 10 &amp; 10 <span class="sc">\\</span></span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a>10 &amp; 10 &amp; 80 &amp; 10 &amp; 10 &amp; 10 <span class="sc">\\</span></span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a>10 &amp; 10 &amp; 80 &amp; 10 &amp; 10 &amp; 10 <span class="sc">\\</span></span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a>10 &amp; 10 &amp; 80 &amp; 10 &amp; 10 &amp; 10</span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}</span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a>Let's zoom on the pixel at (3, 3) and its surrounding pixels.</span>
<span id="cb12-34"><a href="#cb12-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-35"><a href="#cb12-35" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-36"><a href="#cb12-36" aria-hidden="true" tabindex="-1"></a>Z = \begin{bmatrix}</span>
<span id="cb12-37"><a href="#cb12-37" aria-hidden="true" tabindex="-1"></a>10 &amp; 80 &amp; 10 <span class="sc">\\</span></span>
<span id="cb12-38"><a href="#cb12-38" aria-hidden="true" tabindex="-1"></a>\textcolor{blue}{10} &amp; \textcolor{red}{80} &amp; \textcolor{purple}{10} <span class="sc">\\</span></span>
<span id="cb12-39"><a href="#cb12-39" aria-hidden="true" tabindex="-1"></a>10 &amp; 80 &amp; 10</span>
<span id="cb12-40"><a href="#cb12-40" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}</span>
<span id="cb12-41"><a href="#cb12-41" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-42"><a href="#cb12-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-43"><a href="#cb12-43" aria-hidden="true" tabindex="-1"></a>where the central pixel is highlighted in red. Since we are interested in the edge which is a sudden change in brightness along the horizontal direction, we take a derivative at the central pixel by</span>
<span id="cb12-44"><a href="#cb12-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-45"><a href="#cb12-45" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-46"><a href="#cb12-46" aria-hidden="true" tabindex="-1"></a>\nabla Z_{22} = \textcolor{blue}{Z_{2,1}} - \textcolor{purple}{Z_{2,3}}</span>
<span id="cb12-47"><a href="#cb12-47" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-48"><a href="#cb12-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-49"><a href="#cb12-49" aria-hidden="true" tabindex="-1"></a>Following the same process, we can compute the derivative at all pixels, which gives us the (horizontal) derivative of the image.</span>
<span id="cb12-50"><a href="#cb12-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-51"><a href="#cb12-51" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-52"><a href="#cb12-52" aria-hidden="true" tabindex="-1"></a>\begin{bmatrix}</span>
<span id="cb12-53"><a href="#cb12-53" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>&amp; -70 &amp; 0 &amp; 70 &amp; 0 &amp; - <span class="sc">\\</span></span>
<span id="cb12-54"><a href="#cb12-54" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>&amp; -70 &amp; 0 &amp; 70 &amp; 0 &amp; - <span class="sc">\\</span></span>
<span id="cb12-55"><a href="#cb12-55" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>&amp; -70 &amp; 0 &amp; 70 &amp; 0 &amp; - <span class="sc">\\</span></span>
<span id="cb12-56"><a href="#cb12-56" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>&amp; -70 &amp; 0 &amp; 70 &amp; 0 &amp; - <span class="sc">\\</span></span>
<span id="cb12-57"><a href="#cb12-57" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>&amp; -70 &amp; 0 &amp; 70 &amp; 0 &amp; -</span>
<span id="cb12-58"><a href="#cb12-58" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}</span>
<span id="cb12-59"><a href="#cb12-59" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-60"><a href="#cb12-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-61"><a href="#cb12-61" aria-hidden="true" tabindex="-1"></a>The symbol <span class="in">`-`</span> indicates that the derivative is not defined because one of the neighboring pixels is out of the image boundary.</span>
<span id="cb12-62"><a href="#cb12-62" aria-hidden="true" tabindex="-1"></a>We observe that the derivative is high at the edge and low elsewhere. This is a simple but effective way to detect edges in an image.</span>
<span id="cb12-63"><a href="#cb12-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-64"><a href="#cb12-64" aria-hidden="true" tabindex="-1"></a>We can consider a derivative operator along the vertical direction that computes the difference between the vertical neighboring pixels.</span>
<span id="cb12-65"><a href="#cb12-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-66"><a href="#cb12-66" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-67"><a href="#cb12-67" aria-hidden="true" tabindex="-1"></a>\nabla Z_{22} = Z_{1,2} - Z_{3,2}</span>
<span id="cb12-68"><a href="#cb12-68" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-69"><a href="#cb12-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-70"><a href="#cb12-70" aria-hidden="true" tabindex="-1"></a>And, when applied to the entire image, the result is</span>
<span id="cb12-71"><a href="#cb12-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-72"><a href="#cb12-72" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-73"><a href="#cb12-73" aria-hidden="true" tabindex="-1"></a>\begin{bmatrix}</span>
<span id="cb12-74"><a href="#cb12-74" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>&amp; - &amp; - &amp; - &amp; -  &amp; - <span class="sc">\\</span></span>
<span id="cb12-75"><a href="#cb12-75" aria-hidden="true" tabindex="-1"></a>0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 <span class="sc">\\</span></span>
<span id="cb12-76"><a href="#cb12-76" aria-hidden="true" tabindex="-1"></a>0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 <span class="sc">\\</span></span>
<span id="cb12-77"><a href="#cb12-77" aria-hidden="true" tabindex="-1"></a>0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 <span class="sc">\\</span></span>
<span id="cb12-78"><a href="#cb12-78" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>&amp; - &amp; - &amp; - &amp; - &amp; -</span>
<span id="cb12-79"><a href="#cb12-79" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}</span>
<span id="cb12-80"><a href="#cb12-80" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-81"><a href="#cb12-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-82"><a href="#cb12-82" aria-hidden="true" tabindex="-1"></a>The all entries are zero, meaning that there is no edge in the vertical direction.</span>
<span id="cb12-83"><a href="#cb12-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-84"><a href="#cb12-84" aria-hidden="true" tabindex="-1"></a>We can combine the horizontal and vertical derivatives to get the gradient of the image. For example,</span>
<span id="cb12-85"><a href="#cb12-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-86"><a href="#cb12-86" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-87"><a href="#cb12-87" aria-hidden="true" tabindex="-1"></a>\nabla Z_{22} = Z_{12} - Z_{32} + Z_{21} - Z_{23}</span>
<span id="cb12-88"><a href="#cb12-88" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-89"><a href="#cb12-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-90"><a href="#cb12-90" aria-hidden="true" tabindex="-1"></a>When applied to the entire image, the result is the same as the horizontal derivative.</span>
<span id="cb12-91"><a href="#cb12-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-92"><a href="#cb12-92" aria-hidden="true" tabindex="-1"></a><span class="fu">### Convolution</span></span>
<span id="cb12-93"><a href="#cb12-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-94"><a href="#cb12-94" aria-hidden="true" tabindex="-1"></a>We observe that there is a repeated pattern in the derivative computation: we are taking addition and subtraction of neighbiring pixels. This motivates us to generalize the operation to a more general form.</span>
<span id="cb12-95"><a href="#cb12-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-96"><a href="#cb12-96" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-97"><a href="#cb12-97" aria-hidden="true" tabindex="-1"></a>\nabla Z_{22} = \sum_{i=-1}^1 \sum_{j=-1}^1 K_{h-(i+1),w-(j+1)} Z_{2+i, 2+j}</span>
<span id="cb12-98"><a href="#cb12-98" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-99"><a href="#cb12-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-100"><a href="#cb12-100" aria-hidden="true" tabindex="-1"></a>where $K$ is a $3 \times 3$ matrix, and $w=h=3$ represent the width and height of the kernel.</span>
<span id="cb12-101"><a href="#cb12-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-102"><a href="#cb12-102" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-103"><a href="#cb12-103" aria-hidden="true" tabindex="-1"></a>K_{\text{horizontal}} = \begin{bmatrix}</span>
<span id="cb12-104"><a href="#cb12-104" aria-hidden="true" tabindex="-1"></a>0 &amp; 0 &amp; 0 <span class="sc">\\</span></span>
<span id="cb12-105"><a href="#cb12-105" aria-hidden="true" tabindex="-1"></a>-1 &amp; 0 &amp; 1 <span class="sc">\\</span></span>
<span id="cb12-106"><a href="#cb12-106" aria-hidden="true" tabindex="-1"></a>0 &amp; 0 &amp; 0</span>
<span id="cb12-107"><a href="#cb12-107" aria-hidden="true" tabindex="-1"></a>\end{bmatrix},\quad</span>
<span id="cb12-108"><a href="#cb12-108" aria-hidden="true" tabindex="-1"></a>K_{\text{vertical}} = \begin{bmatrix}</span>
<span id="cb12-109"><a href="#cb12-109" aria-hidden="true" tabindex="-1"></a>0 &amp; -1 &amp; 0 <span class="sc">\\</span></span>
<span id="cb12-110"><a href="#cb12-110" aria-hidden="true" tabindex="-1"></a>0 &amp; 0 &amp; 0 <span class="sc">\\</span></span>
<span id="cb12-111"><a href="#cb12-111" aria-hidden="true" tabindex="-1"></a>0 &amp; 1 &amp; 0</span>
<span id="cb12-112"><a href="#cb12-112" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}</span>
<span id="cb12-113"><a href="#cb12-113" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-114"><a href="#cb12-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-115"><a href="#cb12-115" aria-hidden="true" tabindex="-1"></a>The operation of $K$ on the image is called *convolution*, and $K$ is called the *kernel* or *filter*. More generally, the convolution of a kernel $K$ and an image $X$ is defined as</span>
<span id="cb12-116"><a href="#cb12-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-117"><a href="#cb12-117" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-118"><a href="#cb12-118" aria-hidden="true" tabindex="-1"></a>Y_{ij} = \sum_{p}\sum_{q} K_{pq} X_{i+p-\frac{h+1}{2}, j+q-\frac{w+1}{2}}</span>
<span id="cb12-119"><a href="#cb12-119" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-120"><a href="#cb12-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-121"><a href="#cb12-121" aria-hidden="true" tabindex="-1"></a>where $h$ and $w$ are the height and width of the kernel, respectively.</span>
<span id="cb12-122"><a href="#cb12-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-123"><a href="#cb12-123" aria-hidden="true" tabindex="-1"></a><span class="fu">## From Image to Graph</span></span>
<span id="cb12-124"><a href="#cb12-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-125"><a href="#cb12-125" aria-hidden="true" tabindex="-1"></a><span class="fu">### Analogy between image and graph data</span></span>
<span id="cb12-126"><a href="#cb12-126" aria-hidden="true" tabindex="-1"></a>We can think of a convolution of an image from the perspective of networks.</span>
<span id="cb12-127"><a href="#cb12-127" aria-hidden="true" tabindex="-1"></a>In the convolution of an image, a pixel is convolved with its *neighbors*. We can regard each pixel as a node, and each node is connected to its neighboring nodes (pixels) that are involved in the convolution.</span>
<span id="cb12-128"><a href="#cb12-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-129"><a href="#cb12-129" aria-hidden="true" tabindex="-1"></a><span class="al">![](https://av-eks-lekhak.s3.amazonaws.com/media/__sized__/article_images/conv_graph-thumbnail_webp-600x300.webp)</span></span>
<span id="cb12-130"><a href="#cb12-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-131"><a href="#cb12-131" aria-hidden="true" tabindex="-1"></a>Building on this analogy, we can extend the idea of convolution to general graph data.</span>
<span id="cb12-132"><a href="#cb12-132" aria-hidden="true" tabindex="-1"></a>Each node has a pixel value(s) (e.g., feature vector), which is convolved with the values of its neighbors in the graph.</span>
<span id="cb12-133"><a href="#cb12-133" aria-hidden="true" tabindex="-1"></a>This is the key idea of graph convolutional networks.</span>
<span id="cb12-134"><a href="#cb12-134" aria-hidden="true" tabindex="-1"></a>But, there is a key difference: while the number of neighbors for an image is homogeneous, the number of neighbors for a node in a graph can be heterogeneous. Each pixel has the same number of neighbors (except for the boundary pixels), but nodes in a graph can have very different numbers of neighbors. This makes it non-trivial to define the "kernel" for graph convolution.</span>
<span id="cb12-135"><a href="#cb12-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-136"><a href="#cb12-136" aria-hidden="true" tabindex="-1"></a><span class="fu">### Spectral filter on graphs</span></span>
<span id="cb12-137"><a href="#cb12-137" aria-hidden="true" tabindex="-1"></a>Just like we can define a convolution on images in the frequency domain, we can also define a ''frequency domain'' for graphs.</span>
<span id="cb12-138"><a href="#cb12-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-139"><a href="#cb12-139" aria-hidden="true" tabindex="-1"></a>Consider a network of $N$ nodes, where each node has a feature variable ${\mathbf x}_i \in \mathbb{R}$. We are interested in:</span>
<span id="cb12-140"><a href="#cb12-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-141"><a href="#cb12-141" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-142"><a href="#cb12-142" aria-hidden="true" tabindex="-1"></a>J = \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N A_{ij}(x_i - x_j)^2,</span>
<span id="cb12-143"><a href="#cb12-143" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-144"><a href="#cb12-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-145"><a href="#cb12-145" aria-hidden="true" tabindex="-1"></a>where $A_{ij}$ is the adjacency matrix of the graph. The quantity $J$ represents *the total variation* of $x$ between connected nodes; a small $J$ means that connected nodes have similar $x$ (low variation; low frequency), while a large $J$ means that connected nodes have very different $x$ (high variation; high frequency).</span>
<span id="cb12-146"><a href="#cb12-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-147"><a href="#cb12-147" aria-hidden="true" tabindex="-1"></a>We can rewrite $J$ as</span>
<span id="cb12-148"><a href="#cb12-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-149"><a href="#cb12-149" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-150"><a href="#cb12-150" aria-hidden="true" tabindex="-1"></a>J = \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N A_{ij}(x_i - x_j)^2 = {\bf x}^\top {\bf L} {\bf x},</span>
<span id="cb12-151"><a href="#cb12-151" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-152"><a href="#cb12-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-153"><a href="#cb12-153" aria-hidden="true" tabindex="-1"></a>where ${\bf L}$ is the Laplacian matrix of the graph given by</span>
<span id="cb12-154"><a href="#cb12-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-155"><a href="#cb12-155" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-156"><a href="#cb12-156" aria-hidden="true" tabindex="-1"></a>L_{ij} = \begin{cases}</span>
<span id="cb12-157"><a href="#cb12-157" aria-hidden="true" tabindex="-1"></a>-1 &amp; \text{if } i \text{ and } j \text{ are connected} <span class="sc">\\</span></span>
<span id="cb12-158"><a href="#cb12-158" aria-hidden="true" tabindex="-1"></a>k_i &amp; \text{if } i = j <span class="sc">\\</span></span>
<span id="cb12-159"><a href="#cb12-159" aria-hidden="true" tabindex="-1"></a>0 &amp; \text{otherwise}</span>
<span id="cb12-160"><a href="#cb12-160" aria-hidden="true" tabindex="-1"></a>\end{cases}.</span>
<span id="cb12-161"><a href="#cb12-161" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-162"><a href="#cb12-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-163"><a href="#cb12-163" aria-hidden="true" tabindex="-1"></a>and ${\bf x} = <span class="co">[</span><span class="ot">x_1,x_2,\ldots, x_N</span><span class="co">]</span>^\top$ is a column vector of feature variables.</span>
<span id="cb12-164"><a href="#cb12-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-165"><a href="#cb12-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-166"><a href="#cb12-166" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="Detailed derivation"}</span>
<span id="cb12-167"><a href="#cb12-167" aria-hidden="true" tabindex="-1"></a><span class="sc">:tag:</span> note</span>
<span id="cb12-168"><a href="#cb12-168" aria-hidden="true" tabindex="-1"></a><span class="sc">:class:</span> dropdown</span>
<span id="cb12-169"><a href="#cb12-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-170"><a href="#cb12-170" aria-hidden="true" tabindex="-1"></a>The above derivation shows that the total variation of $x$ between connected nodes is proportional to ${\bf x}^\top {\bf L} {\bf x}$.</span>
<span id="cb12-171"><a href="#cb12-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-172"><a href="#cb12-172" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-173"><a href="#cb12-173" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb12-174"><a href="#cb12-174" aria-hidden="true" tabindex="-1"></a>J &amp;= \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N A_{ij}(x_i - x_j)^2 <span class="sc">\\</span></span>
<span id="cb12-175"><a href="#cb12-175" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N \underbrace{A_{ij}\left( x_i^2 +x_j^2\right)}_{\text{symmetric}} - \sum_{i=1}^N\sum_{j=1}^N A_{ij}x_ix_j <span class="sc">\\</span></span>
<span id="cb12-176"><a href="#cb12-176" aria-hidden="true" tabindex="-1"></a>&amp;= \sum_{i=1}^Nx_i^2\underbrace{\sum_{j=1}^N A_{ij}}_{\text{degree of node } i, k_i} - \sum_{i=1}^N\sum_{j=1}^N A_{ij}x_ix_j <span class="sc">\\</span></span>
<span id="cb12-177"><a href="#cb12-177" aria-hidden="true" tabindex="-1"></a>&amp;= \sum_{i=1}^Nx_i^2 k_i - \sum_{i=1}^N\sum_{j=1}^N A_{ij}x_ix_j <span class="sc">\\</span></span>
<span id="cb12-178"><a href="#cb12-178" aria-hidden="true" tabindex="-1"></a>&amp;= \underbrace{<span class="co">[</span><span class="ot">x_1,x_2,\ldots, x_N</span><span class="co">]</span>}_{{\bf x}} \underbrace{\begin{bmatrix} k_1 &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; k_2 &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \cdots &amp; k_N \end{bmatrix}}_{{\bf D}} \underbrace{\begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_N \end{bmatrix}}_{{\bf x}} - 2\underbrace{\sum_{i=1}^N\sum_{j=1}^N A_{ij}}_{{\bf x}^\top {\mathbf A} {\bf x}} {\bf x} <span class="sc">\\</span></span>
<span id="cb12-179"><a href="#cb12-179" aria-hidden="true" tabindex="-1"></a>&amp;= {\bf x}^\top {\bf D} {\bf x} - {\bf x}^\top {\mathbf A} {\bf x} <span class="sc">\\</span></span>
<span id="cb12-180"><a href="#cb12-180" aria-hidden="true" tabindex="-1"></a>&amp;= {\bf x}^\top {\bf L} {\bf x},</span>
<span id="cb12-181"><a href="#cb12-181" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb12-182"><a href="#cb12-182" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-183"><a href="#cb12-183" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-184"><a href="#cb12-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-185"><a href="#cb12-185" aria-hidden="true" tabindex="-1"></a>Let us showcase the analogy between the Fourier transform and the Laplacian matrix.</span>
<span id="cb12-186"><a href="#cb12-186" aria-hidden="true" tabindex="-1"></a>In the Fourier transform, a signal is decomposed into sinusoidal basis functions. Similarly, for a graph, we can decompose the variation $J$ into eigenvector bases.</span>
<span id="cb12-187"><a href="#cb12-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-188"><a href="#cb12-188" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-189"><a href="#cb12-189" aria-hidden="true" tabindex="-1"></a>J = \sum_{i=1}^N \lambda_i  {\bf x}^\top {\mathbf u}_i {\mathbf u}_i^\top {\bf x} = \sum_{i=1}^N \lambda_i  ||{\bf x}^\top {\mathbf u}_i||^2.</span>
<span id="cb12-190"><a href="#cb12-190" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-191"><a href="#cb12-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-192"><a href="#cb12-192" aria-hidden="true" tabindex="-1"></a>where ${\mathbf u}_i$ is the eigenvector corresponding to the eigenvalue $\lambda_i$.</span>
<span id="cb12-193"><a href="#cb12-193" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The term $({\bf x}^\top {\mathbf u}_i)$ is a dot-product between the feature vector ${\bf x}$ and the eigenvector ${\mathbf u}_i$, which measures how much ${\bf x}$ *coheres* with eigenvector ${\mathbf u}_i$, similar to how Fourier coefficients measure coherency with sinusoids.</span>
<span id="cb12-194"><a href="#cb12-194" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Each $||{\bf x}^\top {\mathbf u}_i||^2$ is the ''strength'' of ${\bf x}$ with respect to the eigenvector ${\mathbf u}_i$, and the total variation $J$ is a weighted sum of these strengths.</span>
<span id="cb12-195"><a href="#cb12-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-196"><a href="#cb12-196" aria-hidden="true" tabindex="-1"></a>Some eigenvectors correspond to low-frequency components, while others correspond to high-frequency components. For example, the total variation $J$ for an eigenvector ${\mathbf u}_i$ is given by</span>
<span id="cb12-197"><a href="#cb12-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-198"><a href="#cb12-198" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-199"><a href="#cb12-199" aria-hidden="true" tabindex="-1"></a>J = \frac{1}{2} \sum_{j}\sum_{\ell} A_{j\ell}(u_{ij} - u_{i\ell})^2 = {\mathbf u}_i^\top {\mathbf L} {\mathbf u}_i = \lambda_i.</span>
<span id="cb12-200"><a href="#cb12-200" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-201"><a href="#cb12-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-202"><a href="#cb12-202" aria-hidden="true" tabindex="-1"></a>This equation provides key insight into the meaning of eigenvalues:</span>
<span id="cb12-203"><a href="#cb12-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-204"><a href="#cb12-204" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>For an eigenvector ${\mathbf u}_i$, its eigenvalue $\lambda_i$ measures the total variation for ${\mathbf u}_i$.</span>
<span id="cb12-205"><a href="#cb12-205" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Large eigenvalues mean large differences between neighbors (high frequency), while small eigenvalues mean small differences (low frequency).</span>
<span id="cb12-206"><a href="#cb12-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-207"><a href="#cb12-207" aria-hidden="true" tabindex="-1"></a>Thus, if ${\bf x}$ aligns well with ${\mathbf u}_i$ with a large $\lambda_i$, then ${\bf x}$ has a strong high-frequency component; if ${\bf x}$ aligns well with ${\mathbf u}_i$ with a small $\lambda_i$, then ${\bf x}$ has strong low-frequency component.</span>
<span id="cb12-208"><a href="#cb12-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-209"><a href="#cb12-209" aria-hidden="true" tabindex="-1"></a><span class="fu">### Spectral Filtering</span></span>
<span id="cb12-210"><a href="#cb12-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-211"><a href="#cb12-211" aria-hidden="true" tabindex="-1"></a>Eigenvalues $\lambda_i$ can be thought of as a *filter* that controls which frequency components pass through. Instead of using the filter associated with the Laplacian matrix, we can design a filter $h(\lambda_i)$ to control which frequency components pass through. This leads to the idea of *spectral filtering*. Two common filters are:</span>
<span id="cb12-212"><a href="#cb12-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-213"><a href="#cb12-213" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Low-pass Filter**:</span>
<span id="cb12-214"><a href="#cb12-214" aria-hidden="true" tabindex="-1"></a>   $$h_{\text{low}}(\lambda) = \frac{1}{1 + \alpha\lambda}$$</span>
<span id="cb12-215"><a href="#cb12-215" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Preserves low frequencies (small λ)</span>
<span id="cb12-216"><a href="#cb12-216" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Suppresses high frequencies (large λ)</span>
<span id="cb12-217"><a href="#cb12-217" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Results in smoother signals</span>
<span id="cb12-218"><a href="#cb12-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-219"><a href="#cb12-219" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**High-pass Filter**:</span>
<span id="cb12-220"><a href="#cb12-220" aria-hidden="true" tabindex="-1"></a>   $$h_{\text{high}}(\lambda) = \frac{\alpha\lambda}{1 + \alpha\lambda}$$</span>
<span id="cb12-221"><a href="#cb12-221" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Preserves high frequencies</span>
<span id="cb12-222"><a href="#cb12-222" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Suppresses low frequencies</span>
<span id="cb12-223"><a href="#cb12-223" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Emphasizes differences between neighbors</span>
<span id="cb12-224"><a href="#cb12-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-227"><a href="#cb12-227" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-228"><a href="#cb12-228" aria-hidden="true" tabindex="-1"></a>:tags: [remove<span class="op">-</span><span class="bu">input</span>]</span>
<span id="cb12-229"><a href="#cb12-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-230"><a href="#cb12-230" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb12-231"><a href="#cb12-231" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb12-232"><a href="#cb12-232" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb12-233"><a href="#cb12-233" aria-hidden="true" tabindex="-1"></a>sns.set_context(<span class="st">"talk"</span>)</span>
<span id="cb12-234"><a href="#cb12-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-235"><a href="#cb12-235" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb12-236"><a href="#cb12-236" aria-hidden="true" tabindex="-1"></a>lambdas <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">100</span>)</span>
<span id="cb12-237"><a href="#cb12-237" aria-hidden="true" tabindex="-1"></a>h_low <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> alpha <span class="op">*</span> lambdas)</span>
<span id="cb12-238"><a href="#cb12-238" aria-hidden="true" tabindex="-1"></a>h_high <span class="op">=</span> (alpha <span class="op">*</span> lambdas) <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> alpha <span class="op">*</span> lambdas)</span>
<span id="cb12-239"><a href="#cb12-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-240"><a href="#cb12-240" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb12-241"><a href="#cb12-241" aria-hidden="true" tabindex="-1"></a>sns.lineplot(x<span class="op">=</span>lambdas, y<span class="op">=</span>h_low, label<span class="op">=</span><span class="st">"Low-pass filter"</span>, ax<span class="op">=</span>axes[<span class="dv">0</span>])</span>
<span id="cb12-242"><a href="#cb12-242" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].legend(frameon<span class="op">=</span><span class="va">False</span>).remove()</span>
<span id="cb12-243"><a href="#cb12-243" aria-hidden="true" tabindex="-1"></a>sns.lineplot(x<span class="op">=</span>lambdas, y<span class="op">=</span>h_high, label<span class="op">=</span><span class="st">"High-pass filter"</span>, ax<span class="op">=</span>axes[<span class="dv">1</span>])</span>
<span id="cb12-244"><a href="#cb12-244" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].legend(frameon<span class="op">=</span><span class="va">False</span>).remove()</span>
<span id="cb12-245"><a href="#cb12-245" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">"Low-pass filter"</span>)</span>
<span id="cb12-246"><a href="#cb12-246" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">"High-pass filter"</span>)</span>
<span id="cb12-247"><a href="#cb12-247" aria-hidden="true" tabindex="-1"></a>fig.text(<span class="fl">0.5</span>, <span class="fl">0.01</span>, <span class="st">"Eigenvalue $</span><span class="er">\</span><span class="st">lambda$"</span>, ha<span class="op">=</span><span class="st">"center"</span>)</span>
<span id="cb12-248"><a href="#cb12-248" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_ylabel(<span class="st">"Filter response $h(</span><span class="er">\</span><span class="st">lambda)$"</span>)</span>
<span id="cb12-249"><a href="#cb12-249" aria-hidden="true" tabindex="-1"></a>sns.despine()</span>
<span id="cb12-250"><a href="#cb12-250" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb12-251"><a href="#cb12-251" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-252"><a href="#cb12-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-253"><a href="#cb12-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-254"><a href="#cb12-254" aria-hidden="true" tabindex="-1"></a><span class="fu">### Example</span></span>
<span id="cb12-255"><a href="#cb12-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-256"><a href="#cb12-256" aria-hidden="true" tabindex="-1"></a>Let us showcase the idea of spectral filtering with a simple example with the karate club network.</span>
<span id="cb12-257"><a href="#cb12-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-260"><a href="#cb12-260" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-261"><a href="#cb12-261" aria-hidden="true" tabindex="-1"></a>:tags: [remove<span class="op">-</span><span class="bu">input</span>]</span>
<span id="cb12-262"><a href="#cb12-262" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> igraph <span class="im">as</span> ig</span>
<span id="cb12-263"><a href="#cb12-263" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb12-264"><a href="#cb12-264" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> sparse</span>
<span id="cb12-265"><a href="#cb12-265" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib <span class="im">as</span> mpl</span>
<span id="cb12-266"><a href="#cb12-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-267"><a href="#cb12-267" aria-hidden="true" tabindex="-1"></a>G <span class="op">=</span> ig.Graph.Famous(<span class="st">"Zachary"</span>)</span>
<span id="cb12-268"><a href="#cb12-268" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> G.get_adjacency_sparse()</span>
<span id="cb12-269"><a href="#cb12-269" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-270"><a href="#cb12-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-271"><a href="#cb12-271" aria-hidden="true" tabindex="-1"></a>We will first compute the laplacian matrix and its eigendecomposition.</span>
<span id="cb12-272"><a href="#cb12-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-275"><a href="#cb12-275" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-276"><a href="#cb12-276" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute Laplacian matrix</span></span>
<span id="cb12-277"><a href="#cb12-277" aria-hidden="true" tabindex="-1"></a>deg <span class="op">=</span> np.array(A.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>)).reshape(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb12-278"><a href="#cb12-278" aria-hidden="true" tabindex="-1"></a>D <span class="op">=</span> sparse.diags(deg)</span>
<span id="cb12-279"><a href="#cb12-279" aria-hidden="true" tabindex="-1"></a>L <span class="op">=</span> D <span class="op">-</span> A</span>
<span id="cb12-280"><a href="#cb12-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-281"><a href="#cb12-281" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute eigendecomposition</span></span>
<span id="cb12-282"><a href="#cb12-282" aria-hidden="true" tabindex="-1"></a>evals, evecs <span class="op">=</span> np.linalg.eigh(L.toarray())</span>
<span id="cb12-283"><a href="#cb12-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-284"><a href="#cb12-284" aria-hidden="true" tabindex="-1"></a><span class="co"># Sort eigenvalues and eigenvectors</span></span>
<span id="cb12-285"><a href="#cb12-285" aria-hidden="true" tabindex="-1"></a>order <span class="op">=</span> np.argsort(evals)</span>
<span id="cb12-286"><a href="#cb12-286" aria-hidden="true" tabindex="-1"></a>evals <span class="op">=</span> evals[order]</span>
<span id="cb12-287"><a href="#cb12-287" aria-hidden="true" tabindex="-1"></a>evecs <span class="op">=</span> evecs[:, order]</span>
<span id="cb12-288"><a href="#cb12-288" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-289"><a href="#cb12-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-290"><a href="#cb12-290" aria-hidden="true" tabindex="-1"></a>Now, let's create a low-pass and high-pass filter.</span>
<span id="cb12-291"><a href="#cb12-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-294"><a href="#cb12-294" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-295"><a href="#cb12-295" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb12-296"><a href="#cb12-296" aria-hidden="true" tabindex="-1"></a>L_low <span class="op">=</span> evecs <span class="op">@</span> np.diag(<span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> alpha <span class="op">*</span> evals)) <span class="op">@</span> evecs.T</span>
<span id="cb12-297"><a href="#cb12-297" aria-hidden="true" tabindex="-1"></a>L_high <span class="op">=</span> evecs <span class="op">@</span> np.diag(alpha <span class="op">*</span> evals <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> alpha <span class="op">*</span> evals)) <span class="op">@</span> evecs.T</span>
<span id="cb12-298"><a href="#cb12-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-299"><a href="#cb12-299" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Size of low-pass filter:"</span>, L_low.shape)</span>
<span id="cb12-300"><a href="#cb12-300" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Size of high-pass filter:"</span>, L_high.shape)</span>
<span id="cb12-301"><a href="#cb12-301" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-302"><a href="#cb12-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-303"><a href="#cb12-303" aria-hidden="true" tabindex="-1"></a>Notice that the high-pass filter and low-pass filter are matrices of the same size as the adjacency matrix $A$, which defines a 'convolution' on the graph as follows:</span>
<span id="cb12-304"><a href="#cb12-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-305"><a href="#cb12-305" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-306"><a href="#cb12-306" aria-hidden="true" tabindex="-1"></a>{\bf x}' = {\bf L}_{\text{low}} {\bf x} \quad \text{or} \quad {\bf x}' = {\bf L}_{\text{high}} {\bf x}.</span>
<span id="cb12-307"><a href="#cb12-307" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-308"><a href="#cb12-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-309"><a href="#cb12-309" aria-hidden="true" tabindex="-1"></a>where ${\bf L}_{\text{low}}$ and ${\bf L}_{\text{high}}$ are the low-pass and high-pass filters, respectively, and ${\bf x}'$ is the convolved feature vector.</span>
<span id="cb12-310"><a href="#cb12-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-311"><a href="#cb12-311" aria-hidden="true" tabindex="-1"></a>Now, let's see how these filters work. Our first example is a random feature vector.</span>
<span id="cb12-312"><a href="#cb12-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-315"><a href="#cb12-315" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-316"><a href="#cb12-316" aria-hidden="true" tabindex="-1"></a><span class="co"># Random feature vector</span></span>
<span id="cb12-317"><a href="#cb12-317" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.random.randn(A.shape[<span class="dv">0</span>], <span class="dv">1</span>)</span>
<span id="cb12-318"><a href="#cb12-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-319"><a href="#cb12-319" aria-hidden="true" tabindex="-1"></a><span class="co"># Convolve with low-pass filter</span></span>
<span id="cb12-320"><a href="#cb12-320" aria-hidden="true" tabindex="-1"></a>x_low <span class="op">=</span> L_low <span class="op">@</span> x</span>
<span id="cb12-321"><a href="#cb12-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-322"><a href="#cb12-322" aria-hidden="true" tabindex="-1"></a><span class="co"># Convolve with high-pass filter</span></span>
<span id="cb12-323"><a href="#cb12-323" aria-hidden="true" tabindex="-1"></a>x_high <span class="op">=</span> L_high <span class="op">@</span> x</span>
<span id="cb12-324"><a href="#cb12-324" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-325"><a href="#cb12-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-326"><a href="#cb12-326" aria-hidden="true" tabindex="-1"></a>Let us visualize the results.</span>
<span id="cb12-327"><a href="#cb12-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-330"><a href="#cb12-330" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-331"><a href="#cb12-331" aria-hidden="true" tabindex="-1"></a>:tags: [hide<span class="op">-</span><span class="bu">input</span>]</span>
<span id="cb12-332"><a href="#cb12-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-333"><a href="#cb12-333" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">5</span>))</span>
<span id="cb12-334"><a href="#cb12-334" aria-hidden="true" tabindex="-1"></a>palette <span class="op">=</span> sns.color_palette(<span class="st">"viridis"</span>, as_cmap<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-335"><a href="#cb12-335" aria-hidden="true" tabindex="-1"></a>norm <span class="op">=</span> mpl.colors.Normalize(vmin<span class="op">=-</span><span class="fl">0.3</span>, vmax<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb12-336"><a href="#cb12-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-337"><a href="#cb12-337" aria-hidden="true" tabindex="-1"></a><span class="co"># Original</span></span>
<span id="cb12-338"><a href="#cb12-338" aria-hidden="true" tabindex="-1"></a>values <span class="op">=</span> x.reshape(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb12-339"><a href="#cb12-339" aria-hidden="true" tabindex="-1"></a>values <span class="op">/=</span> np.linalg.norm(values)</span>
<span id="cb12-340"><a href="#cb12-340" aria-hidden="true" tabindex="-1"></a>ig.plot(G, vertex_color<span class="op">=</span>[palette(norm(x)) <span class="cf">for</span> x <span class="kw">in</span> values], bbox<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">500</span>, <span class="dv">500</span>), vertex_size<span class="op">=</span><span class="dv">20</span>, target<span class="op">=</span>axes[<span class="dv">0</span>])</span>
<span id="cb12-341"><a href="#cb12-341" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">"Original"</span>)</span>
<span id="cb12-342"><a href="#cb12-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-343"><a href="#cb12-343" aria-hidden="true" tabindex="-1"></a><span class="co"># Low-pass filter applied</span></span>
<span id="cb12-344"><a href="#cb12-344" aria-hidden="true" tabindex="-1"></a>values <span class="op">=</span> L_low <span class="op">@</span> x</span>
<span id="cb12-345"><a href="#cb12-345" aria-hidden="true" tabindex="-1"></a>values <span class="op">/=</span> np.linalg.norm(values)</span>
<span id="cb12-346"><a href="#cb12-346" aria-hidden="true" tabindex="-1"></a>values <span class="op">=</span> values.reshape(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb12-347"><a href="#cb12-347" aria-hidden="true" tabindex="-1"></a>ig.plot(G, vertex_color<span class="op">=</span>[palette(norm(x)) <span class="cf">for</span> x <span class="kw">in</span> values], bbox<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">500</span>, <span class="dv">500</span>), vertex_size<span class="op">=</span><span class="dv">20</span>, target<span class="op">=</span>axes[<span class="dv">1</span>])</span>
<span id="cb12-348"><a href="#cb12-348" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">"Low-pass filter"</span>)</span>
<span id="cb12-349"><a href="#cb12-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-350"><a href="#cb12-350" aria-hidden="true" tabindex="-1"></a><span class="co"># High-pass filter applied</span></span>
<span id="cb12-351"><a href="#cb12-351" aria-hidden="true" tabindex="-1"></a>values <span class="op">=</span> L_high <span class="op">@</span> x</span>
<span id="cb12-352"><a href="#cb12-352" aria-hidden="true" tabindex="-1"></a>values <span class="op">/=</span> np.linalg.norm(values)</span>
<span id="cb12-353"><a href="#cb12-353" aria-hidden="true" tabindex="-1"></a>values <span class="op">=</span> values.reshape(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb12-354"><a href="#cb12-354" aria-hidden="true" tabindex="-1"></a>ig.plot(G, vertex_color<span class="op">=</span>[palette(norm(x)) <span class="cf">for</span> x <span class="kw">in</span> values], bbox<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">500</span>, <span class="dv">500</span>), vertex_size<span class="op">=</span><span class="dv">20</span>, target<span class="op">=</span>axes[<span class="dv">2</span>])</span>
<span id="cb12-355"><a href="#cb12-355" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].set_title(<span class="st">"High-pass filter"</span>)</span>
<span id="cb12-356"><a href="#cb12-356" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span>
<span id="cb12-357"><a href="#cb12-357" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-358"><a href="#cb12-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-359"><a href="#cb12-359" aria-hidden="true" tabindex="-1"></a>We observe that the low-pass filter results in smoother ${\bf x}$ between connected nodes (i.e., neighboring nodes have similar ${\bf x}$).</span>
<span id="cb12-360"><a href="#cb12-360" aria-hidden="true" tabindex="-1"></a>The original ${\bf x}$ and ${\bf x}'_{\text{low}}$ are very similar because random variables are high-frequency components. In contrast, when we apply the high-pass filter, ${\bf x}'_{\text{high}}$ is similar to ${\bf x}$ because the high-frequency components are not filtered.</span>
<span id="cb12-361"><a href="#cb12-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-362"><a href="#cb12-362" aria-hidden="true" tabindex="-1"></a>Let's now use an eigenvector as our feature vector ${\bf x}$.</span>
<span id="cb12-363"><a href="#cb12-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-366"><a href="#cb12-366" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-367"><a href="#cb12-367" aria-hidden="true" tabindex="-1"></a>:tags: [hide<span class="op">-</span><span class="bu">input</span>]</span>
<span id="cb12-368"><a href="#cb12-368" aria-hidden="true" tabindex="-1"></a>eigen_centrality <span class="op">=</span> np.array(G.eigenvector_centrality()).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb12-369"><a href="#cb12-369" aria-hidden="true" tabindex="-1"></a>low_pass_eigen <span class="op">=</span> L_low <span class="op">@</span> eigen_centrality</span>
<span id="cb12-370"><a href="#cb12-370" aria-hidden="true" tabindex="-1"></a>high_pass_eigen <span class="op">=</span> L_high <span class="op">@</span> eigen_centrality</span>
<span id="cb12-371"><a href="#cb12-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-372"><a href="#cb12-372" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">5</span>))</span>
<span id="cb12-373"><a href="#cb12-373" aria-hidden="true" tabindex="-1"></a>palette <span class="op">=</span> sns.color_palette(<span class="st">"viridis"</span>, as_cmap<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-374"><a href="#cb12-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-375"><a href="#cb12-375" aria-hidden="true" tabindex="-1"></a>norm <span class="op">=</span> mpl.colors.Normalize(vmin<span class="op">=-</span><span class="dv">0</span>, vmax<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb12-376"><a href="#cb12-376" aria-hidden="true" tabindex="-1"></a>values <span class="op">=</span> eigen_centrality.reshape(<span class="op">-</span><span class="dv">1</span>)<span class="co"># high_pass_random.reshape(-1)</span></span>
<span id="cb12-377"><a href="#cb12-377" aria-hidden="true" tabindex="-1"></a>values <span class="op">/=</span> np.linalg.norm(values)</span>
<span id="cb12-378"><a href="#cb12-378" aria-hidden="true" tabindex="-1"></a>values <span class="op">=</span> values.reshape(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb12-379"><a href="#cb12-379" aria-hidden="true" tabindex="-1"></a>ig.plot(G, vertex_color<span class="op">=</span>[palette(norm(x)) <span class="cf">for</span> x <span class="kw">in</span> values], bbox<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">500</span>, <span class="dv">500</span>), vertex_size<span class="op">=</span><span class="dv">20</span>, target<span class="op">=</span>axes[<span class="dv">0</span>])</span>
<span id="cb12-380"><a href="#cb12-380" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">"Original"</span>)</span>
<span id="cb12-381"><a href="#cb12-381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-382"><a href="#cb12-382" aria-hidden="true" tabindex="-1"></a>values <span class="op">=</span> low_pass_eigen.reshape(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb12-383"><a href="#cb12-383" aria-hidden="true" tabindex="-1"></a>values <span class="op">/=</span> np.linalg.norm(values)</span>
<span id="cb12-384"><a href="#cb12-384" aria-hidden="true" tabindex="-1"></a>values <span class="op">=</span> values.reshape(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb12-385"><a href="#cb12-385" aria-hidden="true" tabindex="-1"></a>ig.plot(G, vertex_color<span class="op">=</span>[palette(norm(x)) <span class="cf">for</span> x <span class="kw">in</span> values], bbox<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">500</span>, <span class="dv">500</span>), vertex_size<span class="op">=</span><span class="dv">20</span>, target<span class="op">=</span>axes[<span class="dv">1</span>])</span>
<span id="cb12-386"><a href="#cb12-386" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">"Low-pass filter"</span>)</span>
<span id="cb12-387"><a href="#cb12-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-388"><a href="#cb12-388" aria-hidden="true" tabindex="-1"></a>values <span class="op">=</span> high_pass_eigen.reshape(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb12-389"><a href="#cb12-389" aria-hidden="true" tabindex="-1"></a>values <span class="op">/=</span> np.linalg.norm(values)</span>
<span id="cb12-390"><a href="#cb12-390" aria-hidden="true" tabindex="-1"></a>ig.plot(G, vertex_color<span class="op">=</span>[palette(norm(x)) <span class="cf">for</span> x <span class="kw">in</span> values], bbox<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">500</span>, <span class="dv">500</span>), vertex_size<span class="op">=</span><span class="dv">20</span>, target<span class="op">=</span>axes[<span class="dv">2</span>])</span>
<span id="cb12-391"><a href="#cb12-391" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].set_title(<span class="st">"High-pass filter"</span>)</span>
<span id="cb12-392"><a href="#cb12-392" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span>
<span id="cb12-393"><a href="#cb12-393" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-394"><a href="#cb12-394" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-395"><a href="#cb12-395" aria-hidden="true" tabindex="-1"></a>The high-pass filter increases the contrast of the eigenvector centrality, emphasizing the differences between nodes. On the other hand, the low-pass filter smooths out the eigenvector centrality.</span>
<span id="cb12-396"><a href="#cb12-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-397"><a href="#cb12-397" aria-hidden="true" tabindex="-1"></a><span class="fu">## Graph Convolutional Networks</span></span>
<span id="cb12-398"><a href="#cb12-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-399"><a href="#cb12-399" aria-hidden="true" tabindex="-1"></a>We have seen that spectral filters give us a principled way to think about "convolution" on irregular graph structures, and controlling the frequency components brings out different aspects of the data. We now go one step further: instead of designing filters by hand, we can learn them from data for specific tasks.</span>
<span id="cb12-400"><a href="#cb12-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-401"><a href="#cb12-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-402"><a href="#cb12-402" aria-hidden="true" tabindex="-1"></a><span class="fu">### Spectral Graph Convolutional Networks</span></span>
<span id="cb12-403"><a href="#cb12-403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-404"><a href="#cb12-404" aria-hidden="true" tabindex="-1"></a>A simplest form of learnable spectral filter is given by</span>
<span id="cb12-405"><a href="#cb12-405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-406"><a href="#cb12-406" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-407"><a href="#cb12-407" aria-hidden="true" tabindex="-1"></a>{\bf L}_{\text{learn}} = \sum_{k=1}^K \theta_k {\mathbf u}_k {\mathbf u}_k^\top,</span>
<span id="cb12-408"><a href="#cb12-408" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-409"><a href="#cb12-409" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-410"><a href="#cb12-410" aria-hidden="true" tabindex="-1"></a>where ${\mathbf u}_k$ are the eigenvectors and $\theta_k$ are the learnable parameters. The variable $K$ is the number of eigenvectors used (i.e., the rank of the filter). The weight $\theta_k$ is learned to maximize the performance of the task at hand.</span>
<span id="cb12-411"><a href="#cb12-411" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-412"><a href="#cb12-412" aria-hidden="true" tabindex="-1"></a>Building on this idea, {footcite}<span class="in">`bruna2014spectral`</span> added a nonlinearity to the filter and proposed a spectral convolutional neural network (GCN) by</span>
<span id="cb12-413"><a href="#cb12-413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-414"><a href="#cb12-414" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-415"><a href="#cb12-415" aria-hidden="true" tabindex="-1"></a>{\bf x}^{(\ell+1)} = h\left( L_{\text{learn}} {\bf x}^{(\ell)}\right),</span>
<span id="cb12-416"><a href="#cb12-416" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-417"><a href="#cb12-417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-418"><a href="#cb12-418" aria-hidden="true" tabindex="-1"></a>where $h$ is an activation function, and ${\bf x}^{(\ell)}$ is the feature vector of the $\ell$-th convolution. They further extend this idea to convolve on multidimensional feature vectors, ${\bf X} \in \mathbb{R}^{N \times f_{\text{in}}}$ to produce new feature vectors of different dimensionality, ${\bf X}' \in \mathbb{R}^{N \times f_{\text{out}}}$.</span>
<span id="cb12-419"><a href="#cb12-419" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-420"><a href="#cb12-420" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-421"><a href="#cb12-421" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb12-422"><a href="#cb12-422" aria-hidden="true" tabindex="-1"></a>{\bf X}^{(\ell+1)}_i &amp;= h\left( \sum_j L_{\text{learn}}^{(i,j)} {\bf X}^{(\ell)}_j\right),\quad \text{where} \quad L^{(i,j)}_{\text{learn}} = \sum_{k=1}^K \theta_{k, (i,j)} {\mathbf u}_k {\mathbf u}_k^\top,</span>
<span id="cb12-423"><a href="#cb12-423" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb12-424"><a href="#cb12-424" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-425"><a href="#cb12-425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-426"><a href="#cb12-426" aria-hidden="true" tabindex="-1"></a>Notice that the learnable filter $L_{\text{learn}}^{(i,j)}$ is defined for each pair of input $i$ and output $j$ dimensions.</span>
<span id="cb12-427"><a href="#cb12-427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-428"><a href="#cb12-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-431"><a href="#cb12-431" aria-hidden="true" tabindex="-1"></a><span class="in">```{note}</span></span>
<span id="cb12-432"><a href="#cb12-432" aria-hidden="true" tabindex="-1"></a><span class="in">Many GCNs simple when it comes to implementation despite the complicated formula. And this is one of my ways to learn GNNs. Check out the [Appendix for the Python implementation](appendix.md).</span></span>
<span id="cb12-433"><a href="#cb12-433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-434"><a href="#cb12-434" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-435"><a href="#cb12-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-436"><a href="#cb12-436" aria-hidden="true" tabindex="-1"></a><span class="fu">### From Spectral to Spatial</span></span>
<span id="cb12-437"><a href="#cb12-437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-438"><a href="#cb12-438" aria-hidden="true" tabindex="-1"></a>Spectral GCNs are mathematically elegant but have two main limitations:</span>
<span id="cb12-439"><a href="#cb12-439" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Computational Limitation**: Computing the spectra of the Laplacian is expensive ${\cal O}(N^3)$ and prohibitive for large graphs</span>
<span id="cb12-440"><a href="#cb12-440" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Spatial Locality**: The learned filters are not spatially localized. A node can be influenced by all other nodes in the graph.</span>
<span id="cb12-441"><a href="#cb12-441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-442"><a href="#cb12-442" aria-hidden="true" tabindex="-1"></a>These two limitations motivate the development of spatial GCNs.</span>
<span id="cb12-443"><a href="#cb12-443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-444"><a href="#cb12-444" aria-hidden="true" tabindex="-1"></a><span class="fu">### ChebNet</span></span>
<span id="cb12-445"><a href="#cb12-445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-446"><a href="#cb12-446" aria-hidden="true" tabindex="-1"></a>ChebNet {footcite}<span class="in">`defferrard2016convolutional`</span> is one of the earliest spatial GCNs that bridges the gap between spectral and spatial domains.</span>
<span id="cb12-447"><a href="#cb12-447" aria-hidden="true" tabindex="-1"></a>The key idea is to leverage Chebyshev polynomials to approximate ${\bf L}_{\text{learn}}$ by</span>
<span id="cb12-448"><a href="#cb12-448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-449"><a href="#cb12-449" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-450"><a href="#cb12-450" aria-hidden="true" tabindex="-1"></a>{\bf L}_{\text{learn}} \approx \sum_{k=0}^{K-1} \theta_k T_k(\tilde{{\bf L}}), \quad \text{where} \quad \tilde{{\bf L}} = \frac{2}{\lambda_{\text{max}}}{\bf L} - {\bf I},</span>
<span id="cb12-451"><a href="#cb12-451" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-452"><a href="#cb12-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-453"><a href="#cb12-453" aria-hidden="true" tabindex="-1"></a>where $\tilde{{\bf L}}$ is the scaled and normalized Laplacian matrix in order to have eigenvalues in the range of $<span class="co">[</span><span class="ot">-1,1</span><span class="co">]</span>$. The Chebyshev polynomials $T_k(\tilde{{\bf L}})$ transforms the eigenvalues $\tilde{{\bf L}}$ to the following recursively:</span>
<span id="cb12-454"><a href="#cb12-454" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-455"><a href="#cb12-455" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-456"><a href="#cb12-456" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb12-457"><a href="#cb12-457" aria-hidden="true" tabindex="-1"></a>T_0(\tilde{{\bf L}}) &amp;= {\bf I} <span class="sc">\\</span></span>
<span id="cb12-458"><a href="#cb12-458" aria-hidden="true" tabindex="-1"></a>T_1(\tilde{{\bf L}}) &amp;= \tilde{{\bf L}} <span class="sc">\\</span></span>
<span id="cb12-459"><a href="#cb12-459" aria-hidden="true" tabindex="-1"></a>T_k(\tilde{{\bf L}}) &amp;= 2\tilde{{\bf L}} T_{k-1}(\tilde{{\bf L}}) - T_{k-2}(\tilde{{\bf L}})</span>
<span id="cb12-460"><a href="#cb12-460" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb12-461"><a href="#cb12-461" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-462"><a href="#cb12-462" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-463"><a href="#cb12-463" aria-hidden="true" tabindex="-1"></a>We then replace ${\bf L}_{\text{learn}}$ in the original spectral GCN with the Chebyshev polynomial approximation:</span>
<span id="cb12-464"><a href="#cb12-464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-465"><a href="#cb12-465" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-466"><a href="#cb12-466" aria-hidden="true" tabindex="-1"></a>{\bf x}^{(\ell+1)} = h\left( \sum_{k=0}^{K-1} \theta_k T_k(\tilde{{\bf L}}){\bf x}^{(\ell)}\right),</span>
<span id="cb12-467"><a href="#cb12-467" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-468"><a href="#cb12-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-469"><a href="#cb12-469" aria-hidden="true" tabindex="-1"></a>where:</span>
<span id="cb12-470"><a href="#cb12-470" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$T_k(\tilde{{\bf L}})$ applies the k-th Chebyshev polynomial to the scaled Laplacian matrix</span>
<span id="cb12-471"><a href="#cb12-471" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\theta_k$ are the learnable parameters</span>
<span id="cb12-472"><a href="#cb12-472" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>K is the order of the polynomial (typically small, e.g., K=3)</span>
<span id="cb12-473"><a href="#cb12-473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-474"><a href="#cb12-474" aria-hidden="true" tabindex="-1"></a><span class="fu">### Graph Convolutional Networks by Kipf and Welling</span></span>
<span id="cb12-475"><a href="#cb12-475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-476"><a href="#cb12-476" aria-hidden="true" tabindex="-1"></a>While ChebNet offers a principled way to approximate spectral convolutions, Kipf and Welling (2017) {footcite}<span class="in">`kipf2017semi`</span> proposed an even simpler and highly effective variant called **Graph Convolutional Networks (GCN)**.</span>
<span id="cb12-477"><a href="#cb12-477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-478"><a href="#cb12-478" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-479"><a href="#cb12-479" aria-hidden="true" tabindex="-1"></a><span class="fu">##### First-order Approximation</span></span>
<span id="cb12-480"><a href="#cb12-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-481"><a href="#cb12-481" aria-hidden="true" tabindex="-1"></a>The key departure is to use the first-order approximation of the Chebyshev polynomials.</span>
<span id="cb12-482"><a href="#cb12-482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-483"><a href="#cb12-483" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-484"><a href="#cb12-484" aria-hidden="true" tabindex="-1"></a>g_{\theta'} * x \approx \theta'_0x + \theta'_1(L - I_N)x = \theta'_0x - \theta'_1D^{-\frac{1}{2}}AD^{-\frac{1}{2}}x</span>
<span id="cb12-485"><a href="#cb12-485" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-486"><a href="#cb12-486" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-487"><a href="#cb12-487" aria-hidden="true" tabindex="-1"></a>This is crude approximation but it leads to a much simpler form, leaving only two learnable parameters, instead of $K$ parameters in the original ChebNet.</span>
<span id="cb12-488"><a href="#cb12-488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-489"><a href="#cb12-489" aria-hidden="true" tabindex="-1"></a>Additionally, they further simplify the formula by using the same $\theta$ for both remaining parameters (i.e., $\theta_0 = \theta$ and $\theta_1 = -\theta$). The result is the following convolutional filter:</span>
<span id="cb12-490"><a href="#cb12-490" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-491"><a href="#cb12-491" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-492"><a href="#cb12-492" aria-hidden="true" tabindex="-1"></a>g_{\theta} * x \approx \theta(I_N + D^{-\frac{1}{2}}AD^{-\frac{1}{2}})x</span>
<span id="cb12-493"><a href="#cb12-493" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-494"><a href="#cb12-494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-495"><a href="#cb12-495" aria-hidden="true" tabindex="-1"></a>While this is a very simple filter, one can stack multiple layers of convolutions to perform high-order graph convolutions.</span>
<span id="cb12-496"><a href="#cb12-496" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-497"><a href="#cb12-497" aria-hidden="true" tabindex="-1"></a><span class="fu">##### Deep GCNs can suffer from over-smoothing</span></span>
<span id="cb12-498"><a href="#cb12-498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-499"><a href="#cb12-499" aria-hidden="true" tabindex="-1"></a>GCN models can be deep, and when they are too deep, they start suffering from an ill-posed problem called *gradient vanishing/exploding*, where the gradients of the loss function becomes too small or too large to update the model parameters. It is a common problem in deep learning.</span>
<span id="cb12-500"><a href="#cb12-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-501"><a href="#cb12-501" aria-hidden="true" tabindex="-1"></a>To facilitate the training of deep GCNs, the authors introduce a very simple trick called *renormalization*. The idea is to add self-connections to the graph:</span>
<span id="cb12-502"><a href="#cb12-502" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-503"><a href="#cb12-503" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-504"><a href="#cb12-504" aria-hidden="true" tabindex="-1"></a>\tilde{A} = A + I_N, \quad \text{and} \quad \tilde{D}_{ii} = \sum_j \tilde{A}_{ij}</span>
<span id="cb12-505"><a href="#cb12-505" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-506"><a href="#cb12-506" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-507"><a href="#cb12-507" aria-hidden="true" tabindex="-1"></a>And use $\tilde{A}$ and $\tilde{D}$ to form the convolutional filter.</span>
<span id="cb12-508"><a href="#cb12-508" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-509"><a href="#cb12-509" aria-hidden="true" tabindex="-1"></a>Altogether, this leads to the following layer-wise propagation rule:</span>
<span id="cb12-510"><a href="#cb12-510" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-511"><a href="#cb12-511" aria-hidden="true" tabindex="-1"></a>$$X^{(\ell+1)} = \sigma(\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}X^{(\ell)}W^{(\ell)})$$</span>
<span id="cb12-512"><a href="#cb12-512" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-513"><a href="#cb12-513" aria-hidden="true" tabindex="-1"></a>where:</span>
<span id="cb12-514"><a href="#cb12-514" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$X^{(\ell)}$ is the matrix of node features at layer $\ell$</span>
<span id="cb12-515"><a href="#cb12-515" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$W^{(\ell)}$ is the layer's trainable weight matrix</span>
<span id="cb12-516"><a href="#cb12-516" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\sigma$ is a nonlinear activation function (e.g., ReLU)</span>
<span id="cb12-517"><a href="#cb12-517" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-518"><a href="#cb12-518" aria-hidden="true" tabindex="-1"></a>These simplifications offer several advantages:</span>
<span id="cb12-519"><a href="#cb12-519" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Efficiency**: Linear complexity in number of edges</span>
<span id="cb12-520"><a href="#cb12-520" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Localization**: Each layer only aggregates information from immediate neighbors</span>
<span id="cb12-521"><a href="#cb12-521" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Depth**: Fewer parameters allow building deeper models</span>
<span id="cb12-522"><a href="#cb12-522" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Performance**: Despite (or perhaps due to) its simplicity, it often outperforms more complex models</span>
<span id="cb12-523"><a href="#cb12-523" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-524"><a href="#cb12-524" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="Exercise"}</span>
<span id="cb12-525"><a href="#cb12-525" aria-hidden="true" tabindex="-1"></a><span class="sc">:class:</span> note</span>
<span id="cb12-526"><a href="#cb12-526" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-527"><a href="#cb12-527" aria-hidden="true" tabindex="-1"></a>Let's implement a simple GCN model for node classification.</span>
<span id="cb12-528"><a href="#cb12-528" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">Coding Exercise</span><span class="co">](../../../notebooks/exercise-m09-graph-neural-net.ipynb)</span></span>
<span id="cb12-529"><a href="#cb12-529" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-530"><a href="#cb12-530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-531"><a href="#cb12-531" aria-hidden="true" tabindex="-1"></a><span class="fu">## Popular Graph Neural Networks</span></span>
<span id="cb12-532"><a href="#cb12-532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-533"><a href="#cb12-533" aria-hidden="true" tabindex="-1"></a>In this section, we will introduce three popular GNNs: GraphSAGE, Graph Attention Networks (GAT), and Graph Isomorphism Network (GIN).</span>
<span id="cb12-534"><a href="#cb12-534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-535"><a href="#cb12-535" aria-hidden="true" tabindex="-1"></a><span class="fu">### GraphSAGE: Sample and Aggregate</span></span>
<span id="cb12-536"><a href="#cb12-536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-537"><a href="#cb12-537" aria-hidden="true" tabindex="-1"></a>GraphSAGE {footcite}<span class="in">`hamilton2017graphsage`</span> introduced a different GCN that can be ***generalized to unseen nodes*** (they called it "inductive"). While previous approaches like ChebNet and GCN operate on the entire graph, GraphSAGE proposes an inductive framework that generates embeddings by sampling and aggregating features from a node's neighborhood.</span>
<span id="cb12-538"><a href="#cb12-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-539"><a href="#cb12-539" aria-hidden="true" tabindex="-1"></a><span class="al">![](https://theaisummer.com/static/02e23adc75fe68e5dd249a94f3c1e8cc/c483d/graphsage.png)</span></span>
<span id="cb12-540"><a href="#cb12-540" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-541"><a href="#cb12-541" aria-hidden="true" tabindex="-1"></a><span class="fu">### Key Ideas</span></span>
<span id="cb12-542"><a href="#cb12-542" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-543"><a href="#cb12-543" aria-hidden="true" tabindex="-1"></a>GraphSAGE involves two key ideas: (1) sampling and (2) aggregation.</span>
<span id="cb12-544"><a href="#cb12-544" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-545"><a href="#cb12-545" aria-hidden="true" tabindex="-1"></a><span class="fu">##### Neighborhood Sampling</span></span>
<span id="cb12-546"><a href="#cb12-546" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-547"><a href="#cb12-547" aria-hidden="true" tabindex="-1"></a>The key idea is the *neighborhood sampling*. Instead of using all neighbors, GraphSAGE samples a fixed-size set of neighbors for each node. This controls memory complexity, a key limitation of the previous GNNs.</span>
<span id="cb12-548"><a href="#cb12-548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-549"><a href="#cb12-549" aria-hidden="true" tabindex="-1"></a>Another key advantage of neighborhood sampling is that it enables GraphSAGE to handle dynamic, growing networks. Consider a citation network where new papers (nodes) are continuously added. Traditional GCNs would need to recompute filters for the entire network with each new addition. In contrast, GraphSAGE can immediately generate embeddings for new nodes by simply sampling their neighbors, without any retraining or recomputation.</span>
<span id="cb12-550"><a href="#cb12-550" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-551"><a href="#cb12-551" aria-hidden="true" tabindex="-1"></a><span class="fu">##### Aggregation</span></span>
<span id="cb12-552"><a href="#cb12-552" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-553"><a href="#cb12-553" aria-hidden="true" tabindex="-1"></a>Another key idea is the *aggregation*. GraphSAGE makes a distinction between self-information and neighborhood information. While previous GNNs treat them equally and aggregate them, GraphSAGE treats them differently. Specifically, GraphSAGE introduces an additional step: it concatenates the self-information and the neighborhood information as the input of the convolution.</span>
<span id="cb12-554"><a href="#cb12-554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-555"><a href="#cb12-555" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-556"><a href="#cb12-556" aria-hidden="true" tabindex="-1"></a>Z_v = \text{CONCAT}(X_v, X_{\mathcal{N}(v)})</span>
<span id="cb12-557"><a href="#cb12-557" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-558"><a href="#cb12-558" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-559"><a href="#cb12-559" aria-hidden="true" tabindex="-1"></a>where $X_v$ is the feature of the node itself and $X_{\mathcal{N}(v)}$ is the aggregation of the features of its neighbors. GraphSAGE introduces different ways to aggregate information from neighbors:</span>
<span id="cb12-560"><a href="#cb12-560" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-561"><a href="#cb12-561" aria-hidden="true" tabindex="-1"></a>   $$X_{\mathcal{N}(v)} = \text{AGGREGATE}_k(<span class="sc">\{</span>X_u, \forall u \in \mathcal{N}(v)<span class="sc">\}</span>)$$</span>
<span id="cb12-562"><a href="#cb12-562" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-563"><a href="#cb12-563" aria-hidden="true" tabindex="-1"></a>   Common aggregation functions include:</span>
<span id="cb12-564"><a href="#cb12-564" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Mean aggregator: $\text{AGGREGATE} = \text{mean}(<span class="sc">\{</span>h_u, \forall u \in \mathcal{N}(v)<span class="sc">\}</span>)$</span>
<span id="cb12-565"><a href="#cb12-565" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Max-pooling: $\text{AGGREGATE} = \max(<span class="sc">\{</span>\sigma(W_{\text{pool}}h_u + b), \forall u \in \mathcal{N}(v)<span class="sc">\}</span>)$</span>
<span id="cb12-566"><a href="#cb12-566" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>LSTM aggregator: Apply LSTM to randomly permuted neighbors</span>
<span id="cb12-567"><a href="#cb12-567" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-568"><a href="#cb12-568" aria-hidden="true" tabindex="-1"></a>The concatenated feature $Z_v$ is normalized by the L2 norm.</span>
<span id="cb12-569"><a href="#cb12-569" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-570"><a href="#cb12-570" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-571"><a href="#cb12-571" aria-hidden="true" tabindex="-1"></a>\hat{Z}_v = \frac{Z_v}{<span class="sc">\|</span>Z_v<span class="sc">\|</span>_2}</span>
<span id="cb12-572"><a href="#cb12-572" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-573"><a href="#cb12-573" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-574"><a href="#cb12-574" aria-hidden="true" tabindex="-1"></a>and then fed into the convolution.</span>
<span id="cb12-575"><a href="#cb12-575" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-576"><a href="#cb12-576" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-577"><a href="#cb12-577" aria-hidden="true" tabindex="-1"></a>X_v^k = \sigma(W^k \hat{Z}_v + b^k)</span>
<span id="cb12-578"><a href="#cb12-578" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-579"><a href="#cb12-579" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-580"><a href="#cb12-580" aria-hidden="true" tabindex="-1"></a><span class="fu">### Graph Attention Networks (GAT): Differentiate Individual Neighbors</span></span>
<span id="cb12-581"><a href="#cb12-581" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-582"><a href="#cb12-582" aria-hidden="true" tabindex="-1"></a>A key innovation of GraphSAGE is to treat the self and neighborhood information differently. But should all neighbors be treated equally? Graph Attention Networks (GAT) address this by letting the model learn which neighbors to pay attention to.</span>
<span id="cb12-583"><a href="#cb12-583" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-584"><a href="#cb12-584" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-585"><a href="#cb12-585" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-586"><a href="#cb12-586" aria-hidden="true" tabindex="-1"></a><span class="fu">### Attention Mechanism</span></span>
<span id="cb12-587"><a href="#cb12-587" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-588"><a href="#cb12-588" aria-hidden="true" tabindex="-1"></a><span class="al">![](https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-08_at_7.55.32_PM_vkdDcDx.png)</span></span>
<span id="cb12-589"><a href="#cb12-589" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-590"><a href="#cb12-590" aria-hidden="true" tabindex="-1"></a>The core idea is beautifully simple: instead of using fixed weights like GCN, let's learn attention weights $\alpha_{ij}$ that determine how much node $i$ should attend to node $j$. These weights are computed dynamically based on node features:</span>
<span id="cb12-591"><a href="#cb12-591" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-592"><a href="#cb12-592" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-593"><a href="#cb12-593" aria-hidden="true" tabindex="-1"></a>\alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k \in \mathcal{N}(i)} \exp(e_{ik})}</span>
<span id="cb12-594"><a href="#cb12-594" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-595"><a href="#cb12-595" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-596"><a href="#cb12-596" aria-hidden="true" tabindex="-1"></a>where $e_{ij}$ represents the importance of the edge between node $i$ and node $j$. Variable $e_{ij}$ is a *learnable* parameter and can be negative, and the exponential function is applied to transform it to a non-negative value, with the normalization term $\sum_{k \in \mathcal{N}(i)} \exp(e_{ik})$ to ensure the weights sum to 1.</span>
<span id="cb12-597"><a href="#cb12-597" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-598"><a href="#cb12-598" aria-hidden="true" tabindex="-1"></a>How to compute $e_{ij}$? One simple choice is to use a neural network with a shared weight matrix $W$ and a LeakyReLU activation function. Specifically:</span>
<span id="cb12-599"><a href="#cb12-599" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-600"><a href="#cb12-600" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Let's focus on computing $e_{ij}$ for node $i$ and its neighbor $j$.</span>
<span id="cb12-601"><a href="#cb12-601" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>We use a shared weight matrix $W$ to transform the features of node $i$ and $j$.</span>
<span id="cb12-602"><a href="#cb12-602" aria-hidden="true" tabindex="-1"></a>   $$</span>
<span id="cb12-603"><a href="#cb12-603" aria-hidden="true" tabindex="-1"></a>   \mathbf{\tilde h}_i  = \mathbf{h}_i, \quad \mathbf{\tilde h}_j  = W\mathbf{h}_j</span>
<span id="cb12-604"><a href="#cb12-604" aria-hidden="true" tabindex="-1"></a>   $$</span>
<span id="cb12-605"><a href="#cb12-605" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>We concatenate the transformed features and apply a LeakyReLU activation function.</span>
<span id="cb12-606"><a href="#cb12-606" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-607"><a href="#cb12-607" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-608"><a href="#cb12-608" aria-hidden="true" tabindex="-1"></a>e_{ij} = \text{LeakyReLU}(\mathbf{a}^T<span class="co">[</span><span class="ot">\mathbf{\tilde h}_i, \mathbf{\tilde h}_j</span><span class="co">]</span>)</span>
<span id="cb12-609"><a href="#cb12-609" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-610"><a href="#cb12-610" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-611"><a href="#cb12-611" aria-hidden="true" tabindex="-1"></a>where $\mathbf{a}$ is a trainable parameter vector that sums the two transformed features.</span>
<span id="cb12-612"><a href="#cb12-612" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-613"><a href="#cb12-613" aria-hidden="true" tabindex="-1"></a>Once we have these attention weights, the node update is straightforward - just a weighted sum of neighbor features:</span>
<span id="cb12-614"><a href="#cb12-614" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-615"><a href="#cb12-615" aria-hidden="true" tabindex="-1"></a>$$\mathbf{h}'_i = \sigma\left(\sum_{j \in \mathcal{N}(i) \cup \{i\}} \alpha_{ij}{\bf W}_{\text{feature}}\mathbf{h}_j\right)$$</span>
<span id="cb12-616"><a href="#cb12-616" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-617"><a href="#cb12-617" aria-hidden="true" tabindex="-1"></a>where ${\bf W}_{\text{feature}}$ is a trainable weight matrix. To stabilize training, GAT uses multiple attention heads and concatenates their outputs:</span>
<span id="cb12-618"><a href="#cb12-618" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-619"><a href="#cb12-619" aria-hidden="true" tabindex="-1"></a>$$\mathbf{h}'_i = \parallel_{k=1}^K \sigma\left(\sum_{j \in \mathcal{N}(i) \cup \{i\}} \alpha_{ij}^k{\bf W}^k_{\text{feature}}\mathbf{h}_j\right)$$</span>
<span id="cb12-620"><a href="#cb12-620" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-621"><a href="#cb12-621" aria-hidden="true" tabindex="-1"></a><span class="fu">### Graph Isomorphism Network (GIN): Differentiate the Aggregation</span></span>
<span id="cb12-622"><a href="#cb12-622" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-623"><a href="#cb12-623" aria-hidden="true" tabindex="-1"></a>Graph Isomorphism Networks (GIN) is another popular GNN that born out of a question: what is the maximum discriminative power achievable by Graph Neural Networks? The answer lies in its theoretical connection to **the Weisfeiler-Lehman (WL) test**, a powerful algorithm for graph isomorphism testing.</span>
<span id="cb12-624"><a href="#cb12-624" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-625"><a href="#cb12-625" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-626"><a href="#cb12-626" aria-hidden="true" tabindex="-1"></a><span class="fu">### Weisfeiler-Lehman Test</span></span>
<span id="cb12-627"><a href="#cb12-627" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-628"><a href="#cb12-628" aria-hidden="true" tabindex="-1"></a>Are two graphs structurally identical? Graph isomorphism testing determines if two graphs are structurally identical, with applications in graph classification, clustering, and other tasks.</span>
<span id="cb12-629"><a href="#cb12-629" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-630"><a href="#cb12-630" aria-hidden="true" tabindex="-1"></a><span class="al">![](https://i.sstatic.net/j5sGu.png)</span></span>
<span id="cb12-631"><a href="#cb12-631" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-632"><a href="#cb12-632" aria-hidden="true" tabindex="-1"></a>While the general problem has no known polynomial-time solution, the WL test is an efficient heuristic that works well in practice. The WL test iteratively refines node labels by hashing the multiset of neighboring labels</span>
<span id="cb12-633"><a href="#cb12-633" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-634"><a href="#cb12-634" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-635"><a href="#cb12-635" aria-hidden="true" tabindex="-1"></a><span class="al">![](../figs/weisfeiler-lehman-test.jpg)</span></span>
<span id="cb12-636"><a href="#cb12-636" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-637"><a href="#cb12-637" aria-hidden="true" tabindex="-1"></a>The WL test works as follows:</span>
<span id="cb12-638"><a href="#cb12-638" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-639"><a href="#cb12-639" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Assign all nodes the same initial label.</span>
<span id="cb12-640"><a href="#cb12-640" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>For each node, collect the labels of all its neighbors and *aggregate them* into a hash (e.g., new label). For example, the top node gets {0} from its neighbors, resulting in a collection {0,0}. A new label is created via a hash function $h$ that maps {0, {0, 0}} to a new label 1.</span>
<span id="cb12-641"><a href="#cb12-641" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Repeat the process for a fixed number of iterations or until convergence.</span>
<span id="cb12-642"><a href="#cb12-642" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-643"><a href="#cb12-643" aria-hidden="true" tabindex="-1"></a>Here is the implementation of the WL test in Python:</span>
<span id="cb12-644"><a href="#cb12-644" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-647"><a href="#cb12-647" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-648"><a href="#cb12-648" aria-hidden="true" tabindex="-1"></a>:tags: [hide<span class="op">-</span><span class="bu">input</span>]</span>
<span id="cb12-649"><a href="#cb12-649" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-650"><a href="#cb12-650" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb12-651"><a href="#cb12-651" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> sparse</span>
<span id="cb12-652"><a href="#cb12-652" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-653"><a href="#cb12-653" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> weisfeiler_lehman_test(A, num_iterations):</span>
<span id="cb12-654"><a href="#cb12-654" aria-hidden="true" tabindex="-1"></a>    n_nodes <span class="op">=</span> A.shape[<span class="dv">0</span>]</span>
<span id="cb12-655"><a href="#cb12-655" aria-hidden="true" tabindex="-1"></a>    labels <span class="op">=</span> np.zeros(n_nodes, dtype<span class="op">=</span><span class="bu">int</span>)</span>
<span id="cb12-656"><a href="#cb12-656" aria-hidden="true" tabindex="-1"></a>    color_map <span class="op">=</span> {}</span>
<span id="cb12-657"><a href="#cb12-657" aria-hidden="true" tabindex="-1"></a>    hash_fn <span class="op">=</span> <span class="kw">lambda</span> x: color_map.setdefault(x, <span class="bu">len</span>(color_map))</span>
<span id="cb12-658"><a href="#cb12-658" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_iterations):</span>
<span id="cb12-659"><a href="#cb12-659" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-660"><a href="#cb12-660" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Go through each node</span></span>
<span id="cb12-661"><a href="#cb12-661" aria-hidden="true" tabindex="-1"></a>        labels_old <span class="op">=</span> labels.copy()</span>
<span id="cb12-662"><a href="#cb12-662" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_nodes):</span>
<span id="cb12-663"><a href="#cb12-663" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-664"><a href="#cb12-664" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Collect the labels of all neighbors</span></span>
<span id="cb12-665"><a href="#cb12-665" aria-hidden="true" tabindex="-1"></a>            neighbors <span class="op">=</span> A[i].nonzero()[<span class="dv">1</span>]</span>
<span id="cb12-666"><a href="#cb12-666" aria-hidden="true" tabindex="-1"></a>            neighbor_labels <span class="op">=</span> labels_old[neighbors]</span>
<span id="cb12-667"><a href="#cb12-667" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-668"><a href="#cb12-668" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Count the frequency of each label</span></span>
<span id="cb12-669"><a href="#cb12-669" aria-hidden="true" tabindex="-1"></a>            unique, counts <span class="op">=</span> np.unique(neighbor_labels, return_counts<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-670"><a href="#cb12-670" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-671"><a href="#cb12-671" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Create a hash key by converting the frequency dictionary to a string</span></span>
<span id="cb12-672"><a href="#cb12-672" aria-hidden="true" tabindex="-1"></a>            hash_key <span class="op">=</span> <span class="bu">str</span>({unique[j]: counts[j] <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(unique))})</span>
<span id="cb12-673"><a href="#cb12-673" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-674"><a href="#cb12-674" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Create a new label by hashing the frequency dictionary</span></span>
<span id="cb12-675"><a href="#cb12-675" aria-hidden="true" tabindex="-1"></a>            label <span class="op">=</span> hash_fn(hash_key)</span>
<span id="cb12-676"><a href="#cb12-676" aria-hidden="true" tabindex="-1"></a>            labels[i] <span class="op">=</span> label</span>
<span id="cb12-677"><a href="#cb12-677" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-678"><a href="#cb12-678" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Check convergence</span></span>
<span id="cb12-679"><a href="#cb12-679" aria-hidden="true" tabindex="-1"></a>        unique, counts <span class="op">=</span> np.unique(labels, return_counts<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-680"><a href="#cb12-680" aria-hidden="true" tabindex="-1"></a>        unique_old, counts_old <span class="op">=</span> np.unique(labels_old, return_counts<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-681"><a href="#cb12-681" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> np.array_equal(np.sort(counts), np.sort(counts_old)):</span>
<span id="cb12-682"><a href="#cb12-682" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb12-683"><a href="#cb12-683" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> labels</span>
<span id="cb12-684"><a href="#cb12-684" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-685"><a href="#cb12-685" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-686"><a href="#cb12-686" aria-hidden="true" tabindex="-1"></a>edge_list <span class="op">=</span> [(<span class="dv">0</span>, <span class="dv">1</span>), (<span class="dv">1</span>, <span class="dv">2</span>), (<span class="dv">2</span>, <span class="dv">0</span>), (<span class="dv">3</span>, <span class="dv">4</span>), (<span class="dv">4</span>, <span class="dv">5</span>), (<span class="dv">5</span>, <span class="dv">3</span>)]</span>
<span id="cb12-687"><a href="#cb12-687" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-688"><a href="#cb12-688" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> sparse.csr_matrix(</span>
<span id="cb12-689"><a href="#cb12-689" aria-hidden="true" tabindex="-1"></a>    ([<span class="dv">1</span>] <span class="op">*</span> <span class="bu">len</span>(edge_list), ([e[<span class="dv">0</span>] <span class="cf">for</span> e <span class="kw">in</span> edge_list], [e[<span class="dv">1</span>] <span class="cf">for</span> e <span class="kw">in</span> edge_list])),</span>
<span id="cb12-690"><a href="#cb12-690" aria-hidden="true" tabindex="-1"></a>    shape<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">6</span>),</span>
<span id="cb12-691"><a href="#cb12-691" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-692"><a href="#cb12-692" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> A <span class="op">+</span> A.T</span>
<span id="cb12-693"><a href="#cb12-693" aria-hidden="true" tabindex="-1"></a>A.sort_indices()</span>
<span id="cb12-694"><a href="#cb12-694" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-695"><a href="#cb12-695" aria-hidden="true" tabindex="-1"></a>weisfeiler_lehman_test(A, A.shape[<span class="dv">0</span>])</span>
<span id="cb12-696"><a href="#cb12-696" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-697"><a href="#cb12-697" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-698"><a href="#cb12-698" aria-hidden="true" tabindex="-1"></a>After these iterations:</span>
<span id="cb12-699"><a href="#cb12-699" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Nodes with the same label are structurally identical, meaning that they are indistinguishable unless we label them differently.</span>
<span id="cb12-700"><a href="#cb12-700" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Two graphs are structurally identical if and only if they have the same node labels after the WL test.</span>
<span id="cb12-701"><a href="#cb12-701" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-702"><a href="#cb12-702" aria-hidden="true" tabindex="-1"></a>The WL test is a heuristic and can fail on some graphs. For example, it cannot distinguish regular graphs with the same number of nodes and edges.</span>
<span id="cb12-703"><a href="#cb12-703" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-706"><a href="#cb12-706" aria-hidden="true" tabindex="-1"></a><span class="in">```{note}</span></span>
<span id="cb12-707"><a href="#cb12-707" aria-hidden="true" tabindex="-1"></a><span class="in">The WL test above is called the 1-WL test. There are higher-order WL tests that can distinguish more graphs, which are the basis of advanced GNNs.</span></span>
<span id="cb12-708"><a href="#cb12-708" aria-hidden="true" tabindex="-1"></a><span class="in">Check out [this note](https://www.moldesk.net/blog/weisfeiler-lehman-isomorphism-test/)</span></span>
<span id="cb12-709"><a href="#cb12-709" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-710"><a href="#cb12-710" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-711"><a href="#cb12-711" aria-hidden="true" tabindex="-1"></a><span class="fu">### GIN</span></span>
<span id="cb12-712"><a href="#cb12-712" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-713"><a href="#cb12-713" aria-hidden="true" tabindex="-1"></a>GIN {footcite}<span class="in">`xu2018how`</span> is a GNN that is based on the WL test.</span>
<span id="cb12-714"><a href="#cb12-714" aria-hidden="true" tabindex="-1"></a>The key idea is to focus on the parallel between the WL test and the GNN update rule.</span>
<span id="cb12-715"><a href="#cb12-715" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>In the WL test, we iteratively collect the labels of neighbors and aggregate them through a *hash function*.</span>
<span id="cb12-716"><a href="#cb12-716" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>In the GraphSAGE and GAT, the labels are the nodes' features, and the aggregation is some arithmetic operations such as mean or max.</span>
<span id="cb12-717"><a href="#cb12-717" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-718"><a href="#cb12-718" aria-hidden="true" tabindex="-1"></a>The key difference is that the hash function in the WL test always distinguishes different sets of neighbors' labels, while the aggregation in GraphSAGE and GAT does not always do so. For example, if all nodes have the same feature (e.g., all 1), the aggregation by the mean or max will result in the same value for all nodes, whereas the hash function in the WL test can still distinguish different sets of neighbors' labels by *the count of each label*.</span>
<span id="cb12-719"><a href="#cb12-719" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-720"><a href="#cb12-720" aria-hidden="true" tabindex="-1"></a>The resulting convolution update rule is:</span>
<span id="cb12-721"><a href="#cb12-721" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-722"><a href="#cb12-722" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-723"><a href="#cb12-723" aria-hidden="true" tabindex="-1"></a>h_v^{(k+1)} = \text{MLP}^{(k)}\left((1 + \epsilon^{(k)}) \cdot h_v^{(k)} + \sum_{u \in \mathcal{N}(v)} h_u^{(k)}\right)</span>
<span id="cb12-724"><a href="#cb12-724" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-725"><a href="#cb12-725" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-726"><a href="#cb12-726" aria-hidden="true" tabindex="-1"></a>where $\text{MLP}^{(k)}$ is a multi-layer perceptron (MLP) with $k$ layers, and $\epsilon^{(k)}$ is a fixed or trainable parameter.</span>
<span id="cb12-727"><a href="#cb12-727" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-728"><a href="#cb12-728" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-729"><a href="#cb12-729" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-732"><a href="#cb12-732" aria-hidden="true" tabindex="-1"></a><span class="in">```{footbibliography}</span></span>
<span id="cb12-733"><a href="#cb12-733" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Copyright 2024, Sadamori Kojaku</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions"><ul><li><a href="https://github.com/skojaku/adv-net-sci/edit/main/m09-graph-neural-networks/02-coding.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/skojaku/adv-net-sci/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/skojaku/adv-net-sci">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>