<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Sadamori Kojaku">
<meta name="dcterms.date" content="2025-07-27">

<title>Embedding Methods: Implementation and Practice</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../m08-embedding/03-exercises.html" rel="next">
<link href="../m08-embedding/01-concepts.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-0348920b7671f696dc9078d39bff215e.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-e6dc204ec8b52f55243daf2cac742210.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "|"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../assets/custom.css">
</head>

<body class="nav-sidebar docked nav-fixed fullcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../logo.jpg" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Advanced Topics in Network Science</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-course" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Course</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-course">    
        <li>
    <a class="dropdown-item" href="../course/welcome.html">
 <span class="dropdown-text">Welcome</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../course/about.html">
 <span class="dropdown-text">About</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../course/discord.html">
 <span class="dropdown-text">Discord</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../course/minidora-usage.html">
 <span class="dropdown-text">Minidora</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../course/setup.html">
 <span class="dropdown-text">Setup</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-intro" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Intro</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-intro">    
        <li>
    <a class="dropdown-item" href="../intro/why-networks.html">
 <span class="dropdown-text">Why Networks?</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-foundations" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Foundations</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-foundations">    
        <li class="dropdown-header">─── M01: Euler Path ───</li>
        <li>
    <a class="dropdown-item" href="../m01-euler_tour/01-concepts.html">
 <span class="dropdown-text">Concepts</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m01-euler_tour/02-coding.html">
 <span class="dropdown-text">Coding</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m01-euler_tour/03-exercises.html">
 <span class="dropdown-text">Exercises</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m01-euler_tour/04-advanced.html">
 <span class="dropdown-text">Advanced</span></a>
  </li>  
        <li class="dropdown-header">─── M02: Small World ───</li>
        <li>
    <a class="dropdown-item" href="../m02-small-world/01-concepts.html">
 <span class="dropdown-text">Concepts</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m02-small-world/02-coding.html">
 <span class="dropdown-text">Coding</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m02-small-world/03-exercises.html">
 <span class="dropdown-text">Exercises</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m02-small-world/04-appendix.html">
 <span class="dropdown-text">Appendix</span></a>
  </li>  
        <li class="dropdown-header">─── M03: Robustness ───</li>
        <li>
    <a class="dropdown-item" href="../m03-robustness/01-concepts.html">
 <span class="dropdown-text">Concepts</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m03-robustness/02-coding.html">
 <span class="dropdown-text">Coding</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m03-robustness/03-exercises.html">
 <span class="dropdown-text">Exercises</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m03-robustness/04-appendix.html">
 <span class="dropdown-text">Appendix</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-core-topics" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Core Topics</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-core-topics">    
        <li class="dropdown-header">─── M04: Friendship Paradox ───</li>
        <li>
    <a class="dropdown-item" href="../m04-node-degree/01-concepts.html">
 <span class="dropdown-text">Concepts</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m04-node-degree/02-coding.html">
 <span class="dropdown-text">Coding</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m04-node-degree/03-exercises.html">
 <span class="dropdown-text">Exercises</span></a>
  </li>  
        <li class="dropdown-header">─── M05: Clustering ───</li>
        <li>
    <a class="dropdown-item" href="../m05-clustering/01-concepts.html">
 <span class="dropdown-text">Concepts</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m05-clustering/02-coding.html">
 <span class="dropdown-text">Coding</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m05-clustering/03-exercises.html">
 <span class="dropdown-text">Exercises</span></a>
  </li>  
        <li class="dropdown-header">─── M06: Centrality ───</li>
        <li>
    <a class="dropdown-item" href="../m06-centrality/01-concepts.html">
 <span class="dropdown-text">Concepts</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m06-centrality/02-coding.html">
 <span class="dropdown-text">Coding</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m06-centrality/03-exercises.html">
 <span class="dropdown-text">Exercises</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-advanced-topics" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Advanced Topics</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-advanced-topics">    
        <li class="dropdown-header">─── M07: Random Walks ───</li>
        <li>
    <a class="dropdown-item" href="../m07-random-walks/01-concepts.html">
 <span class="dropdown-text">Concepts</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m07-random-walks/02-coding.html">
 <span class="dropdown-text">Coding</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m07-random-walks/03-exercises.html">
 <span class="dropdown-text">Exercises</span></a>
  </li>  
        <li class="dropdown-header">─── M08: Embedding ───</li>
        <li>
    <a class="dropdown-item" href="../m08-embedding/01-concepts.html">
 <span class="dropdown-text">Concepts</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m08-embedding/02-coding.html">
 <span class="dropdown-text">Coding</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m08-embedding/03-exercises.html">
 <span class="dropdown-text">Exercises</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m08-embedding/04-appendix.html">
 <span class="dropdown-text">Appendix</span></a>
  </li>  
        <li class="dropdown-header">─── M09: Graph Neural Networks ───</li>
        <li>
    <a class="dropdown-item" href="../m09-graph-neural-networks/01-concepts.html">
 <span class="dropdown-text">Concepts</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m09-graph-neural-networks/02-coding.html">
 <span class="dropdown-text">Coding</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m09-graph-neural-networks/03-exercises.html">
 <span class="dropdown-text">Exercises</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../m09-graph-neural-networks/04-appendix.html">
 <span class="dropdown-text">Appendix</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../m08-embedding/01-concepts.html">M08: Embedding</a></li><li class="breadcrumb-item"><a href="../m08-embedding/02-coding.html">Advanced Topics in Network Science</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false">
 <span class="menu-text">Course Information</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course/welcome.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course/about.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About us</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course/discord.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Discord</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course/minidora-usage.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Using Minidora</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../course/how-to-submit-assignment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">How to submit assignment</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">Introduction</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../intro/why-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Networks</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false">
 <span class="menu-text">M01: Euler Path</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m01-euler_tour/01-concepts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">A Stroll, Seven Bridges, and a Mathematical Revolution</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m01-euler_tour/02-coding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Coding Networks in Python</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m01-euler_tour/03-exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exercises</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m01-euler_tour/04-advanced.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Advanced: Sparse Matrices for Large-Scale Networks</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false">
 <span class="menu-text">M02: Small World</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m02-small-world/01-concepts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Core Concepts</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m02-small-world/02-coding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Efficient Network Representation and Computing Paths</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m02-small-world/03-exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exercises and Assignments</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m02-small-world/04-appendix.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Appendix - Brief Introduction to igraph</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false">
 <span class="menu-text">M03: Robustness</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m03-robustness/01-concepts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Core Concepts</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m03-robustness/02-coding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Coding - Network Robustness Analysis</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m03-robustness/03-exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exercises and Assignments</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m03-robustness/04-appendix.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exercises and Assignments</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false">
 <span class="menu-text">M04: Friendship Paradox</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-node-degree/01-concepts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Core Concepts</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-node-degree/02-coding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Visualizing Degree Distributions in Python</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m04-node-degree/03-exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exercises</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false">
 <span class="menu-text">M05: Clustering</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m05-clustering/01-concepts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Core Concepts</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m05-clustering/02-coding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Clustering Algorithms and Implementation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m05-clustering/03-exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exercises and Assignments</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="false">
 <span class="menu-text">M06: Centrality</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m06-centrality/01-concepts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Advanced Topics in Network Science</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m06-centrality/02-coding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Advanced Topics in Network Science</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m06-centrality/03-exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Advanced Topics in Network Science</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false">
 <span class="menu-text">M07: Random Walks</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m07-random-walks/01-concepts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Advanced Topics in Network Science</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m07-random-walks/02-coding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Advanced Topics in Network Science</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m07-random-walks/03-exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Advanced Topics in Network Science</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="true">
 <span class="menu-text">M08: Embedding</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m08-embedding/01-concepts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Advanced Topics in Network Science</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m08-embedding/02-coding.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Advanced Topics in Network Science</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m08-embedding/03-exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Advanced Topics in Network Science</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m08-embedding/04-appendix.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Advanced Topics in Network Science</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="false">
 <span class="menu-text">M09: Graph Neural Networks</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-11" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m09-graph-neural-networks/01-concepts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Advanced Topics in Network Science</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m09-graph-neural-networks/02-coding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Advanced Topics in Network Science</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m09-graph-neural-networks/03-exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Advanced Topics in Network Science</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../m09-graph-neural-networks/04-appendix.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Advanced Topics in Network Science</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../m08-embedding/01-concepts.html">M08: Embedding</a></li><li class="breadcrumb-item"><a href="../m08-embedding/02-coding.html">Advanced Topics in Network Science</a></li></ol></nav>
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Embedding Methods: Implementation and Practice</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Sadamori Kojaku </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">July 27, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="spectral-embedding" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="spectral-embedding"><span class="header-section-number">1</span> Spectral Embedding</h2>
<section id="network-compression-approach" class="level3">
<h3 class="anchored" data-anchor-id="network-compression-approach">Network Compression Approach</h3>
<p>Networks are a high-dimensional discrete data that can be difficult to analyze with traditional machine learning methods that assume continuous and smooth data. Spectral embedding is a technique to embed networks into low-dimensional spaces.</p>
<p>Let us approach the spectral embedding from the perspective of network compression. Suppose we have an adjacency matrix <span class="math inline">\mathbf{A}</span> of a network. The adjacency matrix is a high-dimensional data, i.e., a matrix has size <span class="math inline">N \times N</span> for a network of <span class="math inline">N</span> nodes. We want to compress it into a lower-dimensional matrix <span class="math inline">\mathbf{U}</span> of size <span class="math inline">N \times d</span> for a user-defined small integer <span class="math inline">d &lt; N</span>. A good <span class="math inline">\mathbf{U}</span> should preserve the network structure and thus can reconstruct the original data <span class="math inline">\mathbf{A}</span> as closely as possible. This leads to the following optimization problem:</p>
<p><span class="math display">
\min_{\mathbf{U}} J(\mathbf{U}),\quad J(\mathbf{U}) = \| \mathbf{A} - \mathbf{U}\mathbf{U}^\top \|_F^2
</span></p>
<p>where:</p>
<ol type="1">
<li><span class="math inline">\mathbf{U}\mathbf{U}^\top</span> is the outer product of <span class="math inline">\mathbf{U}</span> and represents the reconstructed network.</li>
<li><span class="math inline">\|\cdot\|_F</span> is the Frobenius norm, which is the sum of the squares of the elements in the matrix.</li>
<li><span class="math inline">J(\mathbf{U})</span> is the loss function that measures the difference between the original network <span class="math inline">\mathbf{A}</span> and the reconstructed network <span class="math inline">\mathbf{U}\mathbf{U}^\top</span>.</li>
</ol>
<p>By minimizing the Frobenius norm with respect to <span class="math inline">\mathbf{U}</span>, we obtain the best low-dimensional embedding of the network.</p>
</section>
<section id="an-intuitive-solution" class="level3">
<h3 class="anchored" data-anchor-id="an-intuitive-solution">An Intuitive Solution</h3>
<p>Let us first understand the solution intuitively. Consider the spectral decomposition of <span class="math inline">\mathbf{A}</span>:</p>
<p><span class="math display">
\mathbf{A} = \sum_{i=1}^N \lambda_i \mathbf{u}_i \mathbf{u}_i^\top
</span></p>
<p>where <span class="math inline">\lambda_i</span> are weights and <span class="math inline">\mathbf{u}_i</span> are column vectors. Each term <span class="math inline">\lambda_i \mathbf{u}_i \mathbf{u}_i^\top</span> is a rank-one matrix that captures a part of the network’s structure. The larger the weight <span class="math inline">\lambda_i</span>, the more important that term is in describing the network.</p>
<p>To compress the network, we can select the <span class="math inline">d</span> terms with the largest weights <span class="math inline">\lambda_i</span>. By combining the corresponding <span class="math inline">\mathbf{u}_i</span> vectors into a matrix <span class="math inline">\mathbf{U}</span>, we obtain a good low-dimensional embedding of the network.</p>
<p><img src="../figs/spectral-decomposition.jpg" class="img-fluid"></p>
<p>For a formal proof, please refer to the <a href="../m08-embedding/04-appendix.html">Appendix section</a>.</p>
</section>
<section id="example-spectral-embedding" class="level3">
<h3 class="anchored" data-anchor-id="example-spectral-embedding">Example: Spectral Embedding</h3>
<p>Let us demonstrate the results with a simple example as follows.</p>
<div id="a8020d69" class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>:tags: [hide<span class="op">-</span><span class="bu">input</span>]</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> networkx <span class="im">as</span> nx</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a small example network</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>G <span class="op">=</span> nx.karate_club_graph()</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> nx.adjacency_matrix(G).toarray()</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> np.unique([d[<span class="dv">1</span>][<span class="st">'club'</span>] <span class="cf">for</span> d <span class="kw">in</span> G.nodes(data<span class="op">=</span><span class="va">True</span>)], return_inverse<span class="op">=</span><span class="va">True</span>)[<span class="dv">1</span>]</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>cmap <span class="op">=</span> sns.color_palette()</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>nx.draw(G, with_labels<span class="op">=</span><span class="va">False</span>, node_color<span class="op">=</span>[cmap[i] <span class="cf">for</span> i <span class="kw">in</span> labels])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="95bef55f" class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>:tags: [hide<span class="op">-</span><span class="bu">input</span>]</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the spectral decomposition</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>eigvals, eigvecs <span class="op">=</span> np.linalg.eig(A)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Find the top d eigenvectors</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>d <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>sorted_indices <span class="op">=</span> np.argsort(eigvals)[::<span class="op">-</span><span class="dv">1</span>][:d]</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>eigvals <span class="op">=</span> eigvals[sorted_indices]</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>eigvecs <span class="op">=</span> eigvecs[:, sorted_indices]</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the results</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">5</span>))</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(x <span class="op">=</span> eigvecs[:, <span class="dv">0</span>], y <span class="op">=</span> eigvecs[:, <span class="dv">1</span>], hue<span class="op">=</span>labels, ax<span class="op">=</span>ax)</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Spectral Embedding'</span>)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Eigenvector 1'</span>)</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Eigenvector 2'</span>)</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Interestingly, the first eigenvector corresponds to the eigen centrality of the network, representing the centrality of the nodes. The second eigenvector captures the community structure of the network, clearly separating the two communities in the network.</p>
</section>
<section id="modularity-embedding" class="level3">
<h3 class="anchored" data-anchor-id="modularity-embedding">Modularity Embedding</h3>
<p>In a similar vein, we can use the modularity matrix to generate a low-dimensional embedding of the network. Namely, let us define the modularity matrix <span class="math inline">\mathbf{Q}</span> as follows:</p>
<p><span class="math display">
Q_{ij} = \frac{1}{2m}A_{ij} - \frac{k_i k_j}{4m^2}
</span></p>
<p>where <span class="math inline">k_i</span> is the degree of node <span class="math inline">i</span>, and <span class="math inline">m</span> is the number of edges in the network.</p>
<p>We then compute the eigenvectors of <span class="math inline">\mathbf{Q}</span> and use them to embed the network into a low-dimensional space just as we did for the adjacency matrix.</p>
<div id="8b125bf0" class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>:tags: [hide<span class="op">-</span><span class="bu">input</span>]</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>deg <span class="op">=</span> np.<span class="bu">sum</span>(A, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> np.<span class="bu">sum</span>(deg) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>Q <span class="op">=</span> A <span class="op">-</span> np.outer(deg, deg) <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> m)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>Q<span class="op">/=</span> <span class="dv">2</span><span class="op">*</span>m</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>eigvals, eigvecs <span class="op">=</span> np.linalg.eig(Q)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Sort the eigenvalues and eigenvectors</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>sorted_indices <span class="op">=</span> np.argsort(<span class="op">-</span>eigvals)[:d]  <span class="co"># Exclude the first eigenvector</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>eigvals <span class="op">=</span> eigvals[sorted_indices]</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>eigvecs <span class="op">=</span> eigvecs[:, sorted_indices]</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">5</span>))</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(x <span class="op">=</span> eigvecs[:, <span class="dv">0</span>], y <span class="op">=</span> eigvecs[:, <span class="dv">1</span>], hue<span class="op">=</span>labels, ax<span class="op">=</span>ax)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Modularity Embedding'</span>)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Eigenvector 1'</span>)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Eigenvector 2'</span>)</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<pre class="{note}"><code>The modularity embedding can be used to bipartition the network into two communities using a simple algorithm: group nodes with the same sign of the second eigenvector {footcite}`newman2006modularity`.</code></pre>
</section>
<section id="laplacian-eigenmap" class="level3">
<h3 class="anchored" data-anchor-id="laplacian-eigenmap">Laplacian Eigenmap</h3>
<p>Laplacian Eigenmap {footcite}<code>belkin2003laplacian</code> is another approach to compress a network into a low-dimensional space. The fundamental idea behind this method is to position connected nodes close to each other in the low-dimensional space. This approach leads to the following optimization problem:</p>
<p><span class="math display">
\min_{\mathbf{U}} J_{LE}(\mathbf{U}),\quad J_{LE}(\mathbf{U}) = \frac{1}{2}\sum_{i,j} A_{ij} \| u_i - u_j \|^2
</span></p>
<p>In this equation, <span class="math inline">\| u_i - u_j \|^2</span> represents the squared distance between nodes <span class="math inline">i</span> and <span class="math inline">j</span> in the low-dimensional space. The goal is to minimize this distance for connected nodes (where <span class="math inline">A_{ij} = 1</span>). The factor <span class="math inline">\frac{1}{2}</span> is included for mathematical convenience in later calculations.</p>
<p>To solve this optimization problem, we rewrite <span class="math inline">J_{LE}(\mathbf{U})</span> as follows:</p>
<p><span class="math display">
\begin{aligned}
J_{LE}(\mathbf{U}) &amp;= \frac{1}{2}\sum_{i}\sum_{j} A_{ij} \| u_i - u_j \|^2 \\
&amp;= \frac{1}{2}\sum_{i}\sum_{j} A_{ij} \left( \| u_i \|^2 - 2 u_i^\top u_j + \| u_j \|^2 \right) \\
&amp;= \sum_{i}\sum_{j} A_{ij} \| u_i \|^2 - \sum_{i}\sum_{j} A_{ij} u_i^\top u_j\\
&amp;= \sum_{i} k_i \| u_i \|^2 - \sum_{i,j} A_{ij} u_i^\top u_j\\
&amp;= \sum_{i,j} L_{ij} u_i^\top u_j
\end{aligned}
</span></p>
<p>where</p>
<p><span class="math display">
L_{ij} = \begin{cases}
k_i &amp; \text{if } i = j \\
-A_{ij} &amp; \text{if } i \neq j
\end{cases}
</span></p>
<p>The minimization problem can be rewritten as:</p>
<p><span class="math display">
J_{LE}(\mathbf{U}) = \text{Tr}(\mathbf{U}^\top \mathbf{L} \mathbf{U})
</span></p>
<p>where</p>
<p><span class="math display">
\mathbf{U} =
\begin{bmatrix}
\mathbf{u}_1 ^\top \\
\mathbf{u}_2 ^\top \\
\vdots \\
\mathbf{u}_N ^\top \\
\end{bmatrix}
</span></p>
<p>See the <a href="../m08-embedding/04-appendix.html">Appendix section</a> for the detailed derivation.</p>
<p>By taking the derivative of <span class="math inline">J_{LE}(\mathbf{U})</span> with respect to <span class="math inline">\mathbf{U}</span> and set it to zero, we obtain the following equation:</p>
<p><span class="math display">
\frac{\partial J_{LE}}{\partial \mathbf{U}} = 0 \implies \mathbf{L} \mathbf{U} = \lambda \mathbf{U}
</span></p>
<p>The solution is the <span class="math inline">d</span> eigenvectors associated with the <span class="math inline">d</span> smallest eigenvalues of <span class="math inline">\mathbf{L}</span>.</p>
<p>It is important to note that the eigenvector corresponding to the smallest eigenvalue (which is always zero for connected graphs) is trivial - it’s the all-one vector. Therefore, in practice, we typically compute the <span class="math inline">d+1</span> smallest eigenvectors and discard the one corresponding to the zero eigenvalue.</p>
</section>
<section id="example-laplacian-eigenmap" class="level3">
<h3 class="anchored" data-anchor-id="example-laplacian-eigenmap">Example: Laplacian Eigenmap</h3>
<p>Let us first compute the Laplacian matrix and its eigenvectors.</p>
<div id="ea1aaf98" class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>:tags: [hide<span class="op">-</span><span class="bu">input</span>]</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>D <span class="op">=</span> np.diag(np.<span class="bu">sum</span>(A, axis<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>L <span class="op">=</span> D <span class="op">-</span> A</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>eigvals, eigvecs <span class="op">=</span> np.linalg.eig(L)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Sort the eigenvalues and eigenvectors</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>sorted_indices <span class="op">=</span> np.argsort(eigvals)[<span class="dv">1</span>:d<span class="op">+</span><span class="dv">1</span>]  <span class="co"># Exclude the first eigenvector</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>eigvals <span class="op">=</span> eigvals[sorted_indices]</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>eigvecs <span class="op">=</span> eigvecs[:, sorted_indices]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The eigenvectors corresponding to the <span class="math inline">d</span> smallest eigenvalues are:</p>
<div id="59471a93" class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>:tags: [hide<span class="op">-</span><span class="bu">input</span>]</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">5</span>))</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(x <span class="op">=</span> eigvecs[:, <span class="dv">0</span>], y <span class="op">=</span> eigvecs[:, <span class="dv">1</span>], hue<span class="op">=</span>labels, ax<span class="op">=</span>ax)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Laplacian Eigenmap'</span>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Eigenvector 2'</span>)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Eigenvector 3'</span>)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="neural-embedding-with-word2vec" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="neural-embedding-with-word2vec"><span class="header-section-number">2</span> Neural Embedding with word2vec</h2>
<section id="introduction-to-word2vec" class="level3">
<h3 class="anchored" data-anchor-id="introduction-to-word2vec">Introduction to word2vec</h3>
<p>In this section, we will introduce <em>word2vec</em>, a powerful technique for learning word embeddings. word2vec is a neural network model that learns words embeddings in a continuous vector space. It was introduced by Tomas Mikolov and his colleagues at Google in 2013 {footcite}<code>mikolov2013distributed</code>.</p>
</section>
<section id="how-it-works" class="level3">
<h3 class="anchored" data-anchor-id="how-it-works">How it works</h3>
<p>“You shall know a word by the company it keeps” {footcite}<code>church1988word</code> is a famous quote in linguistics. It means that you can understand the meaning of a word by looking at the words that appear in the same context. word2vec operates on the same principle. word2vec identifies a word’s context by examining the words within a fixed window around it. For example, in the sentence:</p>
<blockquote class="blockquote">
<p>The quick brown fox jumps over a lazy dog</p>
</blockquote>
<p>The context of the word <em>fox</em> includes <em>quick</em>, <em>brown</em>, <em>jumps</em>, <em>over</em>, and <em>lazy</em>. word2vec is trained to predict which words are likely to appear as the context of an input word.</p>
<pre class="{note}"><code>There are two main architectures for word2vec:
1. **Continuous Bag of Words (CBOW)**: Predicts the target word (center word) from the context words (surrounding words).
2. **Skip-gram**: Predicts the context words (surrounding words) from the target word (center word).</code></pre>
<p>So how are word embeddings learned? word2vec is a neural network model that looks like a bow tie. It has two layers of the vocabulary size coupled with a much smaller hidden layer.</p>
<p><img src="../figs/word2vec.png" class="img-fluid"></p>
<ul>
<li><p><strong>Input layer</strong>: The input layer consists of <span class="math inline">N</span> neurons, where <span class="math inline">N</span> is the size of the vocabulary (i.e., the number of unique words in the corpus). Each neuron corresponds to a unique word in the vocabulary. When a word is inputted, its corresponding neuron is activated and the other neurons are inhibited. Thus, the input layer is essentially a lookup mechanism that transforms the input word into a corresponding one-hot vector.</p></li>
<li><p><strong>Output layer</strong>: The output layer also consists of <span class="math inline">N</span> neurons, each corresponding to a unique word in the vocabulary. Unlike the input layer, multiple neurons can be activated for a single input. The strength of the activation of each neuron (with a normalization by the softmax function) represents the probability of the corresponding word being the input word’s context.</p></li>
<li><p><strong>Hidden layer</strong>: The hidden layer is much smaller than the input and output layers. Multiple neurons in the hidden layer can be activated for a single input, and this activation pattern represents the word’s <em>embedding</em>.</p></li>
</ul>
<p>We can consider word2vec as a <em>dimensionality reduction</em> technique that reduces the dimensionality of the input layer to the hidden layer based on the co-occurrence of words within a short distance. The distance is named the <em>window size</em>, which is a user-defined hyperparameter.</p>
</section>
<section id="whats-special-about-word2vec" class="level3">
<h3 class="anchored" data-anchor-id="whats-special-about-word2vec">What’s special about word2vec?</h3>
<p>With word2vec, words are represented as dense vectors, enabling us to explore their relationships using simple linear algebra. This is in contrast to traditional natural language processing (NLP) methods, such as bag-of-words and topic modeling, which represent words as discrete units or high-dimensional vectors.</p>
<p><img src="https://miro.medium.com/v2/resize:fit:678/1*5F4TXdFYwqi-BWTToQPIfg.jpeg" class="img-fluid"></p>
<p>To showcase the effectiveness of word2vec, let’s walk through an example using the <code>gensim</code> library.</p>
<div id="af61221e" class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gensim</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gensim.downloader</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> Word2Vec</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Load pre-trained word2vec model from Google News</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> gensim.downloader.load(<span class="st">'word2vec-google-news-300'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Our first example is to find the words most similar to <em>king</em>.</p>
<div id="c6542a7e" class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Example usage</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>word <span class="op">=</span> <span class="st">"king"</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>similar_words <span class="op">=</span> model.most_similar(word)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Words most similar to '</span><span class="sc">{</span>word<span class="sc">}</span><span class="ss">':"</span>)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> similar_word, similarity <span class="kw">in</span> similar_words:</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>similar_word<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>similarity<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>A cool (yet controversial) application of word embeddings is analogy solving. Let us consider the following puzzle:</p>
<blockquote class="blockquote">
<p><em>man</em> is to <em>woman</em> as <em>king</em> is to ___ ?</p>
</blockquote>
<p>We can use word embeddings to solve this puzzle.</p>
<div id="c0fd1760" class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># We solve the puzzle by</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="co">#  vec(king) - vec(man) + vec(woman)</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="co"># To solve this, we use the model.most_similar function, with positive words being "king" and "woman" (additive), and negative words being "man" (subtractive).</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>model.most_similar(positive<span class="op">=</span>[<span class="st">'woman'</span>, <span class="st">"king"</span>], negative<span class="op">=</span>[<span class="st">'man'</span>], topn<span class="op">=</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The last example is to visualize the word embeddings.</p>
<div id="3cf3b4f8" class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>:tags: [hide<span class="op">-</span><span class="bu">input</span>]</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>countries <span class="op">=</span> [<span class="st">'Germany'</span>, <span class="st">'France'</span>, <span class="st">'Italy'</span>, <span class="st">'Spain'</span>, <span class="st">'Portugal'</span>, <span class="st">'Greece'</span>]</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>capital_words <span class="op">=</span> [<span class="st">'Berlin'</span>, <span class="st">'Paris'</span>, <span class="st">'Rome'</span>, <span class="st">'Madrid'</span>, <span class="st">'Lisbon'</span>, <span class="st">'Athens'</span>]</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the word embeddings for the countries and capitals</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>country_embeddings <span class="op">=</span> np.array([model[country] <span class="cf">for</span> country <span class="kw">in</span> countries])</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>capital_embeddings <span class="op">=</span> np.array([model[capital] <span class="cf">for</span> capital <span class="kw">in</span> capital_words])</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the PCA</span></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>pca <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>embeddings <span class="op">=</span> np.vstack([country_embeddings, capital_embeddings])</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>embeddings_pca <span class="op">=</span> pca.fit_transform(embeddings)</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a DataFrame for seaborn</span></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(embeddings_pca, columns<span class="op">=</span>[<span class="st">'PC1'</span>, <span class="st">'PC2'</span>])</span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'Label'</span>] <span class="op">=</span> countries <span class="op">+</span> capital_words</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'Type'</span>] <span class="op">=</span> [<span class="st">'Country'</span>] <span class="op">*</span> <span class="bu">len</span>(countries) <span class="op">+</span> [<span class="st">'Capital'</span>] <span class="op">*</span> <span class="bu">len</span>(capital_words)</span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the data</span></span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">10</span>))</span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a scatter plot with seaborn</span></span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a>scatter_plot <span class="op">=</span> sns.scatterplot(data<span class="op">=</span>df, x<span class="op">=</span><span class="st">'PC1'</span>, y<span class="op">=</span><span class="st">'PC2'</span>, hue<span class="op">=</span><span class="st">'Type'</span>, style<span class="op">=</span><span class="st">'Type'</span>, s<span class="op">=</span><span class="dv">200</span>, palette<span class="op">=</span><span class="st">'deep'</span>, markers<span class="op">=</span>[<span class="st">'o'</span>, <span class="st">'s'</span>])</span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Annotate the points</span></span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(df)):</span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a>    plt.text(df[<span class="st">'PC1'</span>][i], df[<span class="st">'PC2'</span>][i] <span class="op">+</span> <span class="fl">0.08</span>, df[<span class="st">'Label'</span>][i], fontsize<span class="op">=</span><span class="dv">12</span>, ha<span class="op">=</span><span class="st">'center'</span>, va<span class="op">=</span><span class="st">'bottom'</span>,</span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a>             bbox<span class="op">=</span><span class="bu">dict</span>(facecolor<span class="op">=</span><span class="st">'white'</span>, edgecolor<span class="op">=</span><span class="st">'none'</span>, alpha<span class="op">=</span><span class="fl">0.8</span>))</span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Draw arrows between countries and capitals</span></span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(countries)):</span>
<span id="cb11-39"><a href="#cb11-39" aria-hidden="true" tabindex="-1"></a>    plt.arrow(df[<span class="st">'PC1'</span>][i], df[<span class="st">'PC2'</span>][i], df[<span class="st">'PC1'</span>][i <span class="op">+</span> <span class="bu">len</span>(countries)] <span class="op">-</span> df[<span class="st">'PC1'</span>][i], df[<span class="st">'PC2'</span>][i <span class="op">+</span> <span class="bu">len</span>(countries)] <span class="op">-</span> df[<span class="st">'PC2'</span>][i],</span>
<span id="cb11-40"><a href="#cb11-40" aria-hidden="true" tabindex="-1"></a>              color<span class="op">=</span><span class="st">'gray'</span>, alpha<span class="op">=</span><span class="fl">0.6</span>, linewidth<span class="op">=</span><span class="fl">1.5</span>, head_width<span class="op">=</span><span class="fl">0.02</span>, head_length<span class="op">=</span><span class="fl">0.03</span>)</span>
<span id="cb11-41"><a href="#cb11-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-42"><a href="#cb11-42" aria-hidden="true" tabindex="-1"></a>plt.legend(title<span class="op">=</span><span class="st">'Type'</span>, title_fontsize<span class="op">=</span><span class="st">'13'</span>, fontsize<span class="op">=</span><span class="st">'11'</span>)</span>
<span id="cb11-43"><a href="#cb11-43" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'PCA of Country and Capital Word Embeddings'</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb11-44"><a href="#cb11-44" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Principal Component 1'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb11-45"><a href="#cb11-45" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Principal Component 2'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb11-46"><a href="#cb11-46" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> plt.gca()</span>
<span id="cb11-47"><a href="#cb11-47" aria-hidden="true" tabindex="-1"></a>ax.set_axis_off()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can see that word2vec places the words representing countries close to each other and so do the words representing their capitals. The country-capital relationship is also roughly preserved, e.g., <em>Germany</em>-<em>Berlin</em> vector is roughly parallel to <em>France</em>-<em>Paris</em> vector.</p>
</section>
</section>
<section id="graph-embedding-with-word2vec" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="graph-embedding-with-word2vec"><span class="header-section-number">3</span> Graph Embedding with word2vec</h2>
<p>How can we apply word2vec to graph data? There is a critical challenge: word2vec takes sequence of words as input, while graph data are discrete and unordered. A solution to fill this gap is <em>random walk</em>, which transforms graph data into a sequence of nodes. Once we have a sequence of nodes, we can treat it as a sequence of words and apply word2vec.</p>
<section id="deepwalk" class="level3">
<h3 class="anchored" data-anchor-id="deepwalk">DeepWalk</h3>
<p><img src="https://dt5vp8kor0orz.cloudfront.net/7c56c256b9fbf06693da47737ac57fae803a5a4f/1-Figure1-1.png" class="img-fluid"></p>
<p>DeepWalk is one of the pioneering works to apply word2vec to graph data {footcite}<code>perozzi2014deepwalk</code>. It views the nodes as words and the nodes random walks on the graph as sentences, and applies word2vec to learn the node embeddings.</p>
<p>More specifically, the method contains the following steps:</p>
<ol type="1">
<li>Sample multiple random walks from the graph.</li>
<li>Treat the random walks as sentences and feed them to word2vev to learn the node embeddings.</li>
</ol>
<p>There are some technical details that we need to be aware of, which we will learn by implementing DeepWalk in the following exercise.</p>
</section>
<section id="exercise-01-implement-deepwalk" class="level3">
<h3 class="anchored" data-anchor-id="exercise-01-implement-deepwalk">Exercise 01: Implement DeepWalk</h3>
<p>In this exercise, we implement DeepWalk step by step.</p>
<section id="step-1-data-preparation" class="level5">
<h5 class="anchored" data-anchor-id="step-1-data-preparation">Step 1: Data preparation</h5>
<p>We will use the karate club network as an example.</p>
<p><strong>Load the data</strong></p>
<div id="d8ceb6d2" class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>:tags: [hide<span class="op">-</span><span class="bu">input</span>]</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> igraph</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> networkx <span class="im">as</span> nx</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> igraph.Graph.Famous(<span class="st">"Zachary"</span>)</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> g.get_adjacency_sparse()</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Add the community labels to the nodes for visualization</span></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>g.vs[<span class="st">"label"</span>] <span class="op">=</span> np.unique([d[<span class="dv">1</span>][<span class="st">'club'</span>] <span class="cf">for</span> d <span class="kw">in</span> nx.karate_club_graph().nodes(data<span class="op">=</span><span class="va">True</span>)], return_inverse<span class="op">=</span><span class="va">True</span>)[<span class="dv">1</span>]</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>palette <span class="op">=</span> sns.color_palette().as_hex()</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>igraph.plot(g, vertex_color<span class="op">=</span>[palette[label] <span class="cf">for</span> label <span class="kw">in</span> g.vs[<span class="st">"label"</span>]], bbox<span class="op">=</span>(<span class="dv">300</span>, <span class="dv">300</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="step-2-generate-random-walks" class="level5">
<h5 class="anchored" data-anchor-id="step-2-generate-random-walks">Step 2: Generate random walks</h5>
<p>Next, we generate the training data for the word2vec model by generating multiple random walks starting from each node in the network. Let us first implement a function to sample random walks from a given network.</p>
<div id="76419ea4" class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> random_walk(net, start_node, walk_length):</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize the walk with the starting node</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    walk <span class="op">=</span> [start_node]</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Continue the walk until the desired length is reached</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="bu">len</span>(walk) <span class="op">&lt;</span> walk_length:</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Get the current node (the last node in the walk)</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>        cur <span class="op">=</span> walk[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Get the neighbors of the current node</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>        cur_nbrs <span class="op">=</span> <span class="bu">list</span>(net[cur].indices)</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># If the current node has neighbors, randomly choose one and add it to the walk</span></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">len</span>(cur_nbrs) <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>            walk.append(np.random.choice(cur_nbrs))</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>            <span class="co"># If the current node has no neighbors, terminate the walk</span></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Return the generated walk</span></span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> walk</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Generate 10 random walks of length 50 starting from each node.</p>
<div id="ab3b9597" class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>n_nodes <span class="op">=</span> g.vcount()</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>n_walkers_per_node <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>walk_length <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>walks <span class="op">=</span> []</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_nodes):</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_walkers_per_node):</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>        walks.append(random_walk(A, i, walk_length))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="step-3-train-the-word2vec-model" class="level5">
<h5 class="anchored" data-anchor-id="step-3-train-the-word2vec-model">Step 3: Train the word2vec model</h5>
<p>Then, we feed the random walks to the word2vec model.</p>
<div id="a6a5ddf3" class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> Word2Vec</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Word2Vec(walks, vector_size<span class="op">=</span><span class="dv">32</span>, window<span class="op">=</span><span class="dv">3</span>, min_count<span class="op">=</span><span class="dv">1</span>, sg<span class="op">=</span><span class="dv">1</span>, hs <span class="op">=</span> <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Here,</p>
<ul>
<li><code>vector_size</code> is the dimension of the embedding vectors.</li>
<li><code>window</code> indicates the maximum distance between a word and its context words. For example, in the random walk <code>[0, 1, 2, 3, 4, 5, 6, 7]</code>, the context words of node 2 are <code>[0, 1, 3, 4, 5]</code> when <code>window=3</code>.</li>
<li><code>min_count</code> is the minimum number of times a word must appear in the training data to be included in the vocabulary.</li>
</ul>
<p>Two parameters <code>sg=1</code> and <code>hs=1</code> indicate that we are using the skip-gram model with negative sampling. Let us understand what they mean in detail as follows.</p>
<ul>
<li><p><strong>Skip-gram model</strong>: it trains word2vec by predicting context words given a target word. For example, given the sentence “The quick brown fox jumps over the lazy dog”, in the skip-gram model, given the target word “fox”, the model will try to predict the context words “quick”, “brown”, “jumps”, and “over”. If <code>sg=0</code>, the input and output are swapped: the model will predict the target word from the context words, e.g., given the context words “quick”, “brown”, “jumps”, and “over”, the model will predict the target word “fox”.</p></li>
<li><p><strong>Hierarchical softmax</strong>: To understand hierarchical softmax better, let’s break down how the word2vec model works. The goal of word2vec is to predict context words given a target word. For example, if our target word is <span class="math inline">w_t</span> and our context word is <span class="math inline">w_c</span>, we want to find the probability of <span class="math inline">w_c</span> given <span class="math inline">w_t</span>. This probability is calculated using the softmax function:</p>
<p><span class="math display">
  P(w_c | w_t) = \frac{\exp(\mathbf{v}_{w_c} \cdot \mathbf{v}_{w_t})}{\sum_{w \in V} \exp(\mathbf{v}_w \cdot \mathbf{u}_{w_t})}
</span></p>
<p>Here, <span class="math inline">\mathbf{v}_w</span> and <span class="math inline">\mathbf{u}_w</span> represent the vector for word <span class="math inline">w</span> as context and target respectively, and <span class="math inline">V</span> is the entire vocabulary. The tricky part is the denominator, which requires summing over all words in the vocabulary. If we have a large vocabulary, this can be very computationally expensive. Imagine having to compute 100,000 exponentials and their sum for each training example if our vocabulary size is 100,000!</p>
<p>Hierarchical softmax helps us solve this problem. Instead of calculating the probability directly, it organizes the vocabulary into a binary tree, where each word is a leaf node. To find the probability of a word, we calculate the product of probabilities along the path from the root to the leaf node. This method significantly reduces the computational complexity. Instead of being proportional to the vocabulary size, it becomes proportional to the logarithm of the vocabulary size. This makes it much more efficient, especially for large vocabularies.</p>
<p><img src="https://lh5.googleusercontent.com/proxy/_omrC8G6quTl2SGarwFe57qzbIs-PtGkEA5yODFE5I0Ny2IHGiJwsUhMrcuUqg5o-R2nD9hkgMuZsQJKoCggP29zXtj-Vz-X8BE.png" class="img-fluid"></p></li>
</ul>
<p>By using the skip-gram model with hierarchical softmax, we can efficiently learn high-quality word embeddings even when dealing with large vocabularies.</p>
<p>Now, we extract the node embeddings from the word2vec model. In the word2vec model, the embeddings are stored in the <code>wv</code> attribute. The embedding of node <span class="math inline">i</span> is given by <code>model.wv[i]</code>.</p>
<div id="f2ed82c0" class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>embedding <span class="op">=</span> []</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_nodes):</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    embedding.append(model.wv[i])</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>embedding <span class="op">=</span> np.array(embedding)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><code>embedding</code> is the matrix of node embeddings. It has the same number of rows as the number of nodes in the network, and the number of columns is the embedding dimension.</p>
<p><strong>Print the first 3 nodes</strong></p>
<div id="d885d052" class="cell">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>:tags: [hide<span class="op">-</span><span class="bu">input</span>]</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>embedding[:<span class="dv">3</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s visualize the node embeddings using UMAP.</p>
<div id="dd2fb413" class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>:tags: [hide<span class="op">-</span><span class="bu">input</span>]</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> umap</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> bokeh.plotting <span class="im">import</span> figure, show</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> bokeh.io <span class="im">import</span> output_notebook</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> bokeh.models <span class="im">import</span> ColumnDataSource, HoverTool</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>reducer <span class="op">=</span> umap.UMAP(n_components<span class="op">=</span><span class="dv">2</span>, random_state<span class="op">=</span><span class="dv">42</span>, n_neighbors<span class="op">=</span><span class="dv">15</span>, metric<span class="op">=</span><span class="st">"cosine"</span>)</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>xy <span class="op">=</span> reducer.fit_transform(embedding)</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>output_notebook()</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the degree of each node</span></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>degrees <span class="op">=</span> A.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>).A1</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>source <span class="op">=</span> ColumnDataSource(data<span class="op">=</span><span class="bu">dict</span>(</span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span>xy[:, <span class="dv">0</span>],</span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>    y<span class="op">=</span>xy[:, <span class="dv">1</span>],</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>    size<span class="op">=</span>np.sqrt(degrees <span class="op">/</span> np.<span class="bu">max</span>(degrees)) <span class="op">*</span> <span class="dv">30</span>,</span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>    community<span class="op">=</span>[palette[label] <span class="cf">for</span> label <span class="kw">in</span> g.vs[<span class="st">"label"</span>]]</span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a>))</span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> figure(title<span class="op">=</span><span class="st">"Node Embeddings from Word2Vec"</span>, x_axis_label<span class="op">=</span><span class="st">"X"</span>, y_axis_label<span class="op">=</span><span class="st">"Y"</span>)</span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a>p.scatter(<span class="st">'x'</span>, <span class="st">'y'</span>, size<span class="op">=</span><span class="st">'size'</span>, source<span class="op">=</span>source, line_color<span class="op">=</span><span class="st">"black"</span>, color<span class="op">=</span><span class="st">"community"</span>)</span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a>show(p)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="step-4-clustering" class="level5">
<h5 class="anchored" data-anchor-id="step-4-clustering">Step 4: Clustering</h5>
<p>One of the interesting applications with node embeddings is clustering. While we have good community detection methods, like the modularity maximization and stochastic block model, we can use clustering methods from machine learning, such as <span class="math inline">K</span>-means and Gaussian mixture model. Let’s see what we can get from the node embeddings.</p>
<div id="21d14d09" class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> KMeans</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> silhouette_score</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Determine the optimal number of clusters using the silhouette score</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> Kmeans_with_silhouette(embedding, n_clusters_range<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">10</span>)):</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>    silhouette_scores <span class="op">=</span> []</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Iterate over a range of cluster numbers from 2 to 9</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> n_clusters <span class="kw">in</span> <span class="bu">range</span>(<span class="op">*</span>n_clusters_range):</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create a KMeans object with the current number of clusters</span></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>        kmeans <span class="op">=</span> KMeans(n_clusters<span class="op">=</span>n_clusters)</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Fit the KMeans model to the embedding data</span></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>        kmeans.fit(embedding)</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate the silhouette score for the current clustering</span></span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>        score <span class="op">=</span> silhouette_score(embedding, kmeans.labels_)</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Append the number of clusters and its corresponding silhouette score to the list</span></span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>        silhouette_scores.append((n_clusters, score))</span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Find the number of clusters that has the highest silhouette score</span></span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>    optimal_n_clusters <span class="op">=</span> <span class="bu">max</span>(silhouette_scores, key<span class="op">=</span><span class="kw">lambda</span> x: x[<span class="dv">1</span>])[<span class="dv">0</span>]</span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create a KMeans object with the optimal number of clusters</span></span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>    kmeans <span class="op">=</span> KMeans(n_clusters<span class="op">=</span>optimal_n_clusters)</span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Fit the KMeans model to the embedding data with the optimal number of clusters</span></span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a>    kmeans.fit(embedding)</span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Return the labels (cluster assignments) for each data point</span></span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> kmeans.labels_</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="a1cbc35c" class="cell">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> Kmeans_with_silhouette(embedding)</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>cmap <span class="op">=</span> sns.color_palette().as_hex()</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>igraph.plot(g, vertex_color<span class="op">=</span>[cmap[label] <span class="cf">for</span> label <span class="kw">in</span> labels], bbox<span class="op">=</span>(<span class="dv">500</span>, <span class="dv">500</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="node2vec" class="level3">
<h3 class="anchored" data-anchor-id="node2vec">node2vec</h3>
<p>node2vec is a sibling of DeepWalk proposed by {footcite}<code>grover2016node2vec</code>. Both use word2vec trained on random walks on networks. So, it appears that they are very similar. However, the following two components make them very different.</p>
<ul>
<li><p><strong>Biased random walk</strong>: node2vec uses biased random walks that can move in different directions. The bias walk is parameterized by two parameters, <span class="math inline">p</span> and <span class="math inline">q</span>:</p>
<p><span class="math display">
  P(v_{t+1} = x | v_t = v, v_{t-1} = t) \propto
  \begin{cases}
  \frac{1}{p} &amp; \text{if } d(v,t) = 0 \\
  1 &amp; \text{if } d(v,t) = 1 \\
  \frac{1}{q} &amp; \text{if } d(v,t) = 2 \\
  \end{cases}
  </span></p>
<p>where <span class="math inline">d(v,x)</span> is the shortest path distance between node <span class="math inline">v</span> and <span class="math inline">x</span>. A smaller <span class="math inline">p</span> leads to more biased towards the previous node, <span class="math inline">v_{t-1} = t</span>. A smaller <span class="math inline">q</span> leads to more biased towards the nodes that are further away from the previous node, <span class="math inline">v_{t-1} = t</span>.</p>
<p>By adjusting the parameters <span class="math inline">p</span> and <span class="math inline">q</span>, we can influence the random walk to behave more like either breadth-first sampling (BFS) or depth-first sampling (DFS).</p>
<ul>
<li><p><strong>Breadth-First Sampling (BFS)</strong>: This type of sampling explores all the neighbors of a node before moving on to the next level of neighbors. It is useful for capturing community structures within the graph. When we set the parameters to favor BFS, the resulting embeddings will reflect these community structures.</p></li>
<li><p><strong>Depth-First Sampling (DFS)</strong>: This type of sampling goes deep into the graph, exploring as far as possible along each branch before backtracking. It is useful for capturing structural equivalence, where nodes that have similar roles in the graph (even if they are not directly connected) are represented similarly. When we set the parameters to favor DFS, the resulting embeddings will reflect these structural equivalences.</p></li>
</ul>
<p><img src="https://www.researchgate.net/publication/354654762/figure/fig3/AS:1069013035655173@1631883977008/A-biased-random-walk-procedure-of-node2vec-B-BFS-and-DFS-search-strategies-from-node-u.png" class="img-fluid"></p>
<p>The embeddings generated by node2vec can capture different aspects of the graph depending on the sampling strategy used. With BFS, we capture community structures, and with DFS, we capture structural equivalence.</p>
<p><img src="https://miro.medium.com/v2/resize:fit:1138/format:webp/1*nCyF5jFSU5uJVdAPdf-0HA.png" class="img-fluid"></p></li>
<li><p><strong>Negative sampling</strong>: node2vec uses negative sampling, instead of hierarchical softmax. This difference appears to be minor, but it has significant consequences on the characteristics of the embeddings. This is beyond the scope of this lecture, but you can refer to {footcite}<code>kojaku2021neurips</code> and {footcite}<code>dyer2014notes</code> for more details.</p></li>
</ul>
</section>
<section id="exercise-02-implement-node2vec" class="level3">
<h3 class="anchored" data-anchor-id="exercise-02-implement-node2vec">Exercise 02: Implement node2vec</h3>
<p>Let’s implement the biased random walk for node2vec</p>
<div id="28905943" class="cell">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> node2vec_random_walk(net, start_node, walk_length, p, q):</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Sample a random walk starting from start_node.</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize the walk with the start_node</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>    walk <span class="op">=</span> [start_node]</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Continue the walk until it reaches the desired length</span></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="bu">len</span>(walk) <span class="op">&lt;</span> walk_length:</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Get the current node in the walk</span></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>        cur <span class="op">=</span> walk[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Get the neighbors of the current node</span></span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>        cur_nbrs <span class="op">=</span> <span class="bu">list</span>(net[cur].indices)</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Check if the current node has any neighbors</span></span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">len</span>(cur_nbrs) <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>            <span class="co"># If the walk has just started, randomly choose the next node from the neighbors</span></span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">len</span>(walk) <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>                walk.append(np.random.choice(cur_nbrs))</span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Get the previous node in the walk</span></span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a>                prev <span class="op">=</span> walk[<span class="op">-</span><span class="dv">2</span>]</span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Use the alias sampling method to choose the next node based on the bias parameters p and q</span></span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a>                next_node <span class="op">=</span> alias_sample(net, cur_nbrs, prev, p, q)</span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Append the chosen next node to the walk</span></span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a>                walk.append(next_node)</span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true" tabindex="-1"></a>            <span class="co"># If the current node has no neighbors, terminate the walk</span></span>
<span id="cb21-28"><a href="#cb21-28" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb21-29"><a href="#cb21-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-30"><a href="#cb21-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> walk</span>
<span id="cb21-31"><a href="#cb21-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-32"><a href="#cb21-32" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> alias_sample(net, neighbors, prev, p, q):</span>
<span id="cb21-33"><a href="#cb21-33" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb21-34"><a href="#cb21-34" aria-hidden="true" tabindex="-1"></a><span class="co">    Helper function to sample the next node in the walk.</span></span>
<span id="cb21-35"><a href="#cb21-35" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb21-36"><a href="#cb21-36" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Implement the logic to sample the next node based on the bias parameters p and q</span></span>
<span id="cb21-37"><a href="#cb21-37" aria-hidden="true" tabindex="-1"></a>    <span class="co"># You can use the formula provided in the instructions to calculate the probabilities</span></span>
<span id="cb21-38"><a href="#cb21-38" aria-hidden="true" tabindex="-1"></a>    <span class="co"># and then sample the next node accordingly.</span></span>
<span id="cb21-39"><a href="#cb21-39" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize an empty list to store the unnormalized probabilities for each neighbor</span></span>
<span id="cb21-40"><a href="#cb21-40" aria-hidden="true" tabindex="-1"></a>    unnormalized_probs <span class="op">=</span> []</span>
<span id="cb21-41"><a href="#cb21-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-42"><a href="#cb21-42" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Iterate over each neighbor of the current node</span></span>
<span id="cb21-43"><a href="#cb21-43" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> neighbor <span class="kw">in</span> neighbors:</span>
<span id="cb21-44"><a href="#cb21-44" aria-hidden="true" tabindex="-1"></a>        <span class="co"># If the neighbor is the same as the previous node in the walk</span></span>
<span id="cb21-45"><a href="#cb21-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> neighbor <span class="op">==</span> prev:</span>
<span id="cb21-46"><a href="#cb21-46" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Append the probability 1/p to the unnormalized probabilities list</span></span>
<span id="cb21-47"><a href="#cb21-47" aria-hidden="true" tabindex="-1"></a>            unnormalized_probs.append(<span class="dv">1</span> <span class="op">/</span> p)</span>
<span id="cb21-48"><a href="#cb21-48" aria-hidden="true" tabindex="-1"></a>        <span class="co"># If the neighbor is connected to the previous node in the walk</span></span>
<span id="cb21-49"><a href="#cb21-49" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> neighbor <span class="kw">in</span> net[prev].indices:</span>
<span id="cb21-50"><a href="#cb21-50" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Append the probability 1 to the unnormalized probabilities list</span></span>
<span id="cb21-51"><a href="#cb21-51" aria-hidden="true" tabindex="-1"></a>            unnormalized_probs.append(<span class="dv">1</span>)</span>
<span id="cb21-52"><a href="#cb21-52" aria-hidden="true" tabindex="-1"></a>        <span class="co"># If the neighbor is not connected to the previous node in the walk</span></span>
<span id="cb21-53"><a href="#cb21-53" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb21-54"><a href="#cb21-54" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Append the probability 1/q to the unnormalized probabilities list</span></span>
<span id="cb21-55"><a href="#cb21-55" aria-hidden="true" tabindex="-1"></a>            unnormalized_probs.append(<span class="dv">1</span> <span class="op">/</span> q)</span>
<span id="cb21-56"><a href="#cb21-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-57"><a href="#cb21-57" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate the normalization constant by summing all unnormalized probabilities</span></span>
<span id="cb21-58"><a href="#cb21-58" aria-hidden="true" tabindex="-1"></a>    norm_const <span class="op">=</span> <span class="bu">sum</span>(unnormalized_probs)</span>
<span id="cb21-59"><a href="#cb21-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-60"><a href="#cb21-60" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Normalize the probabilities by dividing each unnormalized probability by the normalization constant</span></span>
<span id="cb21-61"><a href="#cb21-61" aria-hidden="true" tabindex="-1"></a>    normalized_probs <span class="op">=</span> [<span class="bu">float</span>(prob) <span class="op">/</span> norm_const <span class="cf">for</span> prob <span class="kw">in</span> unnormalized_probs]</span>
<span id="cb21-62"><a href="#cb21-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-63"><a href="#cb21-63" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Randomly choose the next node from the neighbors based on the normalized probabilities</span></span>
<span id="cb21-64"><a href="#cb21-64" aria-hidden="true" tabindex="-1"></a>    next_node <span class="op">=</span> np.random.choice(neighbors, size<span class="op">=</span><span class="dv">1</span>, p<span class="op">=</span>normalized_probs)[<span class="dv">0</span>]</span>
<span id="cb21-65"><a href="#cb21-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-66"><a href="#cb21-66" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Return the chosen next node</span></span>
<span id="cb21-67"><a href="#cb21-67" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> next_node</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now, let’s set up the word2vec model for node2vec.</p>
<div id="743c1b43" class="cell">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>walks <span class="op">=</span> []</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>q <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_nodes):</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_walkers_per_node):</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>        walks.append(node2vec_random_walk(A, i, walk_length, p, q))</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Word2Vec(walks, vector_size<span class="op">=</span><span class="dv">32</span>, window<span class="op">=</span><span class="dv">3</span>, min_count<span class="op">=</span><span class="dv">1</span>, sg<span class="op">=</span><span class="dv">1</span>, hs <span class="op">=</span> <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>where <code>hs=0</code> indicates that we are using negative sampling. Notice that we set <code>sg=1</code> and <code>hs=1</code> instead of <code>sg=1</code> and <code>hs=0</code> in DeepWalk. This is because node2vec uses the skip-gram model with negative sampling.</p>
<p>Now, we extract the node embeddings from the word2vec model.</p>
<div id="5faabadb" class="cell">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>embedding <span class="op">=</span> []</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_nodes):</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>    embedding.append(model.wv[i])</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>embedding <span class="op">=</span> np.array(embedding)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s visualize the node embeddings from node2vec.</p>
<div id="88252c46" class="cell">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>:tags: [hide<span class="op">-</span><span class="bu">input</span>]</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>reducer <span class="op">=</span> umap.UMAP(n_components<span class="op">=</span><span class="dv">2</span>, random_state<span class="op">=</span><span class="dv">42</span>, n_neighbors<span class="op">=</span><span class="dv">15</span>, metric<span class="op">=</span><span class="st">"cosine"</span>)</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>xy <span class="op">=</span> reducer.fit_transform(embedding)</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>output_notebook()</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the degree of each node</span></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>degrees <span class="op">=</span> A.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>).A1</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>source <span class="op">=</span> ColumnDataSource(data<span class="op">=</span><span class="bu">dict</span>(</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span>xy[:, <span class="dv">0</span>],</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>    y<span class="op">=</span>xy[:, <span class="dv">1</span>],</span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>    size<span class="op">=</span>np.sqrt(degrees <span class="op">/</span> np.<span class="bu">max</span>(degrees)) <span class="op">*</span> <span class="dv">30</span>,</span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a>    community<span class="op">=</span>[palette[label] <span class="cf">for</span> label <span class="kw">in</span> g.vs[<span class="st">"label"</span>]],</span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a>    name <span class="op">=</span> [<span class="bu">str</span>(i) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_nodes)]</span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a>))</span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> figure(title<span class="op">=</span><span class="st">"Node Embeddings from Word2Vec"</span>, x_axis_label<span class="op">=</span><span class="st">"X"</span>, y_axis_label<span class="op">=</span><span class="st">"Y"</span>)</span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true" tabindex="-1"></a>p.scatter(<span class="st">'x'</span>, <span class="st">'y'</span>, size<span class="op">=</span><span class="st">'size'</span>, source<span class="op">=</span>source, line_color<span class="op">=</span><span class="st">"black"</span>, color<span class="op">=</span><span class="st">"community"</span>)</span>
<span id="cb24-22"><a href="#cb24-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-23"><a href="#cb24-23" aria-hidden="true" tabindex="-1"></a>hover <span class="op">=</span> HoverTool()</span>
<span id="cb24-24"><a href="#cb24-24" aria-hidden="true" tabindex="-1"></a>hover.tooltips <span class="op">=</span> [</span>
<span id="cb24-25"><a href="#cb24-25" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"Name"</span>, <span class="st">"@name"</span>),</span>
<span id="cb24-26"><a href="#cb24-26" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"Community"</span>, <span class="st">"@community"</span>)</span>
<span id="cb24-27"><a href="#cb24-27" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb24-28"><a href="#cb24-28" aria-hidden="true" tabindex="-1"></a>p.add_tools(hover)</span>
<span id="cb24-29"><a href="#cb24-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-30"><a href="#cb24-30" aria-hidden="true" tabindex="-1"></a>show(p)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The results for clustering are as follows:</p>
<div id="e9c389e0" class="cell">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> Kmeans_with_silhouette(embedding)</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>cmap <span class="op">=</span> sns.color_palette().as_hex()</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>igraph.plot(g, vertex_color<span class="op">=</span>[cmap[label] <span class="cf">for</span> label <span class="kw">in</span> labels], bbox<span class="op">=</span>(<span class="dv">500</span>, <span class="dv">500</span>), vertex_label<span class="op">=</span>[<span class="st">"</span><span class="sc">%d</span><span class="st">"</span> <span class="op">%</span>  d <span class="cf">for</span> d <span class="kw">in</span>  np.arange(n_nodes)])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="line" class="level3">
<h3 class="anchored" data-anchor-id="line">LINE</h3>
<p>LINE {footcite}<code>tang2015line</code> is another pioneering work to learn node embeddings by directly optimizing the graph structure. It is equivalent to node2vec with <span class="math inline">p=1</span>, <span class="math inline">q=1</span>, and window size 1.</p>
<pre class="{footbibliography}"><code></code></pre>


<!-- -->

</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/skojaku\.github\.io\/adv-net-sci\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../m08-embedding/01-concepts.html" class="pagination-link" aria-label="Advanced Topics in Network Science">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Advanced Topics in Network Science</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../m08-embedding/03-exercises.html" class="pagination-link" aria-label="Advanced Topics in Network Science">
        <span class="nav-page-text">Advanced Topics in Network Science</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb27" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="fu"># Embedding Methods: Implementation and Practice</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="fu">## Spectral Embedding</span></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a><span class="fu">### Network Compression Approach</span></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>Networks are a high-dimensional discrete data that can be difficult to analyze with traditional machine learning methods that assume continuous and smooth data. Spectral embedding is a technique to embed networks into low-dimensional spaces.</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>Let us approach the spectral embedding from the perspective of network compression.</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>Suppose we have an adjacency matrix $\mathbf{A}$ of a network.</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>The adjacency matrix is a high-dimensional data, i.e., a matrix has size $N \times N$ for a network of $N$ nodes.</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>We want to compress it into a lower-dimensional matrix $\mathbf{U}$ of size $N \times d$ for a user-defined small integer $d &lt; N$.</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>A good $\mathbf{U}$ should preserve the network structure and thus can reconstruct the original data $\mathbf{A}$ as closely as possible.</span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>This leads to the following optimization problem:</span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a>\min_{\mathbf{U}} J(\mathbf{U}),\quad J(\mathbf{U}) = <span class="sc">\|</span> \mathbf{A} - \mathbf{U}\mathbf{U}^\top <span class="sc">\|</span>_F^2</span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a>where:</span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>$\mathbf{U}\mathbf{U}^\top$ is the outer product of $\mathbf{U}$ and represents the reconstructed network.</span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>$<span class="sc">\|</span>\cdot<span class="sc">\|</span>_F$ is the Frobenius norm, which is the sum of the squares of the elements in the matrix.</span>
<span id="cb27-24"><a href="#cb27-24" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>$J(\mathbf{U})$ is the loss function that measures the difference between the original network $\mathbf{A}$ and the reconstructed network $\mathbf{U}\mathbf{U}^\top$.</span>
<span id="cb27-25"><a href="#cb27-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-26"><a href="#cb27-26" aria-hidden="true" tabindex="-1"></a>By minimizing the Frobenius norm with respect to $\mathbf{U}$, we obtain the best low-dimensional embedding of the network.</span>
<span id="cb27-27"><a href="#cb27-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-28"><a href="#cb27-28" aria-hidden="true" tabindex="-1"></a><span class="fu">### An Intuitive Solution</span></span>
<span id="cb27-29"><a href="#cb27-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-30"><a href="#cb27-30" aria-hidden="true" tabindex="-1"></a>Let us first understand the solution intuitively.</span>
<span id="cb27-31"><a href="#cb27-31" aria-hidden="true" tabindex="-1"></a>Consider the spectral decomposition of $\mathbf{A}$:</span>
<span id="cb27-32"><a href="#cb27-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-33"><a href="#cb27-33" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-34"><a href="#cb27-34" aria-hidden="true" tabindex="-1"></a>\mathbf{A} = \sum_{i=1}^N \lambda_i \mathbf{u}_i \mathbf{u}_i^\top</span>
<span id="cb27-35"><a href="#cb27-35" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-36"><a href="#cb27-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-37"><a href="#cb27-37" aria-hidden="true" tabindex="-1"></a>where $\lambda_i$ are weights and $\mathbf{u}_i$ are column vectors. Each term $\lambda_i \mathbf{u}_i \mathbf{u}_i^\top$ is a rank-one matrix that captures a part of the network's structure. The larger the weight $\lambda_i$, the more important that term is in describing the network.</span>
<span id="cb27-38"><a href="#cb27-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-39"><a href="#cb27-39" aria-hidden="true" tabindex="-1"></a>To compress the network, we can select the $d$ terms with the largest weights $\lambda_i$. By combining the corresponding $\mathbf{u}_i$ vectors into a matrix $\mathbf{U}$, we obtain a good low-dimensional embedding of the network.</span>
<span id="cb27-40"><a href="#cb27-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-41"><a href="#cb27-41" aria-hidden="true" tabindex="-1"></a><span class="al">![](../figs/spectral-decomposition.jpg)</span></span>
<span id="cb27-42"><a href="#cb27-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-43"><a href="#cb27-43" aria-hidden="true" tabindex="-1"></a>For a formal proof, please refer to the <span class="co">[</span><span class="ot">Appendix section</span><span class="co">](./04-appendix.md)</span>.</span>
<span id="cb27-44"><a href="#cb27-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-45"><a href="#cb27-45" aria-hidden="true" tabindex="-1"></a><span class="fu">### Example: Spectral Embedding</span></span>
<span id="cb27-46"><a href="#cb27-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-47"><a href="#cb27-47" aria-hidden="true" tabindex="-1"></a>Let us demonstrate the results with a simple example as follows.</span>
<span id="cb27-48"><a href="#cb27-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-51"><a href="#cb27-51" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb27-52"><a href="#cb27-52" aria-hidden="true" tabindex="-1"></a>:tags: [hide<span class="op">-</span><span class="bu">input</span>]</span>
<span id="cb27-53"><a href="#cb27-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-54"><a href="#cb27-54" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb27-55"><a href="#cb27-55" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> networkx <span class="im">as</span> nx</span>
<span id="cb27-56"><a href="#cb27-56" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb27-57"><a href="#cb27-57" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb27-58"><a href="#cb27-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-59"><a href="#cb27-59" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a small example network</span></span>
<span id="cb27-60"><a href="#cb27-60" aria-hidden="true" tabindex="-1"></a>G <span class="op">=</span> nx.karate_club_graph()</span>
<span id="cb27-61"><a href="#cb27-61" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> nx.adjacency_matrix(G).toarray()</span>
<span id="cb27-62"><a href="#cb27-62" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> np.unique([d[<span class="dv">1</span>][<span class="st">'club'</span>] <span class="cf">for</span> d <span class="kw">in</span> G.nodes(data<span class="op">=</span><span class="va">True</span>)], return_inverse<span class="op">=</span><span class="va">True</span>)[<span class="dv">1</span>]</span>
<span id="cb27-63"><a href="#cb27-63" aria-hidden="true" tabindex="-1"></a>cmap <span class="op">=</span> sns.color_palette()</span>
<span id="cb27-64"><a href="#cb27-64" aria-hidden="true" tabindex="-1"></a>nx.draw(G, with_labels<span class="op">=</span><span class="va">False</span>, node_color<span class="op">=</span>[cmap[i] <span class="cf">for</span> i <span class="kw">in</span> labels])</span>
<span id="cb27-65"><a href="#cb27-65" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-66"><a href="#cb27-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-69"><a href="#cb27-69" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb27-70"><a href="#cb27-70" aria-hidden="true" tabindex="-1"></a>:tags: [hide<span class="op">-</span><span class="bu">input</span>]</span>
<span id="cb27-71"><a href="#cb27-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-72"><a href="#cb27-72" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the spectral decomposition</span></span>
<span id="cb27-73"><a href="#cb27-73" aria-hidden="true" tabindex="-1"></a>eigvals, eigvecs <span class="op">=</span> np.linalg.eig(A)</span>
<span id="cb27-74"><a href="#cb27-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-75"><a href="#cb27-75" aria-hidden="true" tabindex="-1"></a><span class="co"># Find the top d eigenvectors</span></span>
<span id="cb27-76"><a href="#cb27-76" aria-hidden="true" tabindex="-1"></a>d <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb27-77"><a href="#cb27-77" aria-hidden="true" tabindex="-1"></a>sorted_indices <span class="op">=</span> np.argsort(eigvals)[::<span class="op">-</span><span class="dv">1</span>][:d]</span>
<span id="cb27-78"><a href="#cb27-78" aria-hidden="true" tabindex="-1"></a>eigvals <span class="op">=</span> eigvals[sorted_indices]</span>
<span id="cb27-79"><a href="#cb27-79" aria-hidden="true" tabindex="-1"></a>eigvecs <span class="op">=</span> eigvecs[:, sorted_indices]</span>
<span id="cb27-80"><a href="#cb27-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-81"><a href="#cb27-81" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the results</span></span>
<span id="cb27-82"><a href="#cb27-82" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb27-83"><a href="#cb27-83" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">5</span>))</span>
<span id="cb27-84"><a href="#cb27-84" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(x <span class="op">=</span> eigvecs[:, <span class="dv">0</span>], y <span class="op">=</span> eigvecs[:, <span class="dv">1</span>], hue<span class="op">=</span>labels, ax<span class="op">=</span>ax)</span>
<span id="cb27-85"><a href="#cb27-85" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Spectral Embedding'</span>)</span>
<span id="cb27-86"><a href="#cb27-86" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Eigenvector 1'</span>)</span>
<span id="cb27-87"><a href="#cb27-87" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Eigenvector 2'</span>)</span>
<span id="cb27-88"><a href="#cb27-88" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb27-89"><a href="#cb27-89" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-90"><a href="#cb27-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-91"><a href="#cb27-91" aria-hidden="true" tabindex="-1"></a>Interestingly, the first eigenvector corresponds to the eigen centrality of the network, representing the centrality of the nodes.</span>
<span id="cb27-92"><a href="#cb27-92" aria-hidden="true" tabindex="-1"></a>The second eigenvector captures the community structure of the network, clearly separating the two communities in the network.</span>
<span id="cb27-93"><a href="#cb27-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-94"><a href="#cb27-94" aria-hidden="true" tabindex="-1"></a><span class="fu">### Modularity Embedding</span></span>
<span id="cb27-95"><a href="#cb27-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-96"><a href="#cb27-96" aria-hidden="true" tabindex="-1"></a>In a similar vein, we can use the modularity matrix to generate a low-dimensional embedding of the network.</span>
<span id="cb27-97"><a href="#cb27-97" aria-hidden="true" tabindex="-1"></a>Namely, let us define the modularity matrix $\mathbf{Q}$ as follows:</span>
<span id="cb27-98"><a href="#cb27-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-99"><a href="#cb27-99" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-100"><a href="#cb27-100" aria-hidden="true" tabindex="-1"></a>Q_{ij} = \frac{1}{2m}A_{ij} - \frac{k_i k_j}{4m^2}</span>
<span id="cb27-101"><a href="#cb27-101" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-102"><a href="#cb27-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-103"><a href="#cb27-103" aria-hidden="true" tabindex="-1"></a>where $k_i$ is the degree of node $i$, and $m$ is the number of edges in the network.</span>
<span id="cb27-104"><a href="#cb27-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-105"><a href="#cb27-105" aria-hidden="true" tabindex="-1"></a>We then compute the eigenvectors of $\mathbf{Q}$ and use them to embed the network into a low-dimensional space just as we did for the adjacency matrix.</span>
<span id="cb27-106"><a href="#cb27-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-109"><a href="#cb27-109" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb27-110"><a href="#cb27-110" aria-hidden="true" tabindex="-1"></a>:tags: [hide<span class="op">-</span><span class="bu">input</span>]</span>
<span id="cb27-111"><a href="#cb27-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-112"><a href="#cb27-112" aria-hidden="true" tabindex="-1"></a>deg <span class="op">=</span> np.<span class="bu">sum</span>(A, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb27-113"><a href="#cb27-113" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> np.<span class="bu">sum</span>(deg) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb27-114"><a href="#cb27-114" aria-hidden="true" tabindex="-1"></a>Q <span class="op">=</span> A <span class="op">-</span> np.outer(deg, deg) <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> m)</span>
<span id="cb27-115"><a href="#cb27-115" aria-hidden="true" tabindex="-1"></a>Q<span class="op">/=</span> <span class="dv">2</span><span class="op">*</span>m</span>
<span id="cb27-116"><a href="#cb27-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-117"><a href="#cb27-117" aria-hidden="true" tabindex="-1"></a>eigvals, eigvecs <span class="op">=</span> np.linalg.eig(Q)</span>
<span id="cb27-118"><a href="#cb27-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-119"><a href="#cb27-119" aria-hidden="true" tabindex="-1"></a><span class="co"># Sort the eigenvalues and eigenvectors</span></span>
<span id="cb27-120"><a href="#cb27-120" aria-hidden="true" tabindex="-1"></a>sorted_indices <span class="op">=</span> np.argsort(<span class="op">-</span>eigvals)[:d]  <span class="co"># Exclude the first eigenvector</span></span>
<span id="cb27-121"><a href="#cb27-121" aria-hidden="true" tabindex="-1"></a>eigvals <span class="op">=</span> eigvals[sorted_indices]</span>
<span id="cb27-122"><a href="#cb27-122" aria-hidden="true" tabindex="-1"></a>eigvecs <span class="op">=</span> eigvecs[:, sorted_indices]</span>
<span id="cb27-123"><a href="#cb27-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-124"><a href="#cb27-124" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">5</span>))</span>
<span id="cb27-125"><a href="#cb27-125" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(x <span class="op">=</span> eigvecs[:, <span class="dv">0</span>], y <span class="op">=</span> eigvecs[:, <span class="dv">1</span>], hue<span class="op">=</span>labels, ax<span class="op">=</span>ax)</span>
<span id="cb27-126"><a href="#cb27-126" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Modularity Embedding'</span>)</span>
<span id="cb27-127"><a href="#cb27-127" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Eigenvector 1'</span>)</span>
<span id="cb27-128"><a href="#cb27-128" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Eigenvector 2'</span>)</span>
<span id="cb27-129"><a href="#cb27-129" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb27-130"><a href="#cb27-130" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-131"><a href="#cb27-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-134"><a href="#cb27-134" aria-hidden="true" tabindex="-1"></a><span class="in">```{note}</span></span>
<span id="cb27-135"><a href="#cb27-135" aria-hidden="true" tabindex="-1"></a><span class="in">The modularity embedding can be used to bipartition the network into two communities using a simple algorithm: group nodes with the same sign of the second eigenvector {footcite}`newman2006modularity`.</span></span>
<span id="cb27-136"><a href="#cb27-136" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-137"><a href="#cb27-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-138"><a href="#cb27-138" aria-hidden="true" tabindex="-1"></a><span class="fu">### Laplacian Eigenmap</span></span>
<span id="cb27-139"><a href="#cb27-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-140"><a href="#cb27-140" aria-hidden="true" tabindex="-1"></a>Laplacian Eigenmap {footcite}<span class="in">`belkin2003laplacian`</span> is another approach to compress a network into a low-dimensional space. The fundamental idea behind this method is to position connected nodes close to each other in the low-dimensional space. This approach leads to the following optimization problem:</span>
<span id="cb27-141"><a href="#cb27-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-142"><a href="#cb27-142" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-143"><a href="#cb27-143" aria-hidden="true" tabindex="-1"></a>\min_{\mathbf{U}} J_{LE}(\mathbf{U}),\quad J_{LE}(\mathbf{U}) = \frac{1}{2}\sum_{i,j} A_{ij} <span class="sc">\|</span> u_i - u_j <span class="sc">\|</span>^2</span>
<span id="cb27-144"><a href="#cb27-144" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-145"><a href="#cb27-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-146"><a href="#cb27-146" aria-hidden="true" tabindex="-1"></a>In this equation, $<span class="sc">\|</span> u_i - u_j <span class="sc">\|</span>^2$ represents the squared distance between nodes $i$ and $j$ in the low-dimensional space. The goal is to minimize this distance for connected nodes (where $A_{ij} = 1$). The factor $\frac{1}{2}$ is included for mathematical convenience in later calculations.</span>
<span id="cb27-147"><a href="#cb27-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-148"><a href="#cb27-148" aria-hidden="true" tabindex="-1"></a>To solve this optimization problem, we rewrite $J_{LE}(\mathbf{U})$ as follows:</span>
<span id="cb27-149"><a href="#cb27-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-150"><a href="#cb27-150" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-151"><a href="#cb27-151" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb27-152"><a href="#cb27-152" aria-hidden="true" tabindex="-1"></a>J_{LE}(\mathbf{U}) &amp;= \frac{1}{2}\sum_{i}\sum_{j} A_{ij} <span class="sc">\|</span> u_i - u_j <span class="sc">\|</span>^2 <span class="sc">\\</span></span>
<span id="cb27-153"><a href="#cb27-153" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{1}{2}\sum_{i}\sum_{j} A_{ij} \left( <span class="sc">\|</span> u_i <span class="sc">\|</span>^2 - 2 u_i^\top u_j + <span class="sc">\|</span> u_j <span class="sc">\|</span>^2 \right) <span class="sc">\\</span></span>
<span id="cb27-154"><a href="#cb27-154" aria-hidden="true" tabindex="-1"></a>&amp;= \sum_{i}\sum_{j} A_{ij} <span class="sc">\|</span> u_i <span class="sc">\|</span>^2 - \sum_{i}\sum_{j} A_{ij} u_i^\top u_j<span class="sc">\\</span></span>
<span id="cb27-155"><a href="#cb27-155" aria-hidden="true" tabindex="-1"></a>&amp;= \sum_{i} k_i <span class="sc">\|</span> u_i <span class="sc">\|</span>^2 - \sum_{i,j} A_{ij} u_i^\top u_j<span class="sc">\\</span></span>
<span id="cb27-156"><a href="#cb27-156" aria-hidden="true" tabindex="-1"></a>&amp;= \sum_{i,j} L_{ij} u_i^\top u_j</span>
<span id="cb27-157"><a href="#cb27-157" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb27-158"><a href="#cb27-158" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-159"><a href="#cb27-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-160"><a href="#cb27-160" aria-hidden="true" tabindex="-1"></a>where</span>
<span id="cb27-161"><a href="#cb27-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-162"><a href="#cb27-162" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-163"><a href="#cb27-163" aria-hidden="true" tabindex="-1"></a>L_{ij} = \begin{cases}</span>
<span id="cb27-164"><a href="#cb27-164" aria-hidden="true" tabindex="-1"></a>k_i &amp; \text{if } i = j <span class="sc">\\</span></span>
<span id="cb27-165"><a href="#cb27-165" aria-hidden="true" tabindex="-1"></a>-A_{ij} &amp; \text{if } i \neq j</span>
<span id="cb27-166"><a href="#cb27-166" aria-hidden="true" tabindex="-1"></a>\end{cases}</span>
<span id="cb27-167"><a href="#cb27-167" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-168"><a href="#cb27-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-169"><a href="#cb27-169" aria-hidden="true" tabindex="-1"></a>The minimization problem can be rewritten as:</span>
<span id="cb27-170"><a href="#cb27-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-171"><a href="#cb27-171" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-172"><a href="#cb27-172" aria-hidden="true" tabindex="-1"></a>J_{LE}(\mathbf{U}) = \text{Tr}(\mathbf{U}^\top \mathbf{L} \mathbf{U})</span>
<span id="cb27-173"><a href="#cb27-173" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-174"><a href="#cb27-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-175"><a href="#cb27-175" aria-hidden="true" tabindex="-1"></a>where</span>
<span id="cb27-176"><a href="#cb27-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-177"><a href="#cb27-177" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-178"><a href="#cb27-178" aria-hidden="true" tabindex="-1"></a>\mathbf{U} =</span>
<span id="cb27-179"><a href="#cb27-179" aria-hidden="true" tabindex="-1"></a>\begin{bmatrix}</span>
<span id="cb27-180"><a href="#cb27-180" aria-hidden="true" tabindex="-1"></a>\mathbf{u}_1 ^\top <span class="sc">\\</span></span>
<span id="cb27-181"><a href="#cb27-181" aria-hidden="true" tabindex="-1"></a>\mathbf{u}_2 ^\top <span class="sc">\\</span></span>
<span id="cb27-182"><a href="#cb27-182" aria-hidden="true" tabindex="-1"></a>\vdots <span class="sc">\\</span></span>
<span id="cb27-183"><a href="#cb27-183" aria-hidden="true" tabindex="-1"></a>\mathbf{u}_N ^\top <span class="sc">\\</span></span>
<span id="cb27-184"><a href="#cb27-184" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}</span>
<span id="cb27-185"><a href="#cb27-185" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-186"><a href="#cb27-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-187"><a href="#cb27-187" aria-hidden="true" tabindex="-1"></a>See the <span class="co">[</span><span class="ot">Appendix section</span><span class="co">](./04-appendix.md)</span> for the detailed derivation.</span>
<span id="cb27-188"><a href="#cb27-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-189"><a href="#cb27-189" aria-hidden="true" tabindex="-1"></a>By taking the derivative of $J_{LE}(\mathbf{U})$ with respect to $\mathbf{U}$ and set it to zero, we obtain the following equation:</span>
<span id="cb27-190"><a href="#cb27-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-191"><a href="#cb27-191" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-192"><a href="#cb27-192" aria-hidden="true" tabindex="-1"></a>\frac{\partial J_{LE}}{\partial \mathbf{U}} = 0 \implies \mathbf{L} \mathbf{U} = \lambda \mathbf{U}</span>
<span id="cb27-193"><a href="#cb27-193" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-194"><a href="#cb27-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-195"><a href="#cb27-195" aria-hidden="true" tabindex="-1"></a>The solution is the $d$ eigenvectors associated with the $d$ smallest eigenvalues of $\mathbf{L}$.</span>
<span id="cb27-196"><a href="#cb27-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-197"><a href="#cb27-197" aria-hidden="true" tabindex="-1"></a>It is important to note that the eigenvector corresponding to the smallest eigenvalue (which is always zero for connected graphs) is trivial - it's the all-one vector. Therefore, in practice, we typically compute the $d+1$ smallest eigenvectors and discard the one corresponding to the zero eigenvalue.</span>
<span id="cb27-198"><a href="#cb27-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-199"><a href="#cb27-199" aria-hidden="true" tabindex="-1"></a><span class="fu">### Example: Laplacian Eigenmap</span></span>
<span id="cb27-200"><a href="#cb27-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-201"><a href="#cb27-201" aria-hidden="true" tabindex="-1"></a>Let us first compute the Laplacian matrix and its eigenvectors.</span>
<span id="cb27-204"><a href="#cb27-204" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb27-205"><a href="#cb27-205" aria-hidden="true" tabindex="-1"></a>:tags: [hide<span class="op">-</span><span class="bu">input</span>]</span>
<span id="cb27-206"><a href="#cb27-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-207"><a href="#cb27-207" aria-hidden="true" tabindex="-1"></a>D <span class="op">=</span> np.diag(np.<span class="bu">sum</span>(A, axis<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb27-208"><a href="#cb27-208" aria-hidden="true" tabindex="-1"></a>L <span class="op">=</span> D <span class="op">-</span> A</span>
<span id="cb27-209"><a href="#cb27-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-210"><a href="#cb27-210" aria-hidden="true" tabindex="-1"></a>eigvals, eigvecs <span class="op">=</span> np.linalg.eig(L)</span>
<span id="cb27-211"><a href="#cb27-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-212"><a href="#cb27-212" aria-hidden="true" tabindex="-1"></a><span class="co"># Sort the eigenvalues and eigenvectors</span></span>
<span id="cb27-213"><a href="#cb27-213" aria-hidden="true" tabindex="-1"></a>sorted_indices <span class="op">=</span> np.argsort(eigvals)[<span class="dv">1</span>:d<span class="op">+</span><span class="dv">1</span>]  <span class="co"># Exclude the first eigenvector</span></span>
<span id="cb27-214"><a href="#cb27-214" aria-hidden="true" tabindex="-1"></a>eigvals <span class="op">=</span> eigvals[sorted_indices]</span>
<span id="cb27-215"><a href="#cb27-215" aria-hidden="true" tabindex="-1"></a>eigvecs <span class="op">=</span> eigvecs[:, sorted_indices]</span>
<span id="cb27-216"><a href="#cb27-216" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-217"><a href="#cb27-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-218"><a href="#cb27-218" aria-hidden="true" tabindex="-1"></a>The eigenvectors corresponding to the $d$ smallest eigenvalues are:</span>
<span id="cb27-221"><a href="#cb27-221" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb27-222"><a href="#cb27-222" aria-hidden="true" tabindex="-1"></a>:tags: [hide<span class="op">-</span><span class="bu">input</span>]</span>
<span id="cb27-223"><a href="#cb27-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-224"><a href="#cb27-224" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">5</span>))</span>
<span id="cb27-225"><a href="#cb27-225" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(x <span class="op">=</span> eigvecs[:, <span class="dv">0</span>], y <span class="op">=</span> eigvecs[:, <span class="dv">1</span>], hue<span class="op">=</span>labels, ax<span class="op">=</span>ax)</span>
<span id="cb27-226"><a href="#cb27-226" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Laplacian Eigenmap'</span>)</span>
<span id="cb27-227"><a href="#cb27-227" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Eigenvector 2'</span>)</span>
<span id="cb27-228"><a href="#cb27-228" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Eigenvector 3'</span>)</span>
<span id="cb27-229"><a href="#cb27-229" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb27-230"><a href="#cb27-230" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-231"><a href="#cb27-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-232"><a href="#cb27-232" aria-hidden="true" tabindex="-1"></a><span class="fu">## Neural Embedding with word2vec</span></span>
<span id="cb27-233"><a href="#cb27-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-234"><a href="#cb27-234" aria-hidden="true" tabindex="-1"></a><span class="fu">### Introduction to word2vec</span></span>
<span id="cb27-235"><a href="#cb27-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-236"><a href="#cb27-236" aria-hidden="true" tabindex="-1"></a>In this section, we will introduce *word2vec*, a powerful technique for learning word embeddings. word2vec is a neural network model that learns words embeddings in a continuous vector space. It was introduced by Tomas Mikolov and his colleagues at Google in 2013 {footcite}<span class="in">`mikolov2013distributed`</span>.</span>
<span id="cb27-237"><a href="#cb27-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-238"><a href="#cb27-238" aria-hidden="true" tabindex="-1"></a><span class="fu">### How it works</span></span>
<span id="cb27-239"><a href="#cb27-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-240"><a href="#cb27-240" aria-hidden="true" tabindex="-1"></a>"You shall know a word by the company it keeps" {footcite}<span class="in">`church1988word`</span> is a famous quote in linguistics. It means that you can understand the meaning of a word by looking at the words that appear in the same context.</span>
<span id="cb27-241"><a href="#cb27-241" aria-hidden="true" tabindex="-1"></a>word2vec operates on the same principle.</span>
<span id="cb27-242"><a href="#cb27-242" aria-hidden="true" tabindex="-1"></a>word2vec identifies a word's context by examining the words within a fixed window around it. For example, in the sentence:</span>
<span id="cb27-243"><a href="#cb27-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-244"><a href="#cb27-244" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; The quick brown fox jumps over a lazy dog</span></span>
<span id="cb27-245"><a href="#cb27-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-246"><a href="#cb27-246" aria-hidden="true" tabindex="-1"></a>The context of the word *fox* includes *quick*, *brown*, *jumps*, *over*, and *lazy*. word2vec is trained to predict which words are likely to appear as the context of an input word.</span>
<span id="cb27-247"><a href="#cb27-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-250"><a href="#cb27-250" aria-hidden="true" tabindex="-1"></a><span class="in">```{note}</span></span>
<span id="cb27-251"><a href="#cb27-251" aria-hidden="true" tabindex="-1"></a><span class="in">There are two main architectures for word2vec:</span></span>
<span id="cb27-252"><a href="#cb27-252" aria-hidden="true" tabindex="-1"></a><span class="in">1. **Continuous Bag of Words (CBOW)**: Predicts the target word (center word) from the context words (surrounding words).</span></span>
<span id="cb27-253"><a href="#cb27-253" aria-hidden="true" tabindex="-1"></a><span class="in">2. **Skip-gram**: Predicts the context words (surrounding words) from the target word (center word).</span></span>
<span id="cb27-254"><a href="#cb27-254" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-255"><a href="#cb27-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-256"><a href="#cb27-256" aria-hidden="true" tabindex="-1"></a>So how are word embeddings learned? word2vec is a neural network model that looks like a bow tie. It has two layers of the vocabulary size coupled with a much smaller hidden layer.</span>
<span id="cb27-257"><a href="#cb27-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-258"><a href="#cb27-258" aria-hidden="true" tabindex="-1"></a><span class="al">![](../figs/word2vec.png)</span></span>
<span id="cb27-259"><a href="#cb27-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-260"><a href="#cb27-260" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Input layer**: The input layer consists of $N$ neurons, where $N$ is the size of the vocabulary (i.e., the number of unique words in the corpus). Each neuron corresponds to a unique word in the vocabulary. When a word is inputted, its corresponding neuron is activated and the other neurons are inhibited. Thus, the input layer is essentially a lookup mechanism that transforms the input word into a corresponding one-hot vector.</span>
<span id="cb27-261"><a href="#cb27-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-262"><a href="#cb27-262" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Output layer**: The output layer also consists of $N$ neurons, each corresponding to a unique word in the vocabulary. Unlike the input layer, multiple neurons can be activated for a single input. The strength of the activation of each neuron (with a normalization by the softmax function) represents the probability of the corresponding word being the input word's context.</span>
<span id="cb27-263"><a href="#cb27-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-264"><a href="#cb27-264" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Hidden layer**: The hidden layer is much smaller than the input and output layers. Multiple neurons in the hidden layer can be activated for a single input, and this activation pattern represents the word's *embedding*.</span>
<span id="cb27-265"><a href="#cb27-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-266"><a href="#cb27-266" aria-hidden="true" tabindex="-1"></a>We can consider word2vec as a *dimensionality reduction* technique that reduces the dimensionality of the input layer to the hidden layer based on the co-occurrence of words within a short distance. The distance is named the *window size*, which is a user-defined hyperparameter.</span>
<span id="cb27-267"><a href="#cb27-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-268"><a href="#cb27-268" aria-hidden="true" tabindex="-1"></a><span class="fu">### What's special about word2vec?</span></span>
<span id="cb27-269"><a href="#cb27-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-270"><a href="#cb27-270" aria-hidden="true" tabindex="-1"></a>With word2vec, words are represented as dense vectors, enabling us to explore their relationships using simple linear algebra. This is in contrast to traditional natural language processing (NLP) methods, such as bag-of-words and topic modeling, which represent words as discrete units or high-dimensional vectors.</span>
<span id="cb27-271"><a href="#cb27-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-272"><a href="#cb27-272" aria-hidden="true" tabindex="-1"></a><span class="al">![](https://miro.medium.com/v2/resize:fit:678/1*5F4TXdFYwqi-BWTToQPIfg.jpeg)</span></span>
<span id="cb27-273"><a href="#cb27-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-274"><a href="#cb27-274" aria-hidden="true" tabindex="-1"></a>To showcase the effectiveness of word2vec, let's walk through an example using the <span class="in">`gensim`</span> library.</span>
<span id="cb27-275"><a href="#cb27-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-278"><a href="#cb27-278" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb27-279"><a href="#cb27-279" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gensim</span>
<span id="cb27-280"><a href="#cb27-280" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gensim.downloader</span>
<span id="cb27-281"><a href="#cb27-281" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> Word2Vec</span>
<span id="cb27-282"><a href="#cb27-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-283"><a href="#cb27-283" aria-hidden="true" tabindex="-1"></a><span class="co"># Load pre-trained word2vec model from Google News</span></span>
<span id="cb27-284"><a href="#cb27-284" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> gensim.downloader.load(<span class="st">'word2vec-google-news-300'</span>)</span>
<span id="cb27-285"><a href="#cb27-285" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-286"><a href="#cb27-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-287"><a href="#cb27-287" aria-hidden="true" tabindex="-1"></a>Our first example is to find the words most similar to *king*.</span>
<span id="cb27-288"><a href="#cb27-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-291"><a href="#cb27-291" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb27-292"><a href="#cb27-292" aria-hidden="true" tabindex="-1"></a><span class="co"># Example usage</span></span>
<span id="cb27-293"><a href="#cb27-293" aria-hidden="true" tabindex="-1"></a>word <span class="op">=</span> <span class="st">"king"</span></span>
<span id="cb27-294"><a href="#cb27-294" aria-hidden="true" tabindex="-1"></a>similar_words <span class="op">=</span> model.most_similar(word)</span>
<span id="cb27-295"><a href="#cb27-295" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Words most similar to '</span><span class="sc">{</span>word<span class="sc">}</span><span class="ss">':"</span>)</span>
<span id="cb27-296"><a href="#cb27-296" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> similar_word, similarity <span class="kw">in</span> similar_words:</span>
<span id="cb27-297"><a href="#cb27-297" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>similar_word<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>similarity<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb27-298"><a href="#cb27-298" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-299"><a href="#cb27-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-300"><a href="#cb27-300" aria-hidden="true" tabindex="-1"></a>A cool (yet controversial) application of word embeddings is analogy solving. Let us consider the following puzzle:</span>
<span id="cb27-301"><a href="#cb27-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-302"><a href="#cb27-302" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; *man* is to *woman* as *king* is to ___ ?</span></span>
<span id="cb27-303"><a href="#cb27-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-304"><a href="#cb27-304" aria-hidden="true" tabindex="-1"></a>We can use word embeddings to solve this puzzle.</span>
<span id="cb27-305"><a href="#cb27-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-308"><a href="#cb27-308" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb27-309"><a href="#cb27-309" aria-hidden="true" tabindex="-1"></a><span class="co"># We solve the puzzle by</span></span>
<span id="cb27-310"><a href="#cb27-310" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb27-311"><a href="#cb27-311" aria-hidden="true" tabindex="-1"></a><span class="co">#  vec(king) - vec(man) + vec(woman)</span></span>
<span id="cb27-312"><a href="#cb27-312" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb27-313"><a href="#cb27-313" aria-hidden="true" tabindex="-1"></a><span class="co"># To solve this, we use the model.most_similar function, with positive words being "king" and "woman" (additive), and negative words being "man" (subtractive).</span></span>
<span id="cb27-314"><a href="#cb27-314" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb27-315"><a href="#cb27-315" aria-hidden="true" tabindex="-1"></a>model.most_similar(positive<span class="op">=</span>[<span class="st">'woman'</span>, <span class="st">"king"</span>], negative<span class="op">=</span>[<span class="st">'man'</span>], topn<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb27-316"><a href="#cb27-316" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-317"><a href="#cb27-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-318"><a href="#cb27-318" aria-hidden="true" tabindex="-1"></a>The last example is to visualize the word embeddings.</span>
<span id="cb27-319"><a href="#cb27-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-322"><a href="#cb27-322" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb27-323"><a href="#cb27-323" aria-hidden="true" tabindex="-1"></a>:tags: [hide<span class="op">-</span><span class="bu">input</span>]</span>
<span id="cb27-324"><a href="#cb27-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-325"><a href="#cb27-325" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb27-326"><a href="#cb27-326" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb27-327"><a href="#cb27-327" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb27-328"><a href="#cb27-328" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb27-329"><a href="#cb27-329" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb27-330"><a href="#cb27-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-331"><a href="#cb27-331" aria-hidden="true" tabindex="-1"></a>countries <span class="op">=</span> [<span class="st">'Germany'</span>, <span class="st">'France'</span>, <span class="st">'Italy'</span>, <span class="st">'Spain'</span>, <span class="st">'Portugal'</span>, <span class="st">'Greece'</span>]</span>
<span id="cb27-332"><a href="#cb27-332" aria-hidden="true" tabindex="-1"></a>capital_words <span class="op">=</span> [<span class="st">'Berlin'</span>, <span class="st">'Paris'</span>, <span class="st">'Rome'</span>, <span class="st">'Madrid'</span>, <span class="st">'Lisbon'</span>, <span class="st">'Athens'</span>]</span>
<span id="cb27-333"><a href="#cb27-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-334"><a href="#cb27-334" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the word embeddings for the countries and capitals</span></span>
<span id="cb27-335"><a href="#cb27-335" aria-hidden="true" tabindex="-1"></a>country_embeddings <span class="op">=</span> np.array([model[country] <span class="cf">for</span> country <span class="kw">in</span> countries])</span>
<span id="cb27-336"><a href="#cb27-336" aria-hidden="true" tabindex="-1"></a>capital_embeddings <span class="op">=</span> np.array([model[capital] <span class="cf">for</span> capital <span class="kw">in</span> capital_words])</span>
<span id="cb27-337"><a href="#cb27-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-338"><a href="#cb27-338" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the PCA</span></span>
<span id="cb27-339"><a href="#cb27-339" aria-hidden="true" tabindex="-1"></a>pca <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb27-340"><a href="#cb27-340" aria-hidden="true" tabindex="-1"></a>embeddings <span class="op">=</span> np.vstack([country_embeddings, capital_embeddings])</span>
<span id="cb27-341"><a href="#cb27-341" aria-hidden="true" tabindex="-1"></a>embeddings_pca <span class="op">=</span> pca.fit_transform(embeddings)</span>
<span id="cb27-342"><a href="#cb27-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-343"><a href="#cb27-343" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a DataFrame for seaborn</span></span>
<span id="cb27-344"><a href="#cb27-344" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(embeddings_pca, columns<span class="op">=</span>[<span class="st">'PC1'</span>, <span class="st">'PC2'</span>])</span>
<span id="cb27-345"><a href="#cb27-345" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'Label'</span>] <span class="op">=</span> countries <span class="op">+</span> capital_words</span>
<span id="cb27-346"><a href="#cb27-346" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'Type'</span>] <span class="op">=</span> [<span class="st">'Country'</span>] <span class="op">*</span> <span class="bu">len</span>(countries) <span class="op">+</span> [<span class="st">'Capital'</span>] <span class="op">*</span> <span class="bu">len</span>(capital_words)</span>
<span id="cb27-347"><a href="#cb27-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-348"><a href="#cb27-348" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the data</span></span>
<span id="cb27-349"><a href="#cb27-349" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">10</span>))</span>
<span id="cb27-350"><a href="#cb27-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-351"><a href="#cb27-351" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a scatter plot with seaborn</span></span>
<span id="cb27-352"><a href="#cb27-352" aria-hidden="true" tabindex="-1"></a>scatter_plot <span class="op">=</span> sns.scatterplot(data<span class="op">=</span>df, x<span class="op">=</span><span class="st">'PC1'</span>, y<span class="op">=</span><span class="st">'PC2'</span>, hue<span class="op">=</span><span class="st">'Type'</span>, style<span class="op">=</span><span class="st">'Type'</span>, s<span class="op">=</span><span class="dv">200</span>, palette<span class="op">=</span><span class="st">'deep'</span>, markers<span class="op">=</span>[<span class="st">'o'</span>, <span class="st">'s'</span>])</span>
<span id="cb27-353"><a href="#cb27-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-354"><a href="#cb27-354" aria-hidden="true" tabindex="-1"></a><span class="co"># Annotate the points</span></span>
<span id="cb27-355"><a href="#cb27-355" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(df)):</span>
<span id="cb27-356"><a href="#cb27-356" aria-hidden="true" tabindex="-1"></a>    plt.text(df[<span class="st">'PC1'</span>][i], df[<span class="st">'PC2'</span>][i] <span class="op">+</span> <span class="fl">0.08</span>, df[<span class="st">'Label'</span>][i], fontsize<span class="op">=</span><span class="dv">12</span>, ha<span class="op">=</span><span class="st">'center'</span>, va<span class="op">=</span><span class="st">'bottom'</span>,</span>
<span id="cb27-357"><a href="#cb27-357" aria-hidden="true" tabindex="-1"></a>             bbox<span class="op">=</span><span class="bu">dict</span>(facecolor<span class="op">=</span><span class="st">'white'</span>, edgecolor<span class="op">=</span><span class="st">'none'</span>, alpha<span class="op">=</span><span class="fl">0.8</span>))</span>
<span id="cb27-358"><a href="#cb27-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-359"><a href="#cb27-359" aria-hidden="true" tabindex="-1"></a><span class="co"># Draw arrows between countries and capitals</span></span>
<span id="cb27-360"><a href="#cb27-360" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(countries)):</span>
<span id="cb27-361"><a href="#cb27-361" aria-hidden="true" tabindex="-1"></a>    plt.arrow(df[<span class="st">'PC1'</span>][i], df[<span class="st">'PC2'</span>][i], df[<span class="st">'PC1'</span>][i <span class="op">+</span> <span class="bu">len</span>(countries)] <span class="op">-</span> df[<span class="st">'PC1'</span>][i], df[<span class="st">'PC2'</span>][i <span class="op">+</span> <span class="bu">len</span>(countries)] <span class="op">-</span> df[<span class="st">'PC2'</span>][i],</span>
<span id="cb27-362"><a href="#cb27-362" aria-hidden="true" tabindex="-1"></a>              color<span class="op">=</span><span class="st">'gray'</span>, alpha<span class="op">=</span><span class="fl">0.6</span>, linewidth<span class="op">=</span><span class="fl">1.5</span>, head_width<span class="op">=</span><span class="fl">0.02</span>, head_length<span class="op">=</span><span class="fl">0.03</span>)</span>
<span id="cb27-363"><a href="#cb27-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-364"><a href="#cb27-364" aria-hidden="true" tabindex="-1"></a>plt.legend(title<span class="op">=</span><span class="st">'Type'</span>, title_fontsize<span class="op">=</span><span class="st">'13'</span>, fontsize<span class="op">=</span><span class="st">'11'</span>)</span>
<span id="cb27-365"><a href="#cb27-365" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'PCA of Country and Capital Word Embeddings'</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb27-366"><a href="#cb27-366" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Principal Component 1'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb27-367"><a href="#cb27-367" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Principal Component 2'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb27-368"><a href="#cb27-368" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> plt.gca()</span>
<span id="cb27-369"><a href="#cb27-369" aria-hidden="true" tabindex="-1"></a>ax.set_axis_off()</span>
<span id="cb27-370"><a href="#cb27-370" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-371"><a href="#cb27-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-372"><a href="#cb27-372" aria-hidden="true" tabindex="-1"></a>We can see that word2vec places the words representing countries close to each other and so do the words representing their capitals. The country-capital relationship is also roughly preserved, e.g., *Germany*-*Berlin* vector is roughly parallel to *France*-*Paris* vector.</span>
<span id="cb27-373"><a href="#cb27-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-374"><a href="#cb27-374" aria-hidden="true" tabindex="-1"></a><span class="fu">## Graph Embedding with word2vec</span></span>
<span id="cb27-375"><a href="#cb27-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-376"><a href="#cb27-376" aria-hidden="true" tabindex="-1"></a>How can we apply word2vec to graph data? There is a critical challenge: word2vec takes sequence of words as input, while graph data are discrete and unordered. A solution to fill this gap is *random walk*, which transforms graph data into a sequence of nodes. Once we have a sequence of nodes, we can treat it as a sequence of words and apply word2vec.</span>
<span id="cb27-377"><a href="#cb27-377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-378"><a href="#cb27-378" aria-hidden="true" tabindex="-1"></a><span class="fu">### DeepWalk</span></span>
<span id="cb27-379"><a href="#cb27-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-380"><a href="#cb27-380" aria-hidden="true" tabindex="-1"></a><span class="al">![](https://dt5vp8kor0orz.cloudfront.net/7c56c256b9fbf06693da47737ac57fae803a5a4f/1-Figure1-1.png)</span></span>
<span id="cb27-381"><a href="#cb27-381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-382"><a href="#cb27-382" aria-hidden="true" tabindex="-1"></a>DeepWalk is one of the pioneering works to apply word2vec to graph data {footcite}<span class="in">`perozzi2014deepwalk`</span>. It views the nodes as words and the nodes random walks on the graph as sentences, and applies word2vec to learn the node embeddings.</span>
<span id="cb27-383"><a href="#cb27-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-384"><a href="#cb27-384" aria-hidden="true" tabindex="-1"></a>More specifically, the method contains the following steps:</span>
<span id="cb27-385"><a href="#cb27-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-386"><a href="#cb27-386" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Sample multiple random walks from the graph.</span>
<span id="cb27-387"><a href="#cb27-387" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Treat the random walks as sentences and feed them to word2vev to learn the node embeddings.</span>
<span id="cb27-388"><a href="#cb27-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-389"><a href="#cb27-389" aria-hidden="true" tabindex="-1"></a>There are some technical details that we need to be aware of, which we will learn by implementing DeepWalk in the following exercise.</span>
<span id="cb27-390"><a href="#cb27-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-391"><a href="#cb27-391" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 01: Implement DeepWalk</span></span>
<span id="cb27-392"><a href="#cb27-392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-393"><a href="#cb27-393" aria-hidden="true" tabindex="-1"></a>In this exercise, we implement DeepWalk step by step.</span>
<span id="cb27-394"><a href="#cb27-394" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-395"><a href="#cb27-395" aria-hidden="true" tabindex="-1"></a><span class="fu">##### Step 1: Data preparation</span></span>
<span id="cb27-396"><a href="#cb27-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-397"><a href="#cb27-397" aria-hidden="true" tabindex="-1"></a>We will use the karate club network as an example.</span>
<span id="cb27-398"><a href="#cb27-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-399"><a href="#cb27-399" aria-hidden="true" tabindex="-1"></a>**Load the data**</span>
<span id="cb27-402"><a href="#cb27-402" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb27-403"><a href="#cb27-403" aria-hidden="true" tabindex="-1"></a>:tags: [hide<span class="op">-</span><span class="bu">input</span>]</span>
<span id="cb27-404"><a href="#cb27-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-405"><a href="#cb27-405" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> igraph</span>
<span id="cb27-406"><a href="#cb27-406" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> networkx <span class="im">as</span> nx</span>
<span id="cb27-407"><a href="#cb27-407" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb27-408"><a href="#cb27-408" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb27-409"><a href="#cb27-409" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-410"><a href="#cb27-410" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> igraph.Graph.Famous(<span class="st">"Zachary"</span>)</span>
<span id="cb27-411"><a href="#cb27-411" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> g.get_adjacency_sparse()</span>
<span id="cb27-412"><a href="#cb27-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-413"><a href="#cb27-413" aria-hidden="true" tabindex="-1"></a><span class="co"># Add the community labels to the nodes for visualization</span></span>
<span id="cb27-414"><a href="#cb27-414" aria-hidden="true" tabindex="-1"></a>g.vs[<span class="st">"label"</span>] <span class="op">=</span> np.unique([d[<span class="dv">1</span>][<span class="st">'club'</span>] <span class="cf">for</span> d <span class="kw">in</span> nx.karate_club_graph().nodes(data<span class="op">=</span><span class="va">True</span>)], return_inverse<span class="op">=</span><span class="va">True</span>)[<span class="dv">1</span>]</span>
<span id="cb27-415"><a href="#cb27-415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-416"><a href="#cb27-416" aria-hidden="true" tabindex="-1"></a>palette <span class="op">=</span> sns.color_palette().as_hex()</span>
<span id="cb27-417"><a href="#cb27-417" aria-hidden="true" tabindex="-1"></a>igraph.plot(g, vertex_color<span class="op">=</span>[palette[label] <span class="cf">for</span> label <span class="kw">in</span> g.vs[<span class="st">"label"</span>]], bbox<span class="op">=</span>(<span class="dv">300</span>, <span class="dv">300</span>))</span>
<span id="cb27-418"><a href="#cb27-418" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-419"><a href="#cb27-419" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-420"><a href="#cb27-420" aria-hidden="true" tabindex="-1"></a><span class="fu">##### Step 2: Generate random walks</span></span>
<span id="cb27-421"><a href="#cb27-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-422"><a href="#cb27-422" aria-hidden="true" tabindex="-1"></a>Next, we generate the training data for the word2vec model by generating multiple random walks starting from each node in the network.</span>
<span id="cb27-423"><a href="#cb27-423" aria-hidden="true" tabindex="-1"></a>Let us first implement a function to sample random walks from a given network.</span>
<span id="cb27-424"><a href="#cb27-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-427"><a href="#cb27-427" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb27-428"><a href="#cb27-428" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> random_walk(net, start_node, walk_length):</span>
<span id="cb27-429"><a href="#cb27-429" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize the walk with the starting node</span></span>
<span id="cb27-430"><a href="#cb27-430" aria-hidden="true" tabindex="-1"></a>    walk <span class="op">=</span> [start_node]</span>
<span id="cb27-431"><a href="#cb27-431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-432"><a href="#cb27-432" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Continue the walk until the desired length is reached</span></span>
<span id="cb27-433"><a href="#cb27-433" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="bu">len</span>(walk) <span class="op">&lt;</span> walk_length:</span>
<span id="cb27-434"><a href="#cb27-434" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Get the current node (the last node in the walk)</span></span>
<span id="cb27-435"><a href="#cb27-435" aria-hidden="true" tabindex="-1"></a>        cur <span class="op">=</span> walk[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb27-436"><a href="#cb27-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-437"><a href="#cb27-437" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Get the neighbors of the current node</span></span>
<span id="cb27-438"><a href="#cb27-438" aria-hidden="true" tabindex="-1"></a>        cur_nbrs <span class="op">=</span> <span class="bu">list</span>(net[cur].indices)</span>
<span id="cb27-439"><a href="#cb27-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-440"><a href="#cb27-440" aria-hidden="true" tabindex="-1"></a>        <span class="co"># If the current node has neighbors, randomly choose one and add it to the walk</span></span>
<span id="cb27-441"><a href="#cb27-441" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">len</span>(cur_nbrs) <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb27-442"><a href="#cb27-442" aria-hidden="true" tabindex="-1"></a>            walk.append(np.random.choice(cur_nbrs))</span>
<span id="cb27-443"><a href="#cb27-443" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb27-444"><a href="#cb27-444" aria-hidden="true" tabindex="-1"></a>            <span class="co"># If the current node has no neighbors, terminate the walk</span></span>
<span id="cb27-445"><a href="#cb27-445" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb27-446"><a href="#cb27-446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-447"><a href="#cb27-447" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Return the generated walk</span></span>
<span id="cb27-448"><a href="#cb27-448" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> walk</span>
<span id="cb27-449"><a href="#cb27-449" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-450"><a href="#cb27-450" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-451"><a href="#cb27-451" aria-hidden="true" tabindex="-1"></a>Generate 10 random walks of length 50 starting from each node.</span>
<span id="cb27-452"><a href="#cb27-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-455"><a href="#cb27-455" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb27-456"><a href="#cb27-456" aria-hidden="true" tabindex="-1"></a>n_nodes <span class="op">=</span> g.vcount()</span>
<span id="cb27-457"><a href="#cb27-457" aria-hidden="true" tabindex="-1"></a>n_walkers_per_node <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb27-458"><a href="#cb27-458" aria-hidden="true" tabindex="-1"></a>walk_length <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb27-459"><a href="#cb27-459" aria-hidden="true" tabindex="-1"></a>walks <span class="op">=</span> []</span>
<span id="cb27-460"><a href="#cb27-460" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_nodes):</span>
<span id="cb27-461"><a href="#cb27-461" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_walkers_per_node):</span>
<span id="cb27-462"><a href="#cb27-462" aria-hidden="true" tabindex="-1"></a>        walks.append(random_walk(A, i, walk_length))</span>
<span id="cb27-463"><a href="#cb27-463" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-464"><a href="#cb27-464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-465"><a href="#cb27-465" aria-hidden="true" tabindex="-1"></a><span class="fu">##### Step 3: Train the word2vec model</span></span>
<span id="cb27-466"><a href="#cb27-466" aria-hidden="true" tabindex="-1"></a>Then, we feed the random walks to the word2vec model.</span>
<span id="cb27-467"><a href="#cb27-467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-470"><a href="#cb27-470" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb27-471"><a href="#cb27-471" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> Word2Vec</span>
<span id="cb27-472"><a href="#cb27-472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-473"><a href="#cb27-473" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Word2Vec(walks, vector_size<span class="op">=</span><span class="dv">32</span>, window<span class="op">=</span><span class="dv">3</span>, min_count<span class="op">=</span><span class="dv">1</span>, sg<span class="op">=</span><span class="dv">1</span>, hs <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb27-474"><a href="#cb27-474" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-475"><a href="#cb27-475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-476"><a href="#cb27-476" aria-hidden="true" tabindex="-1"></a>Here,</span>
<span id="cb27-477"><a href="#cb27-477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-478"><a href="#cb27-478" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="in">`vector_size`</span> is the dimension of the embedding vectors.</span>
<span id="cb27-479"><a href="#cb27-479" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="in">`window`</span> indicates the maximum distance between a word and its context words. For example, in the random walk <span class="in">`[0, 1, 2, 3, 4, 5, 6, 7]`</span>, the context words of node 2 are <span class="in">`[0, 1, 3, 4, 5]`</span> when <span class="in">`window=3`</span>.</span>
<span id="cb27-480"><a href="#cb27-480" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="in">`min_count`</span> is the minimum number of times a word must appear in the training data to be included in the vocabulary.</span>
<span id="cb27-481"><a href="#cb27-481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-482"><a href="#cb27-482" aria-hidden="true" tabindex="-1"></a>Two parameters <span class="in">`sg=1`</span> and <span class="in">`hs=1`</span> indicate that we are using the skip-gram model with negative sampling. Let us understand what they mean in detail as follows.</span>
<span id="cb27-483"><a href="#cb27-483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-484"><a href="#cb27-484" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Skip-gram model**: it trains word2vec by predicting context words given a target word. For example, given the sentence "The quick brown fox jumps over the lazy dog", in the skip-gram model, given the target word "fox", the model will try to predict the context words "quick", "brown", "jumps", and "over". If <span class="in">`sg=0`</span>, the input and output are swapped: the model will predict the target word from the context words, e.g., given the context words "quick", "brown", "jumps", and "over", the model will predict the target word "fox".</span>
<span id="cb27-485"><a href="#cb27-485" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-486"><a href="#cb27-486" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Hierarchical softmax**: To understand hierarchical softmax better, let's break down how the word2vec model works. The goal of word2vec is to predict context words given a target word. For example, if our target word is $w_t$ and our context word is $w_c$, we want to find the probability of $w_c$ given $w_t$. This probability is calculated using the softmax function:</span>
<span id="cb27-487"><a href="#cb27-487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-488"><a href="#cb27-488" aria-hidden="true" tabindex="-1"></a>    $$</span>
<span id="cb27-489"><a href="#cb27-489" aria-hidden="true" tabindex="-1"></a>    P(w_c | w_t) = \frac{\exp(\mathbf{v}_{w_c} \cdot \mathbf{v}_{w_t})}{\sum_{w \in V} \exp(\mathbf{v}_w \cdot \mathbf{u}_{w_t})}</span>
<span id="cb27-490"><a href="#cb27-490" aria-hidden="true" tabindex="-1"></a>   $$</span>
<span id="cb27-491"><a href="#cb27-491" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-492"><a href="#cb27-492" aria-hidden="true" tabindex="-1"></a>    Here, $\mathbf{v}_w$ and $\mathbf{u}_w$ represent the vector for word $w$ as context and target respectively, and $V$ is the entire vocabulary. The tricky part is the denominator, which requires summing over all words in the vocabulary. If we have a large vocabulary, this can be very computationally expensive. Imagine having to compute 100,000 exponentials and their sum for each training example if our vocabulary size is 100,000!</span>
<span id="cb27-493"><a href="#cb27-493" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-494"><a href="#cb27-494" aria-hidden="true" tabindex="-1"></a>    Hierarchical softmax helps us solve this problem. Instead of calculating the probability directly, it organizes the vocabulary into a binary tree, where each word is a leaf node. To find the probability of a word, we calculate the product of probabilities along the path from the root to the leaf node. This method significantly reduces the computational complexity. Instead of being proportional to the vocabulary size, it becomes proportional to the logarithm of the vocabulary size. This makes it much more efficient, especially for large vocabularies.</span>
<span id="cb27-495"><a href="#cb27-495" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-496"><a href="#cb27-496" aria-hidden="true" tabindex="-1"></a>    <span class="al">![](https://lh5.googleusercontent.com/proxy/_omrC8G6quTl2SGarwFe57qzbIs-PtGkEA5yODFE5I0Ny2IHGiJwsUhMrcuUqg5o-R2nD9hkgMuZsQJKoCggP29zXtj-Vz-X8BE)</span></span>
<span id="cb27-497"><a href="#cb27-497" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-498"><a href="#cb27-498" aria-hidden="true" tabindex="-1"></a>By using the skip-gram model with hierarchical softmax, we can efficiently learn high-quality word embeddings even when dealing with large vocabularies.</span>
<span id="cb27-499"><a href="#cb27-499" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-500"><a href="#cb27-500" aria-hidden="true" tabindex="-1"></a>Now, we extract the node embeddings from the word2vec model. In the word2vec model, the embeddings are stored in the <span class="in">`wv`</span> attribute. The embedding of node $i$ is given by <span class="in">`model.wv[i]`</span>.</span>
<span id="cb27-501"><a href="#cb27-501" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-504"><a href="#cb27-504" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb27-505"><a href="#cb27-505" aria-hidden="true" tabindex="-1"></a>embedding <span class="op">=</span> []</span>
<span id="cb27-506"><a href="#cb27-506" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_nodes):</span>
<span id="cb27-507"><a href="#cb27-507" aria-hidden="true" tabindex="-1"></a>    embedding.append(model.wv[i])</span>
<span id="cb27-508"><a href="#cb27-508" aria-hidden="true" tabindex="-1"></a>embedding <span class="op">=</span> np.array(embedding)</span>
<span id="cb27-509"><a href="#cb27-509" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-510"><a href="#cb27-510" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-511"><a href="#cb27-511" aria-hidden="true" tabindex="-1"></a><span class="in">`embedding`</span> is the matrix of node embeddings. It has the same number of rows as the number of nodes in the network, and the number of columns is the embedding dimension.</span>
<span id="cb27-512"><a href="#cb27-512" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-513"><a href="#cb27-513" aria-hidden="true" tabindex="-1"></a>**Print the first 3 nodes**</span>
<span id="cb27-516"><a href="#cb27-516" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb27-517"><a href="#cb27-517" aria-hidden="true" tabindex="-1"></a>:tags: [hide<span class="op">-</span><span class="bu">input</span>]</span>
<span id="cb27-518"><a href="#cb27-518" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-519"><a href="#cb27-519" aria-hidden="true" tabindex="-1"></a>embedding[:<span class="dv">3</span>]</span>
<span id="cb27-520"><a href="#cb27-520" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-521"><a href="#cb27-521" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-522"><a href="#cb27-522" aria-hidden="true" tabindex="-1"></a>Let's visualize the node embeddings using UMAP.</span>
<span id="cb27-523"><a href="#cb27-523" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-526"><a href="#cb27-526" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb27-527"><a href="#cb27-527" aria-hidden="true" tabindex="-1"></a>:tags: [hide<span class="op">-</span><span class="bu">input</span>]</span>
<span id="cb27-528"><a href="#cb27-528" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> umap</span>
<span id="cb27-529"><a href="#cb27-529" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> bokeh.plotting <span class="im">import</span> figure, show</span>
<span id="cb27-530"><a href="#cb27-530" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> bokeh.io <span class="im">import</span> output_notebook</span>
<span id="cb27-531"><a href="#cb27-531" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> bokeh.models <span class="im">import</span> ColumnDataSource, HoverTool</span>
<span id="cb27-532"><a href="#cb27-532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-533"><a href="#cb27-533" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-534"><a href="#cb27-534" aria-hidden="true" tabindex="-1"></a>reducer <span class="op">=</span> umap.UMAP(n_components<span class="op">=</span><span class="dv">2</span>, random_state<span class="op">=</span><span class="dv">42</span>, n_neighbors<span class="op">=</span><span class="dv">15</span>, metric<span class="op">=</span><span class="st">"cosine"</span>)</span>
<span id="cb27-535"><a href="#cb27-535" aria-hidden="true" tabindex="-1"></a>xy <span class="op">=</span> reducer.fit_transform(embedding)</span>
<span id="cb27-536"><a href="#cb27-536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-537"><a href="#cb27-537" aria-hidden="true" tabindex="-1"></a>output_notebook()</span>
<span id="cb27-538"><a href="#cb27-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-539"><a href="#cb27-539" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the degree of each node</span></span>
<span id="cb27-540"><a href="#cb27-540" aria-hidden="true" tabindex="-1"></a>degrees <span class="op">=</span> A.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>).A1</span>
<span id="cb27-541"><a href="#cb27-541" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-542"><a href="#cb27-542" aria-hidden="true" tabindex="-1"></a>source <span class="op">=</span> ColumnDataSource(data<span class="op">=</span><span class="bu">dict</span>(</span>
<span id="cb27-543"><a href="#cb27-543" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span>xy[:, <span class="dv">0</span>],</span>
<span id="cb27-544"><a href="#cb27-544" aria-hidden="true" tabindex="-1"></a>    y<span class="op">=</span>xy[:, <span class="dv">1</span>],</span>
<span id="cb27-545"><a href="#cb27-545" aria-hidden="true" tabindex="-1"></a>    size<span class="op">=</span>np.sqrt(degrees <span class="op">/</span> np.<span class="bu">max</span>(degrees)) <span class="op">*</span> <span class="dv">30</span>,</span>
<span id="cb27-546"><a href="#cb27-546" aria-hidden="true" tabindex="-1"></a>    community<span class="op">=</span>[palette[label] <span class="cf">for</span> label <span class="kw">in</span> g.vs[<span class="st">"label"</span>]]</span>
<span id="cb27-547"><a href="#cb27-547" aria-hidden="true" tabindex="-1"></a>))</span>
<span id="cb27-548"><a href="#cb27-548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-549"><a href="#cb27-549" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> figure(title<span class="op">=</span><span class="st">"Node Embeddings from Word2Vec"</span>, x_axis_label<span class="op">=</span><span class="st">"X"</span>, y_axis_label<span class="op">=</span><span class="st">"Y"</span>)</span>
<span id="cb27-550"><a href="#cb27-550" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-551"><a href="#cb27-551" aria-hidden="true" tabindex="-1"></a>p.scatter(<span class="st">'x'</span>, <span class="st">'y'</span>, size<span class="op">=</span><span class="st">'size'</span>, source<span class="op">=</span>source, line_color<span class="op">=</span><span class="st">"black"</span>, color<span class="op">=</span><span class="st">"community"</span>)</span>
<span id="cb27-552"><a href="#cb27-552" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-553"><a href="#cb27-553" aria-hidden="true" tabindex="-1"></a>show(p)</span>
<span id="cb27-554"><a href="#cb27-554" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-555"><a href="#cb27-555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-556"><a href="#cb27-556" aria-hidden="true" tabindex="-1"></a><span class="fu">##### Step 4: Clustering</span></span>
<span id="cb27-557"><a href="#cb27-557" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-558"><a href="#cb27-558" aria-hidden="true" tabindex="-1"></a>One of the interesting applications with node embeddings is clustering. While we have good community detection methods, like the modularity maximization and stochastic block model, we can use clustering methods from machine learning, such as $K$-means and Gaussian mixture model. Let's see what we can get from the node embeddings.</span>
<span id="cb27-559"><a href="#cb27-559" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-562"><a href="#cb27-562" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb27-563"><a href="#cb27-563" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> KMeans</span>
<span id="cb27-564"><a href="#cb27-564" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> silhouette_score</span>
<span id="cb27-565"><a href="#cb27-565" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-566"><a href="#cb27-566" aria-hidden="true" tabindex="-1"></a><span class="co"># Determine the optimal number of clusters using the silhouette score</span></span>
<span id="cb27-567"><a href="#cb27-567" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> Kmeans_with_silhouette(embedding, n_clusters_range<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">10</span>)):</span>
<span id="cb27-568"><a href="#cb27-568" aria-hidden="true" tabindex="-1"></a>    silhouette_scores <span class="op">=</span> []</span>
<span id="cb27-569"><a href="#cb27-569" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-570"><a href="#cb27-570" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Iterate over a range of cluster numbers from 2 to 9</span></span>
<span id="cb27-571"><a href="#cb27-571" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> n_clusters <span class="kw">in</span> <span class="bu">range</span>(<span class="op">*</span>n_clusters_range):</span>
<span id="cb27-572"><a href="#cb27-572" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create a KMeans object with the current number of clusters</span></span>
<span id="cb27-573"><a href="#cb27-573" aria-hidden="true" tabindex="-1"></a>        kmeans <span class="op">=</span> KMeans(n_clusters<span class="op">=</span>n_clusters)</span>
<span id="cb27-574"><a href="#cb27-574" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-575"><a href="#cb27-575" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Fit the KMeans model to the embedding data</span></span>
<span id="cb27-576"><a href="#cb27-576" aria-hidden="true" tabindex="-1"></a>        kmeans.fit(embedding)</span>
<span id="cb27-577"><a href="#cb27-577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-578"><a href="#cb27-578" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate the silhouette score for the current clustering</span></span>
<span id="cb27-579"><a href="#cb27-579" aria-hidden="true" tabindex="-1"></a>        score <span class="op">=</span> silhouette_score(embedding, kmeans.labels_)</span>
<span id="cb27-580"><a href="#cb27-580" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-581"><a href="#cb27-581" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Append the number of clusters and its corresponding silhouette score to the list</span></span>
<span id="cb27-582"><a href="#cb27-582" aria-hidden="true" tabindex="-1"></a>        silhouette_scores.append((n_clusters, score))</span>
<span id="cb27-583"><a href="#cb27-583" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-584"><a href="#cb27-584" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Find the number of clusters that has the highest silhouette score</span></span>
<span id="cb27-585"><a href="#cb27-585" aria-hidden="true" tabindex="-1"></a>    optimal_n_clusters <span class="op">=</span> <span class="bu">max</span>(silhouette_scores, key<span class="op">=</span><span class="kw">lambda</span> x: x[<span class="dv">1</span>])[<span class="dv">0</span>]</span>
<span id="cb27-586"><a href="#cb27-586" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-587"><a href="#cb27-587" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create a KMeans object with the optimal number of clusters</span></span>
<span id="cb27-588"><a href="#cb27-588" aria-hidden="true" tabindex="-1"></a>    kmeans <span class="op">=</span> KMeans(n_clusters<span class="op">=</span>optimal_n_clusters)</span>
<span id="cb27-589"><a href="#cb27-589" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-590"><a href="#cb27-590" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Fit the KMeans model to the embedding data with the optimal number of clusters</span></span>
<span id="cb27-591"><a href="#cb27-591" aria-hidden="true" tabindex="-1"></a>    kmeans.fit(embedding)</span>
<span id="cb27-592"><a href="#cb27-592" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-593"><a href="#cb27-593" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Return the labels (cluster assignments) for each data point</span></span>
<span id="cb27-594"><a href="#cb27-594" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> kmeans.labels_</span>
<span id="cb27-595"><a href="#cb27-595" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-596"><a href="#cb27-596" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-599"><a href="#cb27-599" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb27-600"><a href="#cb27-600" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb27-601"><a href="#cb27-601" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> Kmeans_with_silhouette(embedding)</span>
<span id="cb27-602"><a href="#cb27-602" aria-hidden="true" tabindex="-1"></a>cmap <span class="op">=</span> sns.color_palette().as_hex()</span>
<span id="cb27-603"><a href="#cb27-603" aria-hidden="true" tabindex="-1"></a>igraph.plot(g, vertex_color<span class="op">=</span>[cmap[label] <span class="cf">for</span> label <span class="kw">in</span> labels], bbox<span class="op">=</span>(<span class="dv">500</span>, <span class="dv">500</span>))</span>
<span id="cb27-604"><a href="#cb27-604" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-605"><a href="#cb27-605" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-606"><a href="#cb27-606" aria-hidden="true" tabindex="-1"></a><span class="fu">### node2vec</span></span>
<span id="cb27-607"><a href="#cb27-607" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-608"><a href="#cb27-608" aria-hidden="true" tabindex="-1"></a>node2vec is a sibling of DeepWalk proposed by {footcite}<span class="in">`grover2016node2vec`</span>. Both use word2vec trained on random walks on networks. So, it appears that they are very similar. However, the following two components make them very different.</span>
<span id="cb27-609"><a href="#cb27-609" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-610"><a href="#cb27-610" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Biased random walk**: node2vec uses biased random walks that can move in different directions. The bias walk is parameterized by two parameters, $p$ and $q$:</span>
<span id="cb27-611"><a href="#cb27-611" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-612"><a href="#cb27-612" aria-hidden="true" tabindex="-1"></a>    $$</span>
<span id="cb27-613"><a href="#cb27-613" aria-hidden="true" tabindex="-1"></a>    P(v_{t+1} = x | v_t = v, v_{t-1} = t) \propto</span>
<span id="cb27-614"><a href="#cb27-614" aria-hidden="true" tabindex="-1"></a>    \begin{cases}</span>
<span id="cb27-615"><a href="#cb27-615" aria-hidden="true" tabindex="-1"></a>    \frac{1}{p} &amp; \text{if } d(v,t) = 0 <span class="sc">\\</span></span>
<span id="cb27-616"><a href="#cb27-616" aria-hidden="true" tabindex="-1"></a>    1 &amp; \text{if } d(v,t) = 1 <span class="sc">\\</span></span>
<span id="cb27-617"><a href="#cb27-617" aria-hidden="true" tabindex="-1"></a>    \frac{1}{q} &amp; \text{if } d(v,t) = 2 <span class="sc">\\</span></span>
<span id="cb27-618"><a href="#cb27-618" aria-hidden="true" tabindex="-1"></a>    \end{cases}</span>
<span id="cb27-619"><a href="#cb27-619" aria-hidden="true" tabindex="-1"></a>    $$</span>
<span id="cb27-620"><a href="#cb27-620" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-621"><a href="#cb27-621" aria-hidden="true" tabindex="-1"></a>    where $d(v,x)$ is the shortest path distance between node $v$ and $x$. A smaller $p$ leads to more biased towards the previous node, $v_{t-1} = t$. A smaller $q$ leads to more biased towards the nodes that are further away from the previous node, $v_{t-1} = t$.</span>
<span id="cb27-622"><a href="#cb27-622" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-623"><a href="#cb27-623" aria-hidden="true" tabindex="-1"></a>    By adjusting the parameters $p$ and $q$, we can influence the random walk to behave more like either breadth-first sampling (BFS) or depth-first sampling (DFS).</span>
<span id="cb27-624"><a href="#cb27-624" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-625"><a href="#cb27-625" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>**Breadth-First Sampling (BFS)**: This type of sampling explores all the neighbors of a node before moving on to the next level of neighbors. It is useful for capturing community structures within the graph. When we set the parameters to favor BFS, the resulting embeddings will reflect these community structures.</span>
<span id="cb27-626"><a href="#cb27-626" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-627"><a href="#cb27-627" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>**Depth-First Sampling (DFS)**: This type of sampling goes deep into the graph, exploring as far as possible along each branch before backtracking. It is useful for capturing structural equivalence, where nodes that have similar roles in the graph (even if they are not directly connected) are represented similarly. When we set the parameters to favor DFS, the resulting embeddings will reflect these structural equivalences.</span>
<span id="cb27-628"><a href="#cb27-628" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-629"><a href="#cb27-629" aria-hidden="true" tabindex="-1"></a>    <span class="al">![](https://www.researchgate.net/publication/354654762/figure/fig3/AS:1069013035655173@1631883977008/A-biased-random-walk-procedure-of-node2vec-B-BFS-and-DFS-search-strategies-from-node-u.png)</span></span>
<span id="cb27-630"><a href="#cb27-630" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-631"><a href="#cb27-631" aria-hidden="true" tabindex="-1"></a><span class="in">    The embeddings generated by node2vec can capture different aspects of the graph depending on the sampling strategy used. With BFS, we capture community structures, and with DFS, we capture structural equivalence.</span></span>
<span id="cb27-632"><a href="#cb27-632" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-633"><a href="#cb27-633" aria-hidden="true" tabindex="-1"></a><span class="in">    ![](https://miro.medium.com/v2/resize:fit:1138/format:webp/1*nCyF5jFSU5uJVdAPdf-0HA.png)</span></span>
<span id="cb27-634"><a href="#cb27-634" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-635"><a href="#cb27-635" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Negative sampling**: node2vec uses negative sampling, instead of hierarchical softmax. This difference appears to be minor, but it has significant consequences on the characteristics of the embeddings. This is beyond the scope of this lecture, but you can refer to {footcite}<span class="in">`kojaku2021neurips`</span> and {footcite}<span class="in">`dyer2014notes`</span> for more details.</span>
<span id="cb27-636"><a href="#cb27-636" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-637"><a href="#cb27-637" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 02: Implement node2vec</span></span>
<span id="cb27-638"><a href="#cb27-638" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-639"><a href="#cb27-639" aria-hidden="true" tabindex="-1"></a>Let's implement the biased random walk for node2vec</span>
<span id="cb27-642"><a href="#cb27-642" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb27-643"><a href="#cb27-643" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> node2vec_random_walk(net, start_node, walk_length, p, q):</span>
<span id="cb27-644"><a href="#cb27-644" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb27-645"><a href="#cb27-645" aria-hidden="true" tabindex="-1"></a><span class="co">    Sample a random walk starting from start_node.</span></span>
<span id="cb27-646"><a href="#cb27-646" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb27-647"><a href="#cb27-647" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize the walk with the start_node</span></span>
<span id="cb27-648"><a href="#cb27-648" aria-hidden="true" tabindex="-1"></a>    walk <span class="op">=</span> [start_node]</span>
<span id="cb27-649"><a href="#cb27-649" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-650"><a href="#cb27-650" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Continue the walk until it reaches the desired length</span></span>
<span id="cb27-651"><a href="#cb27-651" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="bu">len</span>(walk) <span class="op">&lt;</span> walk_length:</span>
<span id="cb27-652"><a href="#cb27-652" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Get the current node in the walk</span></span>
<span id="cb27-653"><a href="#cb27-653" aria-hidden="true" tabindex="-1"></a>        cur <span class="op">=</span> walk[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb27-654"><a href="#cb27-654" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Get the neighbors of the current node</span></span>
<span id="cb27-655"><a href="#cb27-655" aria-hidden="true" tabindex="-1"></a>        cur_nbrs <span class="op">=</span> <span class="bu">list</span>(net[cur].indices)</span>
<span id="cb27-656"><a href="#cb27-656" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Check if the current node has any neighbors</span></span>
<span id="cb27-657"><a href="#cb27-657" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">len</span>(cur_nbrs) <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb27-658"><a href="#cb27-658" aria-hidden="true" tabindex="-1"></a>            <span class="co"># If the walk has just started, randomly choose the next node from the neighbors</span></span>
<span id="cb27-659"><a href="#cb27-659" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">len</span>(walk) <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb27-660"><a href="#cb27-660" aria-hidden="true" tabindex="-1"></a>                walk.append(np.random.choice(cur_nbrs))</span>
<span id="cb27-661"><a href="#cb27-661" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb27-662"><a href="#cb27-662" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Get the previous node in the walk</span></span>
<span id="cb27-663"><a href="#cb27-663" aria-hidden="true" tabindex="-1"></a>                prev <span class="op">=</span> walk[<span class="op">-</span><span class="dv">2</span>]</span>
<span id="cb27-664"><a href="#cb27-664" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Use the alias sampling method to choose the next node based on the bias parameters p and q</span></span>
<span id="cb27-665"><a href="#cb27-665" aria-hidden="true" tabindex="-1"></a>                next_node <span class="op">=</span> alias_sample(net, cur_nbrs, prev, p, q)</span>
<span id="cb27-666"><a href="#cb27-666" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Append the chosen next node to the walk</span></span>
<span id="cb27-667"><a href="#cb27-667" aria-hidden="true" tabindex="-1"></a>                walk.append(next_node)</span>
<span id="cb27-668"><a href="#cb27-668" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb27-669"><a href="#cb27-669" aria-hidden="true" tabindex="-1"></a>            <span class="co"># If the current node has no neighbors, terminate the walk</span></span>
<span id="cb27-670"><a href="#cb27-670" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb27-671"><a href="#cb27-671" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-672"><a href="#cb27-672" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> walk</span>
<span id="cb27-673"><a href="#cb27-673" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-674"><a href="#cb27-674" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> alias_sample(net, neighbors, prev, p, q):</span>
<span id="cb27-675"><a href="#cb27-675" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb27-676"><a href="#cb27-676" aria-hidden="true" tabindex="-1"></a><span class="co">    Helper function to sample the next node in the walk.</span></span>
<span id="cb27-677"><a href="#cb27-677" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb27-678"><a href="#cb27-678" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Implement the logic to sample the next node based on the bias parameters p and q</span></span>
<span id="cb27-679"><a href="#cb27-679" aria-hidden="true" tabindex="-1"></a>    <span class="co"># You can use the formula provided in the instructions to calculate the probabilities</span></span>
<span id="cb27-680"><a href="#cb27-680" aria-hidden="true" tabindex="-1"></a>    <span class="co"># and then sample the next node accordingly.</span></span>
<span id="cb27-681"><a href="#cb27-681" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize an empty list to store the unnormalized probabilities for each neighbor</span></span>
<span id="cb27-682"><a href="#cb27-682" aria-hidden="true" tabindex="-1"></a>    unnormalized_probs <span class="op">=</span> []</span>
<span id="cb27-683"><a href="#cb27-683" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-684"><a href="#cb27-684" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Iterate over each neighbor of the current node</span></span>
<span id="cb27-685"><a href="#cb27-685" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> neighbor <span class="kw">in</span> neighbors:</span>
<span id="cb27-686"><a href="#cb27-686" aria-hidden="true" tabindex="-1"></a>        <span class="co"># If the neighbor is the same as the previous node in the walk</span></span>
<span id="cb27-687"><a href="#cb27-687" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> neighbor <span class="op">==</span> prev:</span>
<span id="cb27-688"><a href="#cb27-688" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Append the probability 1/p to the unnormalized probabilities list</span></span>
<span id="cb27-689"><a href="#cb27-689" aria-hidden="true" tabindex="-1"></a>            unnormalized_probs.append(<span class="dv">1</span> <span class="op">/</span> p)</span>
<span id="cb27-690"><a href="#cb27-690" aria-hidden="true" tabindex="-1"></a>        <span class="co"># If the neighbor is connected to the previous node in the walk</span></span>
<span id="cb27-691"><a href="#cb27-691" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> neighbor <span class="kw">in</span> net[prev].indices:</span>
<span id="cb27-692"><a href="#cb27-692" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Append the probability 1 to the unnormalized probabilities list</span></span>
<span id="cb27-693"><a href="#cb27-693" aria-hidden="true" tabindex="-1"></a>            unnormalized_probs.append(<span class="dv">1</span>)</span>
<span id="cb27-694"><a href="#cb27-694" aria-hidden="true" tabindex="-1"></a>        <span class="co"># If the neighbor is not connected to the previous node in the walk</span></span>
<span id="cb27-695"><a href="#cb27-695" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb27-696"><a href="#cb27-696" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Append the probability 1/q to the unnormalized probabilities list</span></span>
<span id="cb27-697"><a href="#cb27-697" aria-hidden="true" tabindex="-1"></a>            unnormalized_probs.append(<span class="dv">1</span> <span class="op">/</span> q)</span>
<span id="cb27-698"><a href="#cb27-698" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-699"><a href="#cb27-699" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate the normalization constant by summing all unnormalized probabilities</span></span>
<span id="cb27-700"><a href="#cb27-700" aria-hidden="true" tabindex="-1"></a>    norm_const <span class="op">=</span> <span class="bu">sum</span>(unnormalized_probs)</span>
<span id="cb27-701"><a href="#cb27-701" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-702"><a href="#cb27-702" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Normalize the probabilities by dividing each unnormalized probability by the normalization constant</span></span>
<span id="cb27-703"><a href="#cb27-703" aria-hidden="true" tabindex="-1"></a>    normalized_probs <span class="op">=</span> [<span class="bu">float</span>(prob) <span class="op">/</span> norm_const <span class="cf">for</span> prob <span class="kw">in</span> unnormalized_probs]</span>
<span id="cb27-704"><a href="#cb27-704" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-705"><a href="#cb27-705" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Randomly choose the next node from the neighbors based on the normalized probabilities</span></span>
<span id="cb27-706"><a href="#cb27-706" aria-hidden="true" tabindex="-1"></a>    next_node <span class="op">=</span> np.random.choice(neighbors, size<span class="op">=</span><span class="dv">1</span>, p<span class="op">=</span>normalized_probs)[<span class="dv">0</span>]</span>
<span id="cb27-707"><a href="#cb27-707" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-708"><a href="#cb27-708" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Return the chosen next node</span></span>
<span id="cb27-709"><a href="#cb27-709" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> next_node</span>
<span id="cb27-710"><a href="#cb27-710" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-711"><a href="#cb27-711" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-712"><a href="#cb27-712" aria-hidden="true" tabindex="-1"></a>Now, let's set up the word2vec model for node2vec.</span>
<span id="cb27-713"><a href="#cb27-713" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-716"><a href="#cb27-716" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb27-717"><a href="#cb27-717" aria-hidden="true" tabindex="-1"></a>walks <span class="op">=</span> []</span>
<span id="cb27-718"><a href="#cb27-718" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb27-719"><a href="#cb27-719" aria-hidden="true" tabindex="-1"></a>q <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb27-720"><a href="#cb27-720" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_nodes):</span>
<span id="cb27-721"><a href="#cb27-721" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_walkers_per_node):</span>
<span id="cb27-722"><a href="#cb27-722" aria-hidden="true" tabindex="-1"></a>        walks.append(node2vec_random_walk(A, i, walk_length, p, q))</span>
<span id="cb27-723"><a href="#cb27-723" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Word2Vec(walks, vector_size<span class="op">=</span><span class="dv">32</span>, window<span class="op">=</span><span class="dv">3</span>, min_count<span class="op">=</span><span class="dv">1</span>, sg<span class="op">=</span><span class="dv">1</span>, hs <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb27-724"><a href="#cb27-724" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-725"><a href="#cb27-725" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-726"><a href="#cb27-726" aria-hidden="true" tabindex="-1"></a>where <span class="in">`hs=0`</span> indicates that we are using negative sampling.</span>
<span id="cb27-727"><a href="#cb27-727" aria-hidden="true" tabindex="-1"></a>Notice that we set <span class="in">`sg=1`</span> and <span class="in">`hs=1`</span> instead of <span class="in">`sg=1`</span> and <span class="in">`hs=0`</span> in DeepWalk. This is because node2vec uses the skip-gram model with negative sampling.</span>
<span id="cb27-728"><a href="#cb27-728" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-729"><a href="#cb27-729" aria-hidden="true" tabindex="-1"></a>Now, we extract the node embeddings from the word2vec model.</span>
<span id="cb27-730"><a href="#cb27-730" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-733"><a href="#cb27-733" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb27-734"><a href="#cb27-734" aria-hidden="true" tabindex="-1"></a>embedding <span class="op">=</span> []</span>
<span id="cb27-735"><a href="#cb27-735" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_nodes):</span>
<span id="cb27-736"><a href="#cb27-736" aria-hidden="true" tabindex="-1"></a>    embedding.append(model.wv[i])</span>
<span id="cb27-737"><a href="#cb27-737" aria-hidden="true" tabindex="-1"></a>embedding <span class="op">=</span> np.array(embedding)</span>
<span id="cb27-738"><a href="#cb27-738" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-739"><a href="#cb27-739" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-740"><a href="#cb27-740" aria-hidden="true" tabindex="-1"></a>Let's visualize the node embeddings from node2vec.</span>
<span id="cb27-741"><a href="#cb27-741" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-744"><a href="#cb27-744" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb27-745"><a href="#cb27-745" aria-hidden="true" tabindex="-1"></a>:tags: [hide<span class="op">-</span><span class="bu">input</span>]</span>
<span id="cb27-746"><a href="#cb27-746" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-747"><a href="#cb27-747" aria-hidden="true" tabindex="-1"></a>reducer <span class="op">=</span> umap.UMAP(n_components<span class="op">=</span><span class="dv">2</span>, random_state<span class="op">=</span><span class="dv">42</span>, n_neighbors<span class="op">=</span><span class="dv">15</span>, metric<span class="op">=</span><span class="st">"cosine"</span>)</span>
<span id="cb27-748"><a href="#cb27-748" aria-hidden="true" tabindex="-1"></a>xy <span class="op">=</span> reducer.fit_transform(embedding)</span>
<span id="cb27-749"><a href="#cb27-749" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-750"><a href="#cb27-750" aria-hidden="true" tabindex="-1"></a>output_notebook()</span>
<span id="cb27-751"><a href="#cb27-751" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-752"><a href="#cb27-752" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the degree of each node</span></span>
<span id="cb27-753"><a href="#cb27-753" aria-hidden="true" tabindex="-1"></a>degrees <span class="op">=</span> A.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>).A1</span>
<span id="cb27-754"><a href="#cb27-754" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-755"><a href="#cb27-755" aria-hidden="true" tabindex="-1"></a>source <span class="op">=</span> ColumnDataSource(data<span class="op">=</span><span class="bu">dict</span>(</span>
<span id="cb27-756"><a href="#cb27-756" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span>xy[:, <span class="dv">0</span>],</span>
<span id="cb27-757"><a href="#cb27-757" aria-hidden="true" tabindex="-1"></a>    y<span class="op">=</span>xy[:, <span class="dv">1</span>],</span>
<span id="cb27-758"><a href="#cb27-758" aria-hidden="true" tabindex="-1"></a>    size<span class="op">=</span>np.sqrt(degrees <span class="op">/</span> np.<span class="bu">max</span>(degrees)) <span class="op">*</span> <span class="dv">30</span>,</span>
<span id="cb27-759"><a href="#cb27-759" aria-hidden="true" tabindex="-1"></a>    community<span class="op">=</span>[palette[label] <span class="cf">for</span> label <span class="kw">in</span> g.vs[<span class="st">"label"</span>]],</span>
<span id="cb27-760"><a href="#cb27-760" aria-hidden="true" tabindex="-1"></a>    name <span class="op">=</span> [<span class="bu">str</span>(i) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_nodes)]</span>
<span id="cb27-761"><a href="#cb27-761" aria-hidden="true" tabindex="-1"></a>))</span>
<span id="cb27-762"><a href="#cb27-762" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-763"><a href="#cb27-763" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> figure(title<span class="op">=</span><span class="st">"Node Embeddings from Word2Vec"</span>, x_axis_label<span class="op">=</span><span class="st">"X"</span>, y_axis_label<span class="op">=</span><span class="st">"Y"</span>)</span>
<span id="cb27-764"><a href="#cb27-764" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-765"><a href="#cb27-765" aria-hidden="true" tabindex="-1"></a>p.scatter(<span class="st">'x'</span>, <span class="st">'y'</span>, size<span class="op">=</span><span class="st">'size'</span>, source<span class="op">=</span>source, line_color<span class="op">=</span><span class="st">"black"</span>, color<span class="op">=</span><span class="st">"community"</span>)</span>
<span id="cb27-766"><a href="#cb27-766" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-767"><a href="#cb27-767" aria-hidden="true" tabindex="-1"></a>hover <span class="op">=</span> HoverTool()</span>
<span id="cb27-768"><a href="#cb27-768" aria-hidden="true" tabindex="-1"></a>hover.tooltips <span class="op">=</span> [</span>
<span id="cb27-769"><a href="#cb27-769" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"Name"</span>, <span class="st">"@name"</span>),</span>
<span id="cb27-770"><a href="#cb27-770" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"Community"</span>, <span class="st">"@community"</span>)</span>
<span id="cb27-771"><a href="#cb27-771" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb27-772"><a href="#cb27-772" aria-hidden="true" tabindex="-1"></a>p.add_tools(hover)</span>
<span id="cb27-773"><a href="#cb27-773" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-774"><a href="#cb27-774" aria-hidden="true" tabindex="-1"></a>show(p)</span>
<span id="cb27-775"><a href="#cb27-775" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-776"><a href="#cb27-776" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-777"><a href="#cb27-777" aria-hidden="true" tabindex="-1"></a>The results for clustering are as follows:</span>
<span id="cb27-778"><a href="#cb27-778" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-781"><a href="#cb27-781" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb27-782"><a href="#cb27-782" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb27-783"><a href="#cb27-783" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-784"><a href="#cb27-784" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> Kmeans_with_silhouette(embedding)</span>
<span id="cb27-785"><a href="#cb27-785" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-786"><a href="#cb27-786" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-787"><a href="#cb27-787" aria-hidden="true" tabindex="-1"></a>cmap <span class="op">=</span> sns.color_palette().as_hex()</span>
<span id="cb27-788"><a href="#cb27-788" aria-hidden="true" tabindex="-1"></a>igraph.plot(g, vertex_color<span class="op">=</span>[cmap[label] <span class="cf">for</span> label <span class="kw">in</span> labels], bbox<span class="op">=</span>(<span class="dv">500</span>, <span class="dv">500</span>), vertex_label<span class="op">=</span>[<span class="st">"</span><span class="sc">%d</span><span class="st">"</span> <span class="op">%</span>  d <span class="cf">for</span> d <span class="kw">in</span>  np.arange(n_nodes)])</span>
<span id="cb27-789"><a href="#cb27-789" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-790"><a href="#cb27-790" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-791"><a href="#cb27-791" aria-hidden="true" tabindex="-1"></a><span class="fu">### LINE</span></span>
<span id="cb27-792"><a href="#cb27-792" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-793"><a href="#cb27-793" aria-hidden="true" tabindex="-1"></a>LINE {footcite}<span class="in">`tang2015line`</span> is another pioneering work to learn node embeddings by directly optimizing the graph structure.</span>
<span id="cb27-794"><a href="#cb27-794" aria-hidden="true" tabindex="-1"></a>It is equivalent to node2vec with $p=1$, $q=1$, and window size 1.</span>
<span id="cb27-795"><a href="#cb27-795" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-798"><a href="#cb27-798" aria-hidden="true" tabindex="-1"></a><span class="in">```{footbibliography}</span></span>
<span id="cb27-799"><a href="#cb27-799" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Copyright 2024, Sadamori Kojaku</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions"><ul><li><a href="https://github.com/skojaku/adv-net-sci/edit/main/m08-embedding/02-coding.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/skojaku/adv-net-sci/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/skojaku/adv-net-sci">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>