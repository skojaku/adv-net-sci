{
  "hash": "ba5b0e19d632c2d57483d5fb42ed11f0",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Concepts - From image to graph\"\njupyter: advnetsci\nexecute:\n    enabled: true\n---\n\n## Analogy between image and graph data\nWe can think of a convolution of an image from the perspective of networks.\nIn the convolution of an image, a pixel is convolved with its *neighbors*. We can regard each pixel as a node, and each node is connected to its neighboring nodes (pixels) that are involved in the convolution.\n\n![](https://av-eks-lekhak.s3.amazonaws.com/media/__sized__/article_images/conv_graph-thumbnail_webp-600x300.webp)\n\nBuilding on this analogy, we can extend the idea of convolution to general graph data.\nEach node has a pixel value(s) (e.g., feature vector), which is convolved with the values of its neighbors in the graph.\nThis is the key idea of graph convolutional networks.\nBut, there is a key difference: while the number of neighbors for an image is homogeneous, the number of neighbors for a node in a graph can be heterogeneous. Each pixel has the same number of neighbors (except for the boundary pixels), but nodes in a graph can have very different numbers of neighbors. This makes it non-trivial to define the \"kernel\" for graph convolution.\n\n## Spectral filter on graphs\nJust like we can define a convolution on images in the frequency domain, we can also define a ''frequency domain'' for graphs.\n\nConsider a network of $N$ nodes, where each node has a feature variable ${\\mathbf x}_i \\in \\mathbb{R}$. We are interested in:\n\n$$\nJ = \\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N A_{ij}(x_i - x_j)^2,\n$$\n\nwhere $A_{ij}$ is the adjacency matrix of the graph. The quantity $J$ represents *the total variation* of $x$ between connected nodes; a small $J$ means that connected nodes have similar $x$ (low variation; low frequency), while a large $J$ means that connected nodes have very different $x$ (high variation; high frequency).\n\nWe can rewrite $J$ as\n\n$$\nJ = \\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N A_{ij}(x_i - x_j)^2 = {\\bf x}^\\top {\\bf L} {\\bf x},\n$$\n\nwhere ${\\bf L}$ is the Laplacian matrix of the graph given by\n\n$$\nL_{ij} = \\begin{cases}\n-1 & \\text{if } i \\text{ and } j \\text{ are connected} \\\\\nk_i & \\text{if } i = j \\\\\n0 & \\text{otherwise}\n\\end{cases}.\n$$\n\nand ${\\bf x} = [x_1,x_2,\\ldots, x_N]^\\top$ is a column vector of feature variables.\n\n\n::: {.callout-note title=\"Detailed derivation\"}\n\nThe above derivation shows that the total variation of $x$ between connected nodes is proportional to ${\\bf x}^\\top {\\bf L} {\\bf x}$.\n\n$$\n\\begin{aligned}\nJ &= \\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N A_{ij}(x_i - x_j)^2 \\\\\n&= \\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N \\underbrace{A_{ij}\\left( x_i^2 +x_j^2\\right)}_{\\text{symmetric}} - \\sum_{i=1}^N\\sum_{j=1}^N A_{ij}x_ix_j \\\\\n&= \\sum_{i=1}^Nx_i^2\\underbrace{\\sum_{j=1}^N A_{ij}}_{\\text{degree of node } i, k_i} - \\sum_{i=1}^N\\sum_{j=1}^N A_{ij}x_ix_j \\\\\n&= \\sum_{i=1}^Nx_i^2 k_i - \\sum_{i=1}^N\\sum_{j=1}^N A_{ij}x_ix_j \\\\\n&= \\underbrace{[x_1,x_2,\\ldots, x_N]}_{{\\bf x}} \\underbrace{\\begin{bmatrix} k_1 & 0 & \\cdots & 0 \\\\ 0 & k_2 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & k_N \\end{bmatrix}}_{{\\bf D}} \\underbrace{\\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_N \\end{bmatrix}}_{{\\bf x}} - 2\\underbrace{\\sum_{i=1}^N\\sum_{j=1}^N A_{ij}}_{{\\bf x}^\\top {\\mathbf A} {\\bf x}} {\\bf x} \\\\\n&= {\\bf x}^\\top {\\bf D} {\\bf x} - {\\bf x}^\\top {\\mathbf A} {\\bf x} \\\\\n&= {\\bf x}^\\top {\\bf L} {\\bf x},\n\\end{aligned}\n$$\n:::\n\nLet us showcase the analogy between the Fourier transform and the Laplacian matrix.\nIn the Fourier transform, a signal is decomposed into sinusoidal basis functions. Similarly, for a graph, we can decompose the variation $J$ into eigenvector bases.\n\n$$\nJ = \\sum_{i=1}^N \\lambda_i  {\\bf x}^\\top {\\mathbf u}_i {\\mathbf u}_i^\\top {\\bf x} = \\sum_{i=1}^N \\lambda_i  ||{\\bf x}^\\top {\\mathbf u}_i||^2.\n$$\n\nwhere ${\\mathbf u}_i$ is the eigenvector corresponding to the eigenvalue $\\lambda_i$.\n^\\top {\\mathbf u}_i)$ is a dot-product between the feature vector ${\\bf x}$ and the eigenvector ${\\mathbf u}_i$, which measures how much ${\\bf x}$ *coheres* with eigenvector ${\\mathbf u}_i$, similar to how Fourier coefficients measure coherency with sinusoids.\n- Each $||{\\bf x}^\\top {\\mathbf u}_i||^2$ is the ''strength'' of ${\\bf x}$ with respect to the eigenvector ${\\mathbf u}_i$, and the total variation $J$ is a weighted sum of these strengths.\n\nSome eigenvectors correspond to low-frequency components, while others correspond to high-frequency components. For example, the total variation $J$ for an eigenvector ${\\mathbf u}_i$ is given by\n\n$$\nJ = \\frac{1}{2} \\sum_{j}\\sum_{\\ell} A_{j\\ell}(u_{ij} - u_{i\\ell})^2 = {\\mathbf u}_i^\\top {\\mathbf L} {\\mathbf u}_i = \\lambda_i.\n$$\n\nThis equation provides key insight into the meaning of eigenvalues:\n\n1. For an eigenvector ${\\mathbf u}_i$, its eigenvalue $\\lambda_i$ measures the total variation for ${\\mathbf u}_i$.\n2. Large eigenvalues mean large differences between neighbors (high frequency), while small eigenvalues mean small differences (low frequency).\n\nThus, if ${\\bf x}$ aligns well with ${\\mathbf u}_i$ with a large $\\lambda_i$, then ${\\bf x}$ has a strong high-frequency component; if ${\\bf x}$ aligns well with ${\\mathbf u}_i$ with a small $\\lambda_i$, then ${\\bf x}$ has strong low-frequency component.\n\n### Spectral Filtering\n\nEigenvalues $\\lambda_i$ can be thought of as a *filter* that controls which frequency components pass through. Instead of using the filter associated with the Laplacian matrix, we can design a filter $h(\\lambda_i)$ to control which frequency components pass through. This leads to the idea of *spectral filtering*. Two common filters are:\n\n1. **Low-pass Filter**:\n   $$h_{\\text{low}}(\\lambda) = \\frac{1}{1 + \\alpha\\lambda}$$\n   - Preserves low frequencies (small λ)\n   - Suppresses high frequencies (large λ)\n   - Results in smoother signals\n\n2. **High-pass Filter**:\n   $$h_{\\text{high}}(\\lambda) = \\frac{\\alpha\\lambda}{1 + \\alpha\\lambda}$$\n   - Preserves high frequencies\n   - Suppresses low frequencies\n   - Emphasizes differences between neighbors\n\n::: {#2817f04a .cell execution_count=1}\n\n::: {.cell-output .cell-output-display}\n![](01-concepts_files/figure-html/cell-2-output-1.png){}\n:::\n:::\n\n\n# Graph Convolutional Networks\nWe have seen that spectral filters give us a principled way to think about \"convolution\" on irregular graph structures, and controlling the frequency components brings out different aspects of the data. We now go one step further: instead of designing filters by hand, we can learn them from data for specific tasks.\n\n\n## Spectral Graph Convolutional Networks\n\nA simplest form of learnable spectral filter is given by\n\n$$\n{\\bf L}_{\\text{learn}} = \\sum_{k=1}^K \\theta_k {\\mathbf u}_k {\\mathbf u}_k^\\top,\n$$\n\nwhere ${\\mathbf u}_k$ are the eigenvectors and $\\theta_k$ are the learnable parameters. The variable $K$ is the number of eigenvectors used (i.e., the rank of the filter). The weight $\\theta_k$ is learned to maximize the performance of the task at hand.\n\nBuilding on this idea, [@bruna2014spectral] added a nonlinearity to the filter and proposed a spectral convolutional neural network (GCN) by\n\n$$\n{\\bf x}^{(\\ell+1)} = h\\left( L_{\\text{learn}} {\\bf x}^{(\\ell)}\\right),\n$$\n\nwhere $h$ is an activation function, and ${\\bf x}^{(\\ell)}$ is the feature vector of the $\\ell$-th convolution. They further extend this idea to convolve on multidimensional feature vectors, ${\\bf X} \\in \\mathbb{R}^{N \\times f_{\\text{in}}}$ to produce new feature vectors of different dimensionality, ${\\bf X}' \\in \\mathbb{R}^{N \\times f_{\\text{out}}}$.\n\n$$\n\\begin{aligned}\n{\\bf X}^{(\\ell+1)}_i &= h\\left( \\sum_j L_{\\text{learn}}^{(i,j)} {\\bf X}^{(\\ell)}_j\\right),\\quad \\text{where} \\quad L^{(i,j)}_{\\text{learn}} = \\sum_{k=1}^K \\theta_{k, (i,j)} {\\mathbf u}_k {\\mathbf u}_k^\\top,\n\\end{aligned}\n$$\n\nNotice that the learnable filter $L_{\\text{learn}}^{(i,j)}$ is defined for each pair of input $i$ and output $j$ dimensions.\n\n\n::: {.column-margin}\nMany GCNs simple when it comes to implementation despite the complicated formula. And this is one of my ways to learn GNNs. Check out the [Appendix for the Python implementation](appendix.md).\n\n:::\n\n## From Spectral to Spatial\n\nSpectral GCNs are mathematically elegant but have two main limitations:\n1. **Computational Limitation**: Computing the spectra of the Laplacian is expensive ${\\cal O}(N^3)$ and prohibitive for large graphs\n2. **Spatial Locality**: The learned filters are not spatially localized. A node can be influenced by all other nodes in the graph.\n\nThese two limitations motivate the development of spatial GCNs.\n\n### ChebNet\n\nChebNet [@defferrard2016convolutional] is one of the earliest spatial GCNs that bridges the gap between spectral and spatial domains.\nThe key idea is to leverage Chebyshev polynomials to approximate ${\\bf L}_{\\text{learn}}$ by\n\n$$\n{\\bf L}_{\\text{learn}} \\approx \\sum_{k=0}^{K-1} \\theta_k T_k(\\tilde{{\\bf L}}), \\quad \\text{where} \\quad \\tilde{{\\bf L}} = \\frac{2}{\\lambda_{\\text{max}}}{\\bf L} - {\\bf I},\n$$\n\nwhere $\\tilde{{\\bf L}}$ is the scaled and normalized Laplacian matrix in order to have eigenvalues in the range of $[-1,1]$. The Chebyshev polynomials $T_k(\\tilde{{\\bf L}})$ transforms the eigenvalues $\\tilde{{\\bf L}}$ to the following recursively:\n\n$$\n\\begin{aligned}\nT_0(\\tilde{{\\bf L}}) &= {\\bf I} \\\\\nT_1(\\tilde{{\\bf L}}) &= \\tilde{{\\bf L}} \\\\\nT_k(\\tilde{{\\bf L}}) &= 2\\tilde{{\\bf L}} T_{k-1}(\\tilde{{\\bf L}}) - T_{k-2}(\\tilde{{\\bf L}})\n\\end{aligned}\n$$\n\nWe then replace ${\\bf L}_{\\text{learn}}$ in the original spectral GCN with the Chebyshev polynomial approximation:\n\n$$\n{\\bf x}^{(\\ell+1)} = h\\left( \\sum_{k=0}^{K-1} \\theta_k T_k(\\tilde{{\\bf L}}){\\bf x}^{(\\ell)}\\right),\n$$\n\nwhere:\n- $T_k(\\tilde{{\\bf L}})$ applies the k-th Chebyshev polynomial to the scaled Laplacian matrix\n- $\\theta_k$ are the learnable parameters\n- K is the order of the polynomial (typically small, e.g., K=3)\n\n### Graph Convolutional Networks by Kipf and Welling\n\nWhile ChebNet offers a principled way to approximate spectral convolutions, Kipf and Welling (2017) [@kipf2017semi] proposed an even simpler and highly effective variant called **Graph Convolutional Networks (GCN)**.\n\n\n### First-order Approximation\n\nThe key departure is to use the first-order approximation of the Chebyshev polynomials.\n\n$$\ng_{\\theta'} * x \\approx \\theta'_0x + \\theta'_1(L - I_N)x = \\theta'_0x - \\theta'_1D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}}x\n$$\n\nThis is crude approximation but it leads to a much simpler form, leaving only two learnable parameters, instead of $K$ parameters in the original ChebNet.\n\nAdditionally, they further simplify the formula by using the same $\\theta$ for both remaining parameters (i.e., $\\theta_0 = \\theta$ and $\\theta_1 = -\\theta$). The result is the following convolutional filter:\n\n$$\ng_{\\theta} * x \\approx \\theta(I_N + D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}})x\n$$\n\nWhile this is a very simple filter, one can stack multiple layers of convolutions to perform high-order graph convolutions.\n\n### Deep GCNs can suffer from over-smoothing\n\nGCN models can be deep, and when they are too deep, they start suffering from an ill-posed problem called *gradient vanishing/exploding*, where the gradients of the loss function becomes too small or too large to update the model parameters. It is a common problem in deep learning.\n\nTo facilitate the training of deep GCNs, the authors introduce a very simple trick called *renormalization*. The idea is to add self-connections to the graph:\n\n$$\n\\tilde{A} = A + I_N, \\quad \\text{and} \\quad \\tilde{D}_{ii} = \\sum_j \\tilde{A}_{ij}\n$$\n\nAnd use $\\tilde{A}$ and $\\tilde{D}$ to form the convolutional filter.\n\nAltogether, this leads to the following layer-wise propagation rule:\n\n$$X^{(\\ell+1)} = \\sigma(\\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}X^{(\\ell)}W^{(\\ell)})$$\n\nwhere:\n- $X^{(\\ell)}$ is the matrix of node features at layer $\\ell$\n- $W^{(\\ell)}$ is the layer's trainable weight matrix\n- $\\sigma$ is a nonlinear activation function (e.g., ReLU)\n\nThese simplifications offer several advantages:\n- **Efficiency**: Linear complexity in number of edges\n- **Localization**: Each layer only aggregates information from immediate neighbors\n- **Depth**: Fewer parameters allow building deeper models\n- **Performance**: Despite (or perhaps due to) its simplicity, it often outperforms more complex models\n\n::: {.callout-note title=\"Exercise\"}\n\nLet's implement a simple GCN model for node classification.\n[Coding Exercise](../../../notebooks/exercise-m09-graph-neural-net.ipynb)\n:::\n\n# Popular Graph Neural Networks\n\nIn this note, we will introduce three popular GNNs: GraphSAGE, Graph Attention Networks (GAT), and Graph Isomorphism Network (GIN).\n\n## GraphSAGE: Sample and Aggregate\n\nGraphSAGE [@hamilton2017graphsage] introduced a different GCN that can be ***generalized to unseen nodes*** (they called it \"inductive\"). While previous approaches like ChebNet and GCN operate on the entire graph, GraphSAGE proposes an inductive framework that generates embeddings by sampling and aggregating features from a node's neighborhood.\n\n![](https://theaisummer.com/static/02e23adc75fe68e5dd249a94f3c1e8cc/c483d/graphsage.png)\n\n### Key Ideas\n\nGraphSAGE involves two key ideas: (1) sampling and (2) aggregation.\n\n### Neighborhood Sampling\n\nThe key idea is the *neighborhood sampling*. Instead of using all neighbors, GraphSAGE samples a fixed-size set of neighbors for each node. This controls memory complexity, a key limitation of the previous GNNs.\n\nAnother key advantage of neighborhood sampling is that it enables GraphSAGE to handle dynamic, growing networks. Consider a citation network where new papers (nodes) are continuously added. Traditional GCNs would need to recompute filters for the entire network with each new addition. In contrast, GraphSAGE can immediately generate embeddings for new nodes by simply sampling their neighbors, without any retraining or recomputation.\n\n### Aggregation\n\nAnother key idea is the *aggregation*. GraphSAGE makes a distinction between self-information and neighborhood information. While previous GNNs treat them equally and aggregate them, GraphSAGE treats them differently. Specifically, GraphSAGE introduces an additional step: it concatenates the self-information and the neighborhood information as the input of the convolution.\n\n$$\nZ_v = \\text{CONCAT}(X_v, X_{\\mathcal{N}(v)})\n$$\n\nwhere $X_v$ is the feature of the node itself and $X_{\\mathcal{N}(v)}$ is the aggregation of the features of its neighbors. GraphSAGE introduces different ways to aggregate information from neighbors:\n\n   $$X_{\\mathcal{N}(v)} = \\text{AGGREGATE}_k(\\{X_u, \\forall u \\in \\mathcal{N}(v)\\})$$\n\n   Common aggregation functions include:\n   - Mean aggregator: $\\text{AGGREGATE} = \\text{mean}(\\{h_u, \\forall u \\in \\mathcal{N}(v)\\})$\n   - Max-pooling: $\\text{AGGREGATE} = \\max(\\{\\sigma(W_{\\text{pool}}h_u + b), \\forall u \\in \\mathcal{N}(v)\\})$\n   - LSTM aggregator: Apply LSTM to randomly permuted neighbors\n\nThe concatenated feature $Z_v$ is normalized by the L2 norm.\n\n$$\n\\hat{Z}_v = \\frac{Z_v}{\\|Z_v\\|_2}\n$$\n\nand then fed into the convolution.\n\n$$\nX_v^k = \\sigma(W^k \\hat{Z}_v + b^k)\n$$\n\n## Graph Attention Networks (GAT): Differentiate Individual Neighbors\n\nA key innovation of GraphSAGE is to treat the self and neighborhood information differently. But should all neighbors be treated equally? Graph Attention Networks (GAT) address this by letting the model learn which neighbors to pay attention to.\n\n\n\n### Attention Mechanism\n\n![](https://substackcdn.com/image/fetch/$s_!9JX2!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F1e4bb0f1-420a-4d61-a017-e6a998c9f4f3_1458x928.png)\n\nThe core idea is beautifully simple: instead of using fixed weights like GCN, let's learn attention weights $\\alpha_{ij}$ that determine how much node $i$ should attend to node $j$. These weights are computed dynamically based on node features:\n\n$$\n\\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k \\in \\mathcal{N}(i)} \\exp(e_{ik})}\n$$\n\nwhere $e_{ij}$ represents the importance of the edge between node $i$ and node $j$. Variable $e_{ij}$ is a *learnable* parameter and can be negative, and the exponential function is applied to transform it to a non-negative value, with the normalization term $\\sum_{k \\in \\mathcal{N}(i)} \\exp(e_{ik})$ to ensure the weights sum to 1.\n\nHow to compute $e_{ij}$? One simple choice is to use a neural network with a shared weight matrix $W$ and a LeakyReLU activation function. Specifically:\n\n1. Let's focus on computing $e_{ij}$ for node $i$ and its neighbor $j$.\n2. We use a shared weight matrix $W$ to transform the features of node $i$ and $j$.\n   $$\n   \\mathbf{\\tilde h}_i  = \\mathbf{h}_i, \\quad \\mathbf{\\tilde h}_j  = W\\mathbf{h}_j\n   $$\n3. We concatenate the transformed features and apply a LeakyReLU activation function.\n\n$$\ne_{ij} = \\text{LeakyReLU}(\\mathbf{a}^T[\\mathbf{\\tilde h}_i, \\mathbf{\\tilde h}_j])\n$$\n\nwhere $\\mathbf{a}$ is a trainable parameter vector that sums the two transformed features.\n\nOnce we have these attention weights, the node update is straightforward - just a weighted sum of neighbor features:\n\n$$\\mathbf{h}'_i = \\sigma\\left(\\sum_{j \\in \\mathcal{N}(i) \\cup \\{i\\}} \\alpha_{ij}{\\bf W}_{\\text{feature}}\\mathbf{h}_j\\right)$$\n\nwhere ${\\bf W}_{\\text{feature}}$ is a trainable weight matrix. To stabilize training, GAT uses multiple attention heads and concatenates their outputs:\n\n$$\\mathbf{h}'_i = \\parallel_{k=1}^K \\sigma\\left(\\sum_{j \\in \\mathcal{N}(i) \\cup \\{i\\}} \\alpha_{ij}^k{\\bf W}^k_{\\text{feature}}\\mathbf{h}_j\\right)$$\n\n## Graph Isomorphism Network (GIN): Differentiate the Aggregation\n\nGraph Isomorphism Networks (GIN) is another popular GNN that born out of a question: what is the maximum discriminative power achievable by Graph Neural Networks? The answer lies in its theoretical connection to **the Weisfeiler-Lehman (WL) test**, a powerful algorithm for graph isomorphism testing.\n\n\n### Weisfeiler-Lehman Test\n\nAre two graphs structurally identical? Graph isomorphism testing determines if two graphs are structurally identical, with applications in graph classification, clustering, and other tasks.\n\n![](https://i.sstatic.net/j5sGu.png)\n\nWhile the general problem has no known polynomial-time solution, the WL test is an efficient heuristic that works well in practice. The WL test iteratively refines node labels by hashing the multiset of neighboring labels\n\n\n![](../figs/weisfeiler-lehman-test.jpg)\n\nThe WL test works as follows:\n\n1. Assign all nodes the same initial label.\n2. For each node, collect the labels of all its neighbors and *aggregate them* into a hash (e.g., new label). For example, the top node gets {0} from its neighbors, resulting in a collection {0,0}. A new label is created via a hash function $h$ that maps {0, {0, 0}} to a new label 1.\n3. Repeat the process for a fixed number of iterations or until convergence.\nå\nAfter these iterations:\n- Nodes with the same label are structurally identical, meaning that they are indistinguishable unless we label them differently.\n- Two graphs are structurally identical if and only if they have the same node labels after the WL test.\n\nThe WL test is a heuristic and can fail on some graphs. For example, it cannot distinguish regular graphs with the same number of nodes and edges.\n\n::: {.column-margin}\nThe WL test above is called the 1-WL test. There are higher-order WL tests that can distinguish more graphs, which are the basis of advanced GNNs.\nCheck out [this note](https://www.moldesk.net/blog/weisfeiler-lehman-isomorphism-test/)\n:::\n\n### GIN\n\nGIN [@xu2018how] is a GNN that is based on the WL test.\nThe key idea is to focus on the parallel between the WL test and the GNN update rule.\n- In the WL test, we iteratively collect the labels of neighbors and aggregate them through a *hash function*.\n- In the GraphSAGE and GAT, the labels are the nodes' features, and the aggregation is some arithmetic operations such as mean or max.\n\nThe key difference is that the hash function in the WL test always distinguishes different sets of neighbors' labels, while the aggregation in GraphSAGE and GAT does not always do so. For example, if all nodes have the same feature (e.g., all 1), the aggregation by the mean or max will result in the same value for all nodes, whereas the hash function in the WL test can still distinguish different sets of neighbors' labels by *the count of each label*.\n\nThe resulting convolution update rule is:\n\n$$\nh_v^{(k+1)} = \\text{MLP}^{(k)}\\left((1 + \\epsilon^{(k)}) \\cdot h_v^{(k)} + \\sum_{u \\in \\mathcal{N}(v)} h_u^{(k)}\\right)\n$$\n\nwhere $\\text{MLP}^{(k)}$ is a multi-layer perceptron (MLP) with $k$ layers, and $\\epsilon^{(k)}$ is a fixed or trainable parameter.\n\n",
    "supporting": [
      "01-concepts_files"
    ],
    "filters": [],
    "includes": {}
  }
}