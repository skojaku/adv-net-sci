{
  "hash": "927b45132ac53768b785e36a992feba6",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Preparation - Image processing\"\njupyter: advnetsci\nexecute:\n    enabled: true\n---\n\n\n\n## Preliminaries: Image Processing\n\nGraph Neural Networks are a type of neural network for graph data. node2vec and deepwalk stem from the idea of language modeling.\nIn this module, we will focus on another branch of graph neural networks that stem from image processing.\n\n### Pen and paper exercises\n\n- [‚úçÔ∏è Pen and paper exercises](pen-and-paper/exercise.pdf)\n\nThe pen and paper exercises will help you understand the mathematical foundations of graph neural networks, including:\n\n1. **Spectral Graph Theory**: Understanding eigenvalues and eigenvectors of graph matrices\n2. **Fourier Analysis on Graphs**: Extending classical signal processing to graph domains\n3. **Convolution Operations**: Defining convolution for irregular graph structures\n4. **Message Passing**: Mathematical formulation of information aggregation in graphs\n5. **Network Architecture Design**: Principles for designing effective GNN architectures\n\nThese exercises provide the theoretical foundation necessary to understand how graph neural networks process and learn from graph-structured data.\n\n## Edge Detection Problem in Image Processing\n\nEdge detection is a classical problem in image processing. The goal is to identify the boundaries of objects in an image.\n\n![](https://media.geeksforgeeks.org/wp-content/uploads/20240616211411/Screenshot-(85).webp)\n\nTo approach the problem, let us first remind that an image is a matrix of pixels. Each pixel has RGB values, each of which represents the intensity of red, green, and blue color. To simplify the problem, we focus on grayscale images, in which each pixel has only one value representing the brightness. In this case, an image can be represented as a 2D matrix, where each element in the matrix represents the brightness of a pixel.\n\n![](https://ai.stanford.edu/~syyeung/cvweb/Pictures1/imagematrix.png)\n\n### An example\n\nHuman eyes are very sensitive to brightness changes. An edge in an image appears when there is a *significant brightness change between adjacent pixels*. To be more concrete, let's consider a small example consisting of 6x6 pixels, with a vertical line from the top to the bottom, where the brightness is higher than the neighboring pixels. This is an edge we want to detect.\n\n$$\nX = \\begin{bmatrix}\n10 & 10 & 80 & 10 & 10 & 10 \\\\\n10 & 10 & 80 & 10 & 10 & 10 \\\\\n10 & 10 & 80 & 10 & 10 & 10 \\\\\n10 & 10 & 80 & 10 & 10 & 10 \\\\\n10 & 10 & 80 & 10 & 10 & 10 \\\\\n10 & 10 & 80 & 10 & 10 & 10\n\\end{bmatrix}\n$$\n\nLet's zoom on the pixel at (3, 3) and its surrounding pixels.\n\n$$\nZ = \\begin{bmatrix}\n10 & 80 & 10 \\\\\n\\textcolor{blue}{10} & \\textcolor{red}{80} & \\textcolor{purple}{10} \\\\\n10 & 80 & 10\n\\end{bmatrix}\n$$\n\nwhere the central pixel is highlighted in red. Since we are interested in the edge which is a sudden change in brightness along the horizontal direction, we take a derivative at the central pixel by\n\n$$\n\\nabla Z_{22} = \\textcolor{blue}{Z_{2,1}} - \\textcolor{purple}{Z_{2,3}}\n$$\n\nFollowing the same process, we can compute the derivative at all pixels, which gives us the (horizontal) derivative of the image.\n\n$$\n\\begin{bmatrix}\n- & -70 & 0 & 70 & 0 & - \\\\\n- & -70 & 0 & 70 & 0 & - \\\\\n- & -70 & 0 & 70 & 0 & - \\\\\n- & -70 & 0 & 70 & 0 & - \\\\\n- & -70 & 0 & 70 & 0 & -\n\\end{bmatrix}\n$$\n\nThe symbol `-` indicates that the derivative is not defined because one of the neighboring pixels is out of the image boundary.\nWe observe that the derivative is high at the edge and low elsewhere. This is a simple but effective way to detect edges in an image.\n\nWe can consider a derivative operator along the vertical direction that computes the difference between the vertical neighboring pixels.\n\n$$\n\\nabla Z_{22} = Z_{1,2} - Z_{3,2}\n$$\n\nAnd, when applied to the entire image, the result is\n\n$$\n\\begin{bmatrix}\n- & - & - & - & -  & - \\\\\n0 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 0 \\\\\n- & - & - & - & - & -\n\\end{bmatrix}\n$$\n\nThe all entries are zero, meaning that there is no edge in the vertical direction.\n\nWe can combine the horizontal and vertical derivatives to get the gradient of the image. For example,\n\n$$\n\\nabla Z_{22} = Z_{12} - Z_{32} + Z_{21} - Z_{23}\n$$\n\nWhen applied to the entire image, the result is the same as the horizontal derivative.\n\n### Convolution\n\nWe observe that there is a repeated pattern in the derivative computation: we are taking addition and subtraction of neighbiring pixels. This motivates us to generalize the operation to a more general form.\n\n$$\n\\nabla Z_{22} = \\sum_{i=-1}^1 \\sum_{j=-1}^1 K_{h-(i+1),w-(j+1)} Z_{2+i, 2+j}\n$$\n\nwhere $K$ is a $3 \\times 3$ matrix, and $w=h=3$ represent the width and height of the kernel.\n\n$$\nK = \\begin{bmatrix}\nK_{11} & K_{12} & K_{13} \\\\\nK_{21} & K_{22} & K_{23} \\\\\nK_{31} & K_{32} & K_{33}\n\\end{bmatrix}\n$$\n\nThe matrix $K$ is called a **kernel**, and applying it to the image is called **convolution**.\n\n::: {.column-margin}\nThe index of the kernel is conventionally reversed. Namely, we reorder the entries of the kernel such that\n\n$$\n\\begin{bmatrix}\nK_{33} & K_{32} & K_{31} \\\\\nK_{23} & K_{22} & K_{21} \\\\\nK_{13} & K_{12} & K_{11}\n\\end{bmatrix}\n$$\n\nThen, take the element-wise product with $Z$\n\n$$\n\\begin{bmatrix}\nZ_{11} K_{33} & Z_{12} K_{32} & Z_{13} K_{31} \\\\\nZ_{21} K_{23} & Z_{22} K_{22} & Z_{23} K_{21} \\\\\nZ_{31} K_{13} & Z_{32} K_{12} & Z_{33} K_{11}\n\\end{bmatrix}\n$$\n\nand sum up all the elements to get the new pixel value $\\nabla Z_{22}$.\nWhy do we reverse the kernel? This is to match with the mathematical definition of convolution, which will be introduced later.\n:::\n\n::: {.column-margin}\nIn the previous example, we used a $3 \\times 3$ kernels called the Prewitt operator, which in terms of $K$ is\n\n$$\nK_h = \\begin{bmatrix}\n-1 & 0 & 1 \\\\\n-1 & 0 & 1 \\\\\n-1 & 0 & 1\n\\end{bmatrix}\n\\quad \\text{or} \\quad\nK_v = \\begin{bmatrix}\n-1 & -1 & -1 \\\\\n0 & 0 & 0 \\\\\n1 & 1 & 1\n\\end{bmatrix}\n$$\n\nwhere $K_h$ is the horizontal Prewitt operator and $K_v$ is the vertical Prewitt operator.\n:::\n\nA kernel represents a local pattern we want to detect. The new pixel value after the convolution is maximized when the pattern is most similar to the kernel in terms of the inner product. This can be confirmed by:\n\n$$\n\\nabla Z_{22} = \\sum_{i=-1}^1 \\sum_{j=-1}^1 K_{h-(i+1),w-(j+1)} Z_{2+i, 2+j} = \\langle \\hat K, Z \\rangle\n$$\n\nwhere $\\langle \\cdot, \\cdot \\rangle$ is the inner product, and $\\hat K$ is the order-reversed kernel.\n\n::: {.column-margin}\nCheck out this awesome interactive demo to see how different kernels work: [Demo](https://setosa.io/ev/image-kernels/)\n:::\n\n## Fourier Transform\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*D6iRfzDkz-sEzyjYoVZ73w.gif)\n\nConvolution computes the new pixel values by sliding a kernel over an image. How is the resulting image related to the original image?\n\n\n\nTo answer this question, let us consider a row of an image and convolve it with a kernel $K$.\n\n$$\n\\begin{aligned}\nX &= \\begin{bmatrix}\nX_1 & X_2 & X_3 & X_4 & X_5 & X_6\n\\end{bmatrix} \\\\\nK &= \\begin{bmatrix}\nK_1 & K_2 & K_3\n\\end{bmatrix}\n\\end{aligned}\n$$\n\nThe convolution of $X$ and $K$ is\n\n$$\nX * K = \\begin{bmatrix}\nX_1 K_3 + X_2 K_2 + X_3 K_1 & X_2 K_3 + X_3 K_2 + X_4 K_1 & X_3 K_3 + X_4 K_2 + X_5 K_1 & X_4 K_3 + X_5 K_2 + X_6 K_1\n\\end{bmatrix}\n$$\n\n...which is complicated, right? üòÖ So let's make it simple by using a useful theorem called **the convolution theorem**.\n\nThe convolution theorem gives us a simpler way to think about convolution. Instead of doing the complex sliding window operation in the original domain (like pixel values), we can:\n\n1. Transform both signals to the frequency domain using Fourier transform\n2. Multiply them together (much simpler!)\n3. Transform back to get the same result\n\nMathematically, the above steps can be written as:\n\n1. $\\mathcal{F}(X), \\mathcal{F}(K)$ - Transform both signals to frequency domain (Fourier transform)\n2. $\\mathcal{F}(X) \\cdot \\mathcal{F}(K)$ - Multiply the transformed signals\n3. $\\mathcal{F}^{-1}(\\mathcal{F}(X) \\cdot \\mathcal{F}(K))$ - Transform back to get $X * K$\n\nwhere $\\mathcal{F}^{-1}$ is the inverse Fourier transform that brings us back to the original domain. This is much easier than computing the convolution directly!\n\nFor a discrete signal $x[n]$ with $N$ points, the Fourier transform $\\mathcal{F}$ is defined as:\n\n$$\n\\mathcal{F}(x)[k] = \\sum_{n=0}^{N-1} x[n] \\cdot e^{-2\\pi i \\frac{nk}{N}}\n$$\n\nwhere $i$ is the imaginary unit. Or equivalently,\n\n$$\n\\mathcal{F}(x)[k] = \\sum_{n=0}^{N-1} x[n] \\cdot \\left[ \\cos\\left(2\\pi \\frac{nk}{N}\\right) - i \\sin\\left(2\\pi \\frac{nk}{N}\\right) \\right]\n$$\n\nusing Euler's formula $e^{ix} = \\cos(x) + i\\sin(x)$.\n\n::: {.column-margin}\nComplex number can be thought of as a way to represent a 2D vector using a single value (which is a computer science perspective; mathematically, it is a bit more subtle). For example, $e^{i\\pi/2} = \\cos(\\pi/2) + i\\sin(\\pi/2)$ represents the 2D vector $(\\cos(\\pi/2), \\sin(\\pi/2))$. In the context of Fourier transform, we interpret $e^{-2\\pi i \\frac{nk}{N}}$ as two *base waves*, i.e., sine and cosine, with phase $\\frac{2\\pi k}{N}$.\n\n![](https://upload.wikimedia.org/wikipedia/commons/thumb/7/71/Euler%27s_formula.svg/360px-Euler%27s_formula.svg.png)\n:::\n\nIn simple terms, $\\mathcal{F}$ takes a signal (like our row of pixel values) and breaks it down into sine and cosine waves of different frequencies. Each frequency component $k$ tells us \"how much\" of that frequency exists in our original signal.\nDon't worry too much about the complex math. The key idea is that $\\mathcal{F}$ represents a signal as a sum of multiple waves with different frequencies, so we can understand the signal in terms of its frequencies rather than its original values.\n\n\n![](https://devincody.github.io/Blog/post/an_intuitive_interpretation_of_the_fourier_transform/img/FFT-Time-Frequency-View_hu24c1c8fe894ecd0dad24174b2bed08c9_99850_800x0_resize_lanczos_2.png)\n\n\n::: {.column-margin}\n3Blue1Brown makes a beautiful video explaining Fourier transform: [Video](https://www.youtube.com/watch?v=spUNpyF58BY). Here is a great interactive demo on Fourier transform by Jez Swanson: [Demo](https://www.jezzamon.com/fourier/).\n:::\n\n### An example for the Fourier transform\n\nNow, let's perform the convolution using the Fourier transform using an example.\n\n\n\nLet us first perform the convolution directly.\n\n::: {#d4714864 .cell execution_count=2}\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\narray([-70.,   0.,  70.,   0.])\n```\n:::\n:::\n\n\nLet us now perform the convolution using the Fourier transform. We compute the Fourier transform of $X$ and $K$.\n\n::: {#c65b475f .cell execution_count=3}\n``` {.python .cell-code}\n# Step 1: Transform X and K to frequency domain\nFX = np.fft.fft(X)\n# Pad K with zeros to match the length of X before FFT\nK_padded = np.pad(K, (0, len(X) - len(K)), 'constant') # [-1  0  1  0  0  0]\nFK = np.fft.fft(K_padded)\nprint(\"FX:\", FX)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFX: [130.+0.00000000e+00j -35.-6.06217783e+01j -35.+6.06217783e+01j\n  70.+7.10542736e-15j -35.-6.06217783e+01j -35.+6.06217783e+01j]\n```\n:::\n:::\n\n\n- We add zeros to $K$ to make it the same length as $X$ before applying the Fourier transform. This is necessary because the convolution theorem requires the signals to have the same length.\n- `FX` is the Fourier transform of $X$, which is a complex number. Each entry $FX[k]$ represents the weight of the cosine wave in its real part and the weight of the sine wave in its imaginary part, with phase $2\\pi k / N$. Similarly for `FK`.\n\nNext, we multiply the transformed signals.\n\n::: {#47a61548 .cell execution_count=4}\n``` {.python .cell-code}\nFXKconv = FX * FK\n```\n:::\n\n\nThis is the convolution in the frequency domain. Finally, we transform back to get the convolution.\n\n::: {#352d09cd .cell execution_count=5}\n``` {.python .cell-code}\nXKconv_ft = np.real(np.fft.ifft(FXKconv))\nXKconv_ft\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\narray([ 2.84957243e-15,  3.05036092e-15, -7.00000000e+01,  1.88737914e-15,\n        7.00000000e+01, -1.05195948e-15])\n```\n:::\n:::\n\n\n- We take the real part. The imaginary part is due to numerical artifacts that do not matter in practice.\n- The Fourier transform convolution produces a longer output than direct convolution because it includes partial overlaps between K and X at the boundaries. Since we only want the full overlaps, we need to truncate the first two elements of `XKconv_ft` (as K has length 3) to match the length of the direct convolution result.\n- For example, let's look at what happens at the beginning of the convolution:\n  - At position -2: Only the last element of K overlaps with X: `[0, 0, 10] * [-1, 0, 1] = 10`\n  - At position -1: Two elements of K overlap with X: `[0, 10, 10] * [-1, 0, 1] = 10`\n  - At position 0: Full overlap begins: `[10, 10, 80] * [-1, 0, 1] = 70`\n\n  The Fourier transform method gives us all these positions (-2, -1, 0, ...), but we only want the full overlaps starting from position 0, which is why we truncate the first two elements.\n\n::: {#9cd02ea5 .cell execution_count=6}\n``` {.python .cell-code}\nXKconv_ft = XKconv_ft[2:]\nXKconv_ft\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\narray([-7.00000000e+01,  1.88737914e-15,  7.00000000e+01, -1.05195948e-15])\n```\n:::\n:::\n\n\nThis gives us the same result as the direct convolution up to numerical errors.\n\n## Fourier Transform of Images\n\nLet's extend the above example to an image which is a 2D matrix.\nThe idea is the same: we take the Fourier transform of each row and column of the image, and then multiply them together to get the convolution in the frequency domain.\nMore specifically, for an image $X$ with size $H \\times W$, the Fourier transform of $X$ is\n\n$$\n\\begin{aligned}\n\\mathcal{F}(X)[h, w] &= \\sum_{k=0}^{H-1} \\sum_{\\ell=0}^{W-1} X[k, \\ell] \\cdot e^{-2\\pi i \\frac{hk}{H}} \\cdot e^{-2\\pi i \\frac{w\\ell}{W}} \\\\\n&= \\sum_{k=0}^{H-1} \\sum_{\\ell=0}^{W-1} X[k, \\ell] e^{-2\\pi i \\left(\\frac{hk}{H} + \\frac{w\\ell}{W}\\right)}\n\\end{aligned}\n$$\n\nComparing with the 1D case, we see that the 2D Fourier transform is *functionally* the same as the 1D Fourier transform, except that we now have two indices $h$ and $w$ to represent the frequency in both dimensions.\nThe basis waves are 2D waves as shown below.\n\n**Cosine waves**\n\n::: {#46c2210d .cell execution_count=7}\n\n::: {.cell-output .cell-output-display}\n![](00-preparation_files/figure-html/cell-8-output-1.png){}\n:::\n:::\n\n\n**Sine waves**\n\n::: {#3cb51482 .cell execution_count=8}\n\n::: {.cell-output .cell-output-display}\n![](00-preparation_files/figure-html/cell-9-output-1.png){}\n:::\n:::\n\n\nThe Fourier transform of an image is a decomposition of an image into the sum of these basis waves.\n\n### An example of Fourier transform\n\nLet us apply the Fourier transform to an image.\n\n::: {#f1917570 .cell execution_count=9}\n\n::: {.cell-output .cell-output-display}\n![](00-preparation_files/figure-html/cell-10-output-1.png){}\n:::\n:::\n\n\nTake the Fourier transform of the image.\n\n::: {#d4e2e5cc .cell execution_count=10}\n``` {.python .cell-code}\nft_img_gray = np.fft.fft2(img_gray)\n```\n:::\n\n\nThis decomposes the image into a sum of basis waves. Let's see the weights of the basis waves.\n\n::: {#1325e2fc .cell execution_count=11}\n\n::: {.cell-output .cell-output-display}\n![](00-preparation_files/figure-html/cell-12-output-1.png){}\n:::\n:::\n\n\nThe corresponding basis waves look like this:\n\n::: {#ba7f523f .cell execution_count=12}\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\nText(0.5, 0.98, 'Imaginary Part of Basis Functions')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](00-preparation_files/figure-html/cell-13-output-2.png){}\n:::\n\n::: {.cell-output .cell-output-display}\n![](00-preparation_files/figure-html/cell-13-output-3.png){}\n:::\n:::\n\n\nNow, let's see the convolution of the image with a Prewitt operator.\n\n::: {#68af6639 .cell execution_count=13}\n``` {.python .cell-code}\nK = np.array([[-1, -1, -1], [0, 0, 0], [1, 1, 1]]) # Prewitt operator\n\nK_padd = np.zeros((img_gray.shape[0], img_gray.shape[1]))\nK_padd[:K.shape[0], :K.shape[1]] = K\n\n# convolution\nFK = np.fft.fft2(K_padd)\n```\n:::\n\n\nThe Fourier transform of the Prewitt operator looks like this:\n\n::: {#be4240bf .cell execution_count=14}\n``` {.python .cell-code}\nplt.imshow(np.abs(FK), cmap='gray')\ncbar = plt.colorbar()\n```\n\n::: {.cell-output .cell-output-display}\n![](00-preparation_files/figure-html/cell-15-output-1.png){}\n:::\n:::\n\n\nWe can think of the frequency domain of the kernel as a **filter** that suppresses some frequencies and allows others to pass through. In the example of the Prewitt operator, the kernel `FK` has a low value around the center of the image. The product $FX \\cdot FK$ then suppresses the low-frequency components of the image, and we are left with the high-frequency components that correspond to the horizontal edges. We can think of this as a high-pass filter that only allows high-frequency components to pass through.\n\nLet's see the convolution result.\n\n::: {#a729e04b .cell execution_count=15}\n\n::: {.cell-output .cell-output-display}\n![](00-preparation_files/figure-html/cell-16-output-1.png){}\n:::\n:::\n\n\nWe observe that the horizontal edges are highlighted.\n\n::: {.column-margin}\nA widespread application of the 2D Fourier transform is JPEG format. Here's how it works:\n\n(1) It first breaks the image into small 8x8 squares.\n(2) It converts each square into frequencies using the Discrete Cosine Transform. The sine part is discarded for compression.\n(3) It keeps the important low frequencies that our eyes can see well.\n(4) It throws away most of the high frequencies that our eyes don't notice much.\n\nThese steps make the file much smaller while still looking good to us.\n:::\n\n\n\n## A key lesson from image processing\n\nWe have seen an equivalence between convolution in the pixel (spatial) domain and multiplication in the frequency domain.\nUsing the Fourier transform, an image is decomposed into a sum of basis waves.\nThe *kernel* can be thought of as *a filter* that suppresses some basis waves and allows others to pass through.\n\nThis idea is the key to understand graph convolutional networks we will see in the next page.\n\n",
    "supporting": [
      "00-preparation_files"
    ],
    "filters": [],
    "includes": {}
  }
}