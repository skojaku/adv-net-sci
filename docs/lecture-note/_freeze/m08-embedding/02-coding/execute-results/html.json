{
  "hash": "0ff9de9edd14ba589660647fd44d2b24",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Network Embedding Concepts\"\njupyter: advnetsci\nexecute:\n    enabled: true\n---\n\nIn this section, we implement the embedding methods discussed in the [concepts section](./01-concepts.md).\n\n## Data Preparation\n\nWe will use the karate club network throughout this notebook.\n\n::: {#a2112655 .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport igraph\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the karate club network\ng = igraph.Graph.Famous(\"Zachary\")\nA = g.get_adjacency_sparse()\n\n# Get community labels (Mr. Hi = 0, Officer = 1)\nlabels = np.array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\ng.vs[\"label\"] = labels\n\n# Visualize the network\npalette = sns.color_palette().as_hex()\nigraph.plot(g, vertex_color=[palette[label] for label in labels], bbox=(300, 300))\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n![](02-coding_files/figure-html/cell-2-output-1.svg){}\n:::\n:::\n\n\n## Spectral Embedding\n\n### Example: Spectral Embedding with Adjacency Matrix\n\nLet us demonstrate spectral embedding with the karate club network.\n\n::: {#e968b2bd .cell execution_count=2}\n``` {.python .cell-code}\n# Convert to dense array for eigendecomposition\nA_dense = A.toarray()\n```\n:::\n\n\n::: {#bb3c8231 .cell execution_count=3}\n``` {.python .cell-code}\n# Compute the spectral decomposition\neigvals, eigvecs = np.linalg.eig(A_dense)\n\n# Find the top d eigenvectors\nd = 2\nsorted_indices = np.argsort(eigvals)[::-1][:d]\neigvals = eigvals[sorted_indices]\neigvecs = eigvecs[:, sorted_indices]\n\n# Plot the results\nfig, ax = plt.subplots(figsize=(7, 5))\nsns.scatterplot(x = eigvecs[:, 0], y = eigvecs[:, 1], hue=labels, ax=ax)\nax.set_title('Spectral Embedding')\nax.set_xlabel('Eigenvector 1')\nax.set_ylabel('Eigenvector 2')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/Users/skojaku-admin/miniforge3/envs/advnetsci/lib/python3.11/site-packages/matplotlib/cbook.py:1709: ComplexWarning: Casting complex values to real discards the imaginary part\n  return math.isfinite(val)\n/Users/skojaku-admin/miniforge3/envs/advnetsci/lib/python3.11/site-packages/matplotlib/cbook.py:1709: ComplexWarning: Casting complex values to real discards the imaginary part\n  return math.isfinite(val)\n/Users/skojaku-admin/miniforge3/envs/advnetsci/lib/python3.11/site-packages/pandas/core/dtypes/astype.py:133: ComplexWarning: Casting complex values to real discards the imaginary part\n  return arr.astype(dtype, copy=True)\n/Users/skojaku-admin/miniforge3/envs/advnetsci/lib/python3.11/site-packages/pandas/core/dtypes/astype.py:133: ComplexWarning: Casting complex values to real discards the imaginary part\n  return arr.astype(dtype, copy=True)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](02-coding_files/figure-html/cell-4-output-2.png){}\n:::\n:::\n\n\nInterestingly, the first eigenvector corresponds to the eigencentrality of the network, representing the centrality of the nodes.\nThe second eigenvector captures the community structure of the network, clearly separating the two communities in the network.\n\n### Example: Modularity Embedding\n\nWe can use the modularity matrix to generate a low-dimensional embedding of the network.\n\n::: {#6ae73a74 .cell execution_count=4}\n``` {.python .cell-code}\ndeg = np.sum(A_dense, axis=1)\nm = np.sum(deg) / 2\nQ = A_dense - np.outer(deg, deg) / (2 * m)\nQ/= 2*m\n\neigvals, eigvecs = np.linalg.eig(Q)\n\n# Sort the eigenvalues and eigenvectors\nsorted_indices = np.argsort(-eigvals)[:d]\neigvals = eigvals[sorted_indices]\neigvecs = eigvecs[:, sorted_indices]\n\nfig, ax = plt.subplots(figsize=(7, 5))\nsns.scatterplot(x = eigvecs[:, 0], y = eigvecs[:, 1], hue=labels, ax=ax)\nax.set_title('Modularity Embedding')\nax.set_xlabel('Eigenvector 1')\nax.set_ylabel('Eigenvector 2')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/Users/skojaku-admin/miniforge3/envs/advnetsci/lib/python3.11/site-packages/matplotlib/cbook.py:1709: ComplexWarning: Casting complex values to real discards the imaginary part\n  return math.isfinite(val)\n/Users/skojaku-admin/miniforge3/envs/advnetsci/lib/python3.11/site-packages/matplotlib/cbook.py:1709: ComplexWarning: Casting complex values to real discards the imaginary part\n  return math.isfinite(val)\n/Users/skojaku-admin/miniforge3/envs/advnetsci/lib/python3.11/site-packages/pandas/core/dtypes/astype.py:133: ComplexWarning: Casting complex values to real discards the imaginary part\n  return arr.astype(dtype, copy=True)\n/Users/skojaku-admin/miniforge3/envs/advnetsci/lib/python3.11/site-packages/pandas/core/dtypes/astype.py:133: ComplexWarning: Casting complex values to real discards the imaginary part\n  return arr.astype(dtype, copy=True)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](02-coding_files/figure-html/cell-5-output-2.png){}\n:::\n:::\n\n\n### Example: Laplacian Eigenmap\n\nLet us first compute the Laplacian matrix and its eigenvectors.\n\n::: {#a102073b .cell execution_count=5}\n``` {.python .cell-code}\nD = np.diag(np.sum(A_dense, axis=1))\nL = D - A_dense\n\neigvals, eigvecs = np.linalg.eig(L)\n\n# Sort the eigenvalues and eigenvectors\nsorted_indices = np.argsort(eigvals)[1:d+1]  # Exclude the first eigenvector\neigvals = eigvals[sorted_indices]\neigvecs = eigvecs[:, sorted_indices]\n\n# Plot the results\nfig, ax = plt.subplots(figsize=(7, 5))\nsns.scatterplot(x = eigvecs[:, 0], y = eigvecs[:, 1], hue=labels, ax=ax)\nax.set_title('Laplacian Eigenmap')\nax.set_xlabel('Eigenvector 2')\nax.set_ylabel('Eigenvector 3')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](02-coding_files/figure-html/cell-6-output-1.png){}\n:::\n:::\n\n\n## Neural Embedding with word2vec\n\n### Example: word2vec with Text\n\nTo showcase the effectiveness of word2vec, let's walk through an example using the `gensim` library.\n\n::: {#765e20fd .cell execution_count=6}\n``` {.python .cell-code}\nimport gensim\nimport gensim.downloader\nfrom gensim.models import Word2Vec\n\n# Load pre-trained word2vec model from Google News\nmodel = gensim.downloader.load('word2vec-google-news-300')\n```\n:::\n\n\nOur first example is to find the words most similar to *king*.\n\n::: {#36f68d12 .cell execution_count=7}\n``` {.python .cell-code}\n# Example usage\nword = \"king\"\nsimilar_words = model.most_similar(word)\nprint(f\"Words most similar to '{word}':\")\nfor similar_word, similarity in similar_words:\n    print(f\"{similar_word}: {similarity:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWords most similar to 'king':\nkings: 0.7138\nqueen: 0.6511\nmonarch: 0.6413\ncrown_prince: 0.6204\nprince: 0.6160\nsultan: 0.5865\nruler: 0.5798\nprinces: 0.5647\nPrince_Paras: 0.5433\nthrone: 0.5422\n```\n:::\n:::\n\n\nA cool (yet controversial) application of word embeddings is analogy solving. Let us consider the following puzzle:\n\n> *man* is to *woman* as *king* is to ___ ?\n\nWe can use word embeddings to solve this puzzle.\n\n::: {#397f91a9 .cell execution_count=8}\n``` {.python .cell-code}\n# We solve the puzzle by\n#\n#  vec(king) - vec(man) + vec(woman)\n#\n# To solve this, we use the model.most_similar function, with positive words being \"king\" and \"woman\" (additive), and negative words being \"man\" (subtractive).\n#\nmodel.most_similar(positive=['woman', \"king\"], negative=['man'], topn=5)\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\n[('queen', 0.7118192911148071),\n ('monarch', 0.6189674735069275),\n ('princess', 0.5902431011199951),\n ('crown_prince', 0.5499460697174072),\n ('prince', 0.5377321243286133)]\n```\n:::\n:::\n\n\nThe last example is to visualize the word embeddings.\n\n::: {#fb225004 .cell execution_count=9}\n\n::: {.cell-output .cell-output-display}\n![](02-coding_files/figure-html/cell-10-output-1.png){}\n:::\n:::\n\n\nWe can see that word2vec places the words representing countries close to each other and so do the words representing their capitals. The country-capital relationship is also roughly preserved, e.g., *Germany*-*Berlin* vector is roughly parallel to *France*-*Paris* vector.\n\n## Graph Embedding with word2vec\n\n### DeepWalk: Learning Network Embeddings via Random Walks\n\nDeepWalk treats random walks on a graph as \"sentences\" and applies word2vec to learn node embeddings. The key insight is that nodes appearing in similar contexts (neighborhoods) should have similar embeddings.\n\n::: {.column-margin}\nDeepWalk was introduced by Perozzi et al. (2014) and was one of the first methods to successfully apply natural language processing techniques to graph embedding.\n:::\n\n#### Step 1: Generate Random Walks\n\nThe first step is to generate training data for word2vec. We do this by sampling random walks from the network. Each random walk is like a \"sentence\" where nodes are \"words\".\n\nLet's implement a function to sample random walks. A random walk starts at a node and repeatedly moves to a random neighbor until it reaches the desired length.\n\n::: {#aa6db027 .cell execution_count=10}\n``` {.python .cell-code}\ndef random_walk(net, start_node, walk_length):\n    \"\"\"\n    Generate a random walk starting from start_node.\n\n    Parameters:\n    -----------\n    net : sparse matrix\n        Adjacency matrix of the network\n    start_node : int\n        Starting node for the walk\n    walk_length : int\n        Length of the walk\n\n    Returns:\n    --------\n    walk : list\n        List of node indices representing the random walk\n    \"\"\"\n    walk = [start_node]\n\n    while len(walk) < walk_length:\n        cur = walk[-1]\n        cur_nbrs = list(net[cur].indices)\n\n        if len(cur_nbrs) > 0:\n            # Randomly choose one of the neighbors\n            walk.append(np.random.choice(cur_nbrs))\n        else:\n            # Dead end - terminate the walk\n            break\n\n    return walk\n```\n:::\n\n\n::: {.column-margin}\nIn practice, we generate multiple walks per node to ensure each node appears in various contexts, which helps the model learn better representations.\n:::\n\nNow we generate multiple random walks starting from each node. We'll use 10 walks per node, each of length 50.\n\n::: {#320e01b9 .cell execution_count=11}\n``` {.python .cell-code}\nn_nodes = g.vcount()\nn_walkers_per_node = 10\nwalk_length = 50\n\nwalks = []\nfor i in range(n_nodes):\n    for _ in range(n_walkers_per_node):\n        walks.append(random_walk(A, i, walk_length))\n\nprint(f\"Generated {len(walks)} random walks\")\nprint(f\"Example walk: {walks[0][:10]}...\")  # Show first 10 nodes of first walk\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGenerated 340 random walks\nExample walk: [0, 31, 32, 31, 0, 21, 1, 21, 0, 4]...\n```\n:::\n:::\n\n\n#### Step 2: Train the Word2Vec Model\n\nNow we feed the random walks to the word2vec model. The model will learn to predict which nodes appear together in the same walk, similar to how it learns which words appear together in sentences.\n\n::: {#090bd09d .cell execution_count=12}\n``` {.python .cell-code}\nfrom gensim.models import Word2Vec\n\nmodel = Word2Vec(\n    walks,\n    vector_size=32,   # Dimension of the embedding vectors\n    window=3,         # Maximum distance between current and predicted node\n    min_count=1,      # Minimum frequency for a node to be included\n    sg=1,             # Use skip-gram model (vs CBOW)\n    hs=1              # Use hierarchical softmax for training\n)\n```\n:::\n\n\n::: {.column-margin}\n**Key Parameters:**\n\n- `vector_size`: Higher dimensions capture more information but require more data and computation.\n- `window`: Larger windows capture broader context but may dilute local structure.\n- `sg=1`: Skip-gram predicts context from target. It works better for small datasets.\n- `hs=1`: Hierarchical softmax is faster than negative sampling for small vocabularies.\n:::\n\nThe `window` parameter is crucial. For a random walk `[0, 1, 2, 3, 4, 5, 6, 7]`, when `window=3`, the context of node 2 includes nodes `[0, 1, 3, 4, 5]` - all nodes within distance 3.\n\n#### Step 3: Extract Node Embeddings\n\nAfter training, we can extract the learned embeddings for each node from the model.\n\n::: {#920fbceb .cell execution_count=13}\n``` {.python .cell-code}\n# Extract embeddings for all nodes\nembedding = np.array([model.wv[i] for i in range(n_nodes)])\n\nprint(f\"Embedding matrix shape: {embedding.shape}\")\nprint(f\"First node embedding (first 5 dimensions): {embedding[0][:5]}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEmbedding matrix shape: (34, 32)\nFirst node embedding (first 5 dimensions): [-0.08491512 -0.25203085  0.7852992  -0.00493573  0.07329542]\n```\n:::\n:::\n\n\n#### Step 4: Visualize Embeddings\n\nLet's visualize the learned embeddings in 2D using UMAP (Uniform Manifold Approximation and Projection). UMAP reduces the 32-dimensional embeddings to 2D while preserving the local structure.\n\n::: {.column-margin}\nUMAP is a dimensionality reduction technique that preserves both local and global structure better than t-SNE, making it ideal for visualizing high-dimensional embeddings.\n:::\n\n::: {#0aa3211e .cell execution_count=14}\n\n::: {.cell-output .cell-output-stderr}\n```\n/Users/skojaku-admin/miniforge3/envs/advnetsci/lib/python3.11/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n  warn(\nOMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n    <style>\n        .bk-notebook-logo {\n            display: block;\n            width: 20px;\n            height: 20px;\n            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n        }\n    </style>\n    <div>\n        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n        <span id=\"e13a5654-7a72-4763-995d-51a3867d54bb\">Loading BokehJS ...</span>\n    </div>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<script type=\"application/javascript\">\n'use strict';\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\nconst JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    function drop(id) {\n      const view = Bokeh.index.get_by_id(id)\n      if (view != null) {\n        view.model.document.clear()\n        Bokeh.index.delete(view)\n      }\n    }\n\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n\n    // Clean up Bokeh references\n    if (id != null) {\n      drop(id)\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim()\n            drop(id)\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded(error = null) {\n    const el = document.getElementById(\"e13a5654-7a72-4763-995d-51a3867d54bb\");\n    if (el != null) {\n      const html = (() => {\n        if (typeof root.Bokeh === \"undefined\") {\n          if (error == null) {\n            return \"BokehJS is loading ...\";\n          } else {\n            return \"BokehJS failed to load.\";\n          }\n        } else {\n          const prefix = `BokehJS ${root.Bokeh.version}`;\n          if (error == null) {\n            return `${prefix} successfully loaded.`;\n          } else {\n            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n          }\n        }\n      })();\n      el.innerHTML = html;\n\n      if (error != null) {\n        const wrapper = document.createElement(\"div\");\n        wrapper.style.overflow = \"auto\";\n        wrapper.style.height = \"5em\";\n        wrapper.style.resize = \"vertical\";\n        const content = document.createElement(\"div\");\n        content.style.fontFamily = \"monospace\";\n        content.style.whiteSpace = \"pre-wrap\";\n        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n        content.textContent = error.stack ?? error.toString();\n        wrapper.append(content);\n        el.append(wrapper);\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(() => display_loaded(error), 100);\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.8.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.8.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.8.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.8.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.8.0.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n      try {\n            for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n\n      } catch (error) {display_loaded(error);throw error;\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"e13a5654-7a72-4763-995d-51a3867d54bb\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));\n</script>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n  <div id=\"a6e0ea8d-db09-4473-aee8-0afe001bd8c6\" data-root-id=\"p1011\" style=\"display: contents;\"></div>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<script type=\"application/javascript\">\n(function(root) {\n  function embed_document(root) {\n  const docs_json = {\"6675cdba-2a86-447b-876a-bad8b0c45de9\":{\"version\":\"3.8.0\",\"title\":\"Bokeh Application\",\"config\":{\"type\":\"object\",\"name\":\"DocumentConfig\",\"id\":\"p1055\",\"attributes\":{\"notifications\":{\"type\":\"object\",\"name\":\"Notifications\",\"id\":\"p1056\"}}},\"roots\":[{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"p1011\",\"attributes\":{\"x_range\":{\"type\":\"object\",\"name\":\"DataRange1d\",\"id\":\"p1012\"},\"y_range\":{\"type\":\"object\",\"name\":\"DataRange1d\",\"id\":\"p1013\"},\"x_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1021\"},\"y_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1022\"},\"title\":{\"type\":\"object\",\"name\":\"Title\",\"id\":\"p1014\",\"attributes\":{\"text\":\"DeepWalk Node Embeddings (UMAP projection)\"}},\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1052\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1008\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1009\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1010\"},\"data\":{\"type\":\"map\",\"entries\":[[\"x\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"H4sIAAEAAAAC/wGIAHf/hmlOQKPlf0D2Q5ZAjcZ9QA5pE0BxlSBA5loPQK/hiUDC6KtAvUe3QNtmJkAtOkNAaCFoQCyvlECjBbxAfmuvQNXS9j+wjVhA2G+wQNSBiUCJfZ5AQstuQE4FpUBrEM5A2tXcQLlW6kBdzcFAK2/eQFIKzUCBechAzgiqQIhz4EDtZa1AAnioQH/4XeiIAAAA\"},\"shape\":[34],\"dtype\":\"float32\",\"order\":\"little\"}],[\"y\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"H4sIAAEAAAAC/wGIAHf/7GyVP3/YUT/9GUA/WRiwPoR9J0BdOCRAeCI3QPytAj+YY8pALoTMQJB7PkDTHD8/U8tEP+bBpD/qB/xAZV3uQJAgL0AzWhE/MRYCQXEjvz+iePNAPNuGP6b6+0COw/tAKiHjQCNr3UDN3gZBtqrQQC7QyEC7KQNBUSG7QNWj2kAZXOlA7frbQLSjaWeIAAAA\"},\"shape\":[34],\"dtype\":\"float32\",\"order\":\"little\"}],[\"size\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"H4sIAAEAAAAC/9NqcT2yUcrWQUHZJJj1iqmDhXdFmxeTuUNOyuyJMy8ZOsi375tZa6LpoAVWp4tBHzknOk3A1cChYd3pA/lTVNDUy8LF0dURomHm4JKHmQdTh64e5j5c7kf1n6UDAxjYOQAA2QugHBABAAA=\"},\"shape\":[34],\"dtype\":\"float64\",\"order\":\"little\"}],[\"community\",[\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\",\"#ff7f0e\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\",\"#ff7f0e\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\",\"#1f77b4\",\"#ff7f0e\",\"#1f77b4\",\"#ff7f0e\",\"#ff7f0e\",\"#ff7f0e\",\"#ff7f0e\",\"#ff7f0e\",\"#ff7f0e\",\"#ff7f0e\",\"#ff7f0e\",\"#ff7f0e\",\"#ff7f0e\",\"#ff7f0e\",\"#ff7f0e\"]]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1053\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1054\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1049\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"field\",\"field\":\"size\"},\"fill_color\":{\"type\":\"field\",\"field\":\"community\"},\"hatch_color\":{\"type\":\"field\",\"field\":\"community\"}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1050\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"field\",\"field\":\"size\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"fill_color\":{\"type\":\"field\",\"field\":\"community\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_color\":{\"type\":\"field\",\"field\":\"community\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1051\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"field\",\"field\":\"size\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"fill_color\":{\"type\":\"field\",\"field\":\"community\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_color\":{\"type\":\"field\",\"field\":\"community\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1020\",\"attributes\":{\"tools\":[{\"type\":\"object\",\"name\":\"PanTool\",\"id\":\"p1033\"},{\"type\":\"object\",\"name\":\"WheelZoomTool\",\"id\":\"p1034\",\"attributes\":{\"renderers\":\"auto\"}},{\"type\":\"object\",\"name\":\"BoxZoomTool\",\"id\":\"p1035\",\"attributes\":{\"dimensions\":\"both\",\"overlay\":{\"type\":\"object\",\"name\":\"BoxAnnotation\",\"id\":\"p1036\",\"attributes\":{\"syncable\":false,\"line_color\":\"black\",\"line_alpha\":1.0,\"line_width\":2,\"line_dash\":[4,4],\"fill_color\":\"lightgrey\",\"fill_alpha\":0.5,\"level\":\"overlay\",\"visible\":false,\"left\":{\"type\":\"number\",\"value\":\"nan\"},\"right\":{\"type\":\"number\",\"value\":\"nan\"},\"top\":{\"type\":\"number\",\"value\":\"nan\"},\"bottom\":{\"type\":\"number\",\"value\":\"nan\"},\"left_units\":\"canvas\",\"right_units\":\"canvas\",\"top_units\":\"canvas\",\"bottom_units\":\"canvas\",\"handles\":{\"type\":\"object\",\"name\":\"BoxInteractionHandles\",\"id\":\"p1042\",\"attributes\":{\"all\":{\"type\":\"object\",\"name\":\"AreaVisuals\",\"id\":\"p1041\",\"attributes\":{\"fill_color\":\"white\",\"hover_fill_color\":\"lightgray\"}}}}}}}},{\"type\":\"object\",\"name\":\"SaveTool\",\"id\":\"p1043\"},{\"type\":\"object\",\"name\":\"ResetTool\",\"id\":\"p1044\"},{\"type\":\"object\",\"name\":\"HelpTool\",\"id\":\"p1045\"}]}},\"left\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1028\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1029\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1030\"},\"axis_label\":\"UMAP 2\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1031\"}}}],\"below\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1023\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1024\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1025\"},\"axis_label\":\"UMAP 1\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1026\"}}}],\"center\":[{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1027\",\"attributes\":{\"axis\":{\"id\":\"p1023\"}}},{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1032\",\"attributes\":{\"dimension\":1,\"axis\":{\"id\":\"p1028\"}}}]}}]}};\n  const render_items = [{\"docid\":\"6675cdba-2a86-447b-876a-bad8b0c45de9\",\"roots\":{\"p1011\":\"a6e0ea8d-db09-4473-aee8-0afe001bd8c6\"},\"root_ids\":[\"p1011\"]}];\n  void root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n  }\n  if (root.Bokeh !== undefined) {\n    embed_document(root);\n  } else {\n    let attempts = 0;\n    const timer = setInterval(function(root) {\n      if (root.Bokeh !== undefined) {\n        clearInterval(timer);\n        embed_document(root);\n      } else {\n        attempts++;\n        if (attempts > 100) {\n          clearInterval(timer);\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n        }\n      }\n    }, 10, root)\n  }\n})(window);\n</script>\n```\n:::\n:::\n\n\nNotice how nodes from the same community (shown in the same color) tend to cluster together in the embedding space. This demonstrates that DeepWalk successfully captures the community structure.\n\n#### Step 5: Clustering with K-means\n\nOne practical application of node embeddings is clustering. While we have dedicated community detection methods like modularity maximization, embeddings allow us to use general machine learning algorithms like K-means.\n\n::: {.column-margin}\nThe advantage of using embeddings is that we can leverage the rich ecosystem of machine learning tools designed for vector data.\n:::\n\nLet's implement K-means clustering with automatic selection of the number of clusters using the silhouette score.\n\n::: {#f481e29e .cell execution_count=15}\n``` {.python .cell-code}\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\ndef find_optimal_clusters(embedding, n_clusters_range=(2, 10)):\n    \"\"\"\n    Find the optimal number of clusters using silhouette score.\n\n    The silhouette score measures how well each node fits within its cluster\n    compared to other clusters. Scores range from -1 to 1, where higher is better.\n    \"\"\"\n    silhouette_scores = []\n\n    for n_clusters in range(*n_clusters_range):\n        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n        cluster_labels = kmeans.fit_predict(embedding)\n        score = silhouette_score(embedding, cluster_labels)\n        silhouette_scores.append((n_clusters, score))\n        print(f\"k={n_clusters}: silhouette score = {score:.3f}\")\n\n    # Select the number of clusters with highest silhouette score\n    optimal_k = max(silhouette_scores, key=lambda x: x[1])[0]\n    print(f\"\\nOptimal number of clusters: {optimal_k}\")\n\n    # Perform final clustering with optimal k\n    kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n    return kmeans.fit_predict(embedding)\n\n# Find clusters\ncluster_labels = find_optimal_clusters(embedding)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nk=2: silhouette score = 0.423\nk=3: silhouette score = 0.476\nk=4: silhouette score = 0.505\nk=5: silhouette score = 0.447\nk=6: silhouette score = 0.414\nk=7: silhouette score = 0.356\nk=8: silhouette score = 0.330\nk=9: silhouette score = 0.300\n\nOptimal number of clusters: 4\n```\n:::\n:::\n\n\n::: {.column-margin}\nThe silhouette score measures both cohesion (how close nodes are within their cluster) and separation (how far clusters are from each other).\n:::\n\nNow let's visualize the discovered clusters on the network:\n\n::: {#4663fd23 .cell execution_count=16}\n``` {.python .cell-code}\n# Visualize the clustering results\ncmap = sns.color_palette().as_hex()\nigraph.plot(\n    g,\n    vertex_color=[cmap[label] for label in cluster_labels],\n    bbox=(500, 500),\n    vertex_size=20\n)\n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n![](02-coding_files/figure-html/cell-17-output-1.svg){}\n:::\n:::\n\n\nThe K-means algorithm successfully identifies community structure using only the learned embeddings, demonstrating that DeepWalk captures meaningful structural properties of the network.\n\n### Node2vec: Flexible Graph Embeddings\n\nNode2vec extends DeepWalk by introducing a biased random walk strategy. Instead of uniformly choosing the next node, node2vec uses two parameters, $p$ and $q$, to control the exploration strategy:\n\n- **Return parameter $p$**: Controls the likelihood of returning to the previous node\n- **In-out parameter $q$**: Controls whether the walk explores locally (BFS-like) or ventures further (DFS-like)\n\n::: {.column-margin}\nNode2vec was introduced by Grover and Leskovec (2016). The biased walk allows it to learn embeddings that capture different structural properties depending on the task.\n:::\n\n#### Step 1: Implement Biased Random Walk\n\nThe key innovation in node2vec is the biased random walk. Let's implement it step by step.\n\n::: {#15b74401 .cell execution_count=17}\n``` {.python .cell-code}\ndef node2vec_random_walk(net, start_node, walk_length, p, q):\n    \"\"\"\n    Generate a biased random walk for node2vec.\n\n    Parameters:\n    -----------\n    net : sparse matrix\n        Adjacency matrix of the network\n    start_node : int\n        Starting node for the walk\n    walk_length : int\n        Length of the walk\n    p : float\n        Return parameter (controls likelihood of returning to previous node)\n    q : float\n        In-out parameter (controls BFS vs DFS behavior)\n\n    Returns:\n    --------\n    walk : list\n        List of node indices representing the biased random walk\n    \"\"\"\n    walk = [start_node]\n\n    while len(walk) < walk_length:\n        cur = walk[-1]\n        cur_nbrs = list(net[cur].indices)\n\n        if len(cur_nbrs) > 0:\n            if len(walk) == 1:\n                # First step: uniform random choice\n                walk.append(np.random.choice(cur_nbrs))\n            else:\n                # Subsequent steps: biased choice based on p and q\n                prev = walk[-2]\n                next_node = biased_choice(net, cur_nbrs, prev, p, q)\n                walk.append(next_node)\n        else:\n            break\n\n    return walk\n\ndef biased_choice(net, neighbors, prev, p, q):\n    \"\"\"\n    Choose the next node with bias controlled by p and q.\n\n    The transition probability is:\n    - 1/p if returning to the previous node\n    - 1   if moving to a neighbor of the previous node (distance 1)\n    - 1/q if moving away from the previous node (distance 2)\n    \"\"\"\n    unnormalized_probs = []\n\n    for neighbor in neighbors:\n        if neighbor == prev:\n            # Returning to previous node\n            unnormalized_probs.append(1 / p)\n        elif neighbor in net[prev].indices:\n            # Moving to a common neighbor (BFS-like)\n            unnormalized_probs.append(1.0)\n        else:\n            # Moving away from previous node (DFS-like)\n            unnormalized_probs.append(1 / q)\n\n    # Normalize probabilities\n    norm_const = sum(unnormalized_probs)\n    normalized_probs = [prob / norm_const for prob in unnormalized_probs]\n\n    # Sample next node\n    return np.random.choice(neighbors, p=normalized_probs)\n```\n:::\n\n\n::: {.column-margin}\n**Understanding p and q:**\n\n- Small $p$ ($p < 1$): Encourages returning to previous node, leading to local exploration\n- Large $q$ ($q > 1$): Discourages moving away, resulting in BFS-like behavior\n- Small $q$ ($q < 1$): Encourages exploration, resulting in DFS-like behavior\n:::\n\n#### Step 2: Generate Walks and Train Model\n\nNow let's generate biased random walks and train the word2vec model. We'll use $p=1$ and $q=0.1$, which encourages outward exploration (DFS-like behavior) to capture community structure.\n\n::: {#927fe52e .cell execution_count=18}\n``` {.python .cell-code}\n# Generate biased random walks\np = 1.0   # Return parameter\nq = 0.1   # In-out parameter (q < 1 means DFS-like)\n\nwalks_node2vec = []\nfor i in range(n_nodes):\n    for _ in range(n_walkers_per_node):\n        walks_node2vec.append(node2vec_random_walk(A, i, walk_length, p, q))\n\nprint(f\"Generated {len(walks_node2vec)} biased random walks\")\nprint(f\"Example walk: {walks_node2vec[0][:10]}...\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGenerated 340 biased random walks\nExample walk: [0, 21, 1, 2, 9, 33, 32, 22, 32, 20]...\n```\n:::\n:::\n\n\n::: {.column-margin}\nWith $q=0.1$, the walk is 10 times more likely to explore distant nodes than return to the immediate neighborhood, encouraging discovery of global community structure.\n:::\n\nTrain the word2vec model on the biased walks:\n\n::: {#99dece9e .cell execution_count=19}\n``` {.python .cell-code}\n# Train node2vec model\nmodel_node2vec = Word2Vec(\n    walks_node2vec,\n    vector_size=32,\n    window=3,\n    min_count=1,\n    sg=1,\n    hs=1\n)\n\n# Extract embeddings\nembedding_node2vec = np.array([model_node2vec.wv[i] for i in range(n_nodes)])\nprint(f\"Node2vec embedding shape: {embedding_node2vec.shape}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNode2vec embedding shape: (34, 32)\n```\n:::\n:::\n\n\n#### Step 3: Visualize Node2vec Embeddings\n\nLet's visualize the node2vec embeddings and compare them with DeepWalk.\n\n::: {#f68d02b2 .cell execution_count=20}\n\n::: {.cell-output .cell-output-stderr}\n```\n/Users/skojaku-admin/miniforge3/envs/advnetsci/lib/python3.11/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n  warn(\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n    <style>\n        .bk-notebook-logo {\n            display: block;\n            width: 20px;\n            height: 20px;\n            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n        }\n    </style>\n    <div>\n        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n        <span id=\"b19198d0-d40b-4e3c-9d71-5e68aa2a869c\">Loading BokehJS ...</span>\n    </div>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<script type=\"application/javascript\">\n'use strict';\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\nconst JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    function drop(id) {\n      const view = Bokeh.index.get_by_id(id)\n      if (view != null) {\n        view.model.document.clear()\n        Bokeh.index.delete(view)\n      }\n    }\n\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n\n    // Clean up Bokeh references\n    if (id != null) {\n      drop(id)\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim()\n            drop(id)\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded(error = null) {\n    const el = document.getElementById(\"b19198d0-d40b-4e3c-9d71-5e68aa2a869c\");\n    if (el != null) {\n      const html = (() => {\n        if (typeof root.Bokeh === \"undefined\") {\n          if (error == null) {\n            return \"BokehJS is loading ...\";\n          } else {\n            return \"BokehJS failed to load.\";\n          }\n        } else {\n          const prefix = `BokehJS ${root.Bokeh.version}`;\n          if (error == null) {\n            return `${prefix} successfully loaded.`;\n          } else {\n            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n          }\n        }\n      })();\n      el.innerHTML = html;\n\n      if (error != null) {\n        const wrapper = document.createElement(\"div\");\n        wrapper.style.overflow = \"auto\";\n        wrapper.style.height = \"5em\";\n        wrapper.style.resize = \"vertical\";\n        const content = document.createElement(\"div\");\n        content.style.fontFamily = \"monospace\";\n        content.style.whiteSpace = \"pre-wrap\";\n        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n        content.textContent = error.stack ?? error.toString();\n        wrapper.append(content);\n        el.append(wrapper);\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(() => display_loaded(error), 100);\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.8.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.8.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.8.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.8.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.8.0.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n      try {\n            for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n\n      } catch (error) {display_loaded(error);throw error;\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"b19198d0-d40b-4e3c-9d71-5e68aa2a869c\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));\n</script>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n  <div id=\"daf57d73-39a0-40d4-a786-cdfcf713a8aa\" data-root-id=\"p1063\" style=\"display: contents;\"></div>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<script type=\"application/javascript\">\n(function(root) {\n  function embed_document(root) {\n  const docs_json = {\"70e07e2a-9294-4b50-965a-8c518bb6bb58\":{\"version\":\"3.8.0\",\"title\":\"Bokeh Application\",\"config\":{\"type\":\"object\",\"name\":\"DocumentConfig\",\"id\":\"p1108\",\"attributes\":{\"notifications\":{\"type\":\"object\",\"name\":\"Notifications\",\"id\":\"p1109\"}}},\"roots\":[{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"p1063\",\"attributes\":{\"x_range\":{\"type\":\"object\",\"name\":\"DataRange1d\",\"id\":\"p1064\"},\"y_range\":{\"type\":\"object\",\"name\":\"DataRange1d\",\"id\":\"p1065\"},\"x_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1073\"},\"y_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1074\"},\"title\":{\"type\":\"object\",\"name\":\"Title\",\"id\":\"p1066\",\"attributes\":{\"text\":\"Node2vec Embeddings (UMAP projection)\"}},\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1104\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1060\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1061\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1062\"},\"data\":{\"type\":\"map\",\"entries\":[[\"x\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"H4sIAAEAAAAC/wGIAHf/YtvBQIg7q0Cg5cVA+5CiQB+YA0FrmwBB1Hb3QNXUokCY9tJA56zMQGSVBEHeDs1Ap0StQECNrUC1AgBBo03pQOw3AEHd4bFAvvH3QBNh2EAGOfJAEJC3QPAX7UDC+c9ADgrPQB+byEDF1fpABC/BQBqDxkDuQuZAfa+zQBsNukCYHQRBQsX8QCXRPQWIAAAA\"},\"shape\":[34],\"dtype\":\"float32\",\"order\":\"little\"}],[\"y\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"H4sIAAEAAAAC/wGIAHf/O4WFQazvfEFEQkNBWReEQVBohkG93odB+kOJQUFagEFp90RBu9o8QQ7FiEF5m4dB9X6FQfMyc0EUazRBjRYxQdAEi0H524FB4TI3QdAwP0FlOy9Btk2HQf8VOkHhNChBrqofQfe3IkG9nyxBg2gpQc08OEEJfiVBoyV5QdG1JkFIDixBPIYlQQhzbROIAAAA\"},\"shape\":[34],\"dtype\":\"float32\",\"order\":\"little\"}],[\"size\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"H4sIAAEAAAAC/9NqcT2yUcrWQUHZJJj1iqmDhXdFmxeTuUNOyuyJMy8ZOsi375tZa6LpoAVWp4tBHzknOk3A1cChYd3pA/lTVNDUy8LF0dURomHm4JKHmQdTh64e5j5c7kf1n6UDAxjYOQAA2QugHBABAAA=\"},\"shape\":[34],\"dtype\":\"float64\",\"order\":\"little\"}],[\"community\",[\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\",\"#ff7f0e\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\",\"#ff7f0e\",\"#1f77b4\",\"#1f77b4\",\"#ff7f0e\",\"#1f77b4\",\"#ff7f0e\",\"#1f77b4\",\"#ff7f0e\",\"#ff7f0e\",\"#ff7f0e\",\"#ff7f0e\",\"#ff7f0e\",\"#ff7f0e\",\"#ff7f0e\",\"#ff7f0e\",\"#ff7f0e\",\"#ff7f0e\",\"#ff7f0e\",\"#ff7f0e\"]],[\"name\",[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\"]]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1105\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1106\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1101\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"field\",\"field\":\"size\"},\"fill_color\":{\"type\":\"field\",\"field\":\"community\"},\"hatch_color\":{\"type\":\"field\",\"field\":\"community\"}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1102\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"field\",\"field\":\"size\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"fill_color\":{\"type\":\"field\",\"field\":\"community\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_color\":{\"type\":\"field\",\"field\":\"community\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1103\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"field\",\"field\":\"size\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"fill_color\":{\"type\":\"field\",\"field\":\"community\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_color\":{\"type\":\"field\",\"field\":\"community\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1072\",\"attributes\":{\"tools\":[{\"type\":\"object\",\"name\":\"PanTool\",\"id\":\"p1085\"},{\"type\":\"object\",\"name\":\"WheelZoomTool\",\"id\":\"p1086\",\"attributes\":{\"renderers\":\"auto\"}},{\"type\":\"object\",\"name\":\"BoxZoomTool\",\"id\":\"p1087\",\"attributes\":{\"dimensions\":\"both\",\"overlay\":{\"type\":\"object\",\"name\":\"BoxAnnotation\",\"id\":\"p1088\",\"attributes\":{\"syncable\":false,\"line_color\":\"black\",\"line_alpha\":1.0,\"line_width\":2,\"line_dash\":[4,4],\"fill_color\":\"lightgrey\",\"fill_alpha\":0.5,\"level\":\"overlay\",\"visible\":false,\"left\":{\"type\":\"number\",\"value\":\"nan\"},\"right\":{\"type\":\"number\",\"value\":\"nan\"},\"top\":{\"type\":\"number\",\"value\":\"nan\"},\"bottom\":{\"type\":\"number\",\"value\":\"nan\"},\"left_units\":\"canvas\",\"right_units\":\"canvas\",\"top_units\":\"canvas\",\"bottom_units\":\"canvas\",\"handles\":{\"type\":\"object\",\"name\":\"BoxInteractionHandles\",\"id\":\"p1094\",\"attributes\":{\"all\":{\"type\":\"object\",\"name\":\"AreaVisuals\",\"id\":\"p1093\",\"attributes\":{\"fill_color\":\"white\",\"hover_fill_color\":\"lightgray\"}}}}}}}},{\"type\":\"object\",\"name\":\"SaveTool\",\"id\":\"p1095\"},{\"type\":\"object\",\"name\":\"ResetTool\",\"id\":\"p1096\"},{\"type\":\"object\",\"name\":\"HelpTool\",\"id\":\"p1097\"},{\"type\":\"object\",\"name\":\"HoverTool\",\"id\":\"p1107\",\"attributes\":{\"renderers\":\"auto\",\"tooltips\":[[\"Node\",\"@name\"],[\"Community\",\"@community\"]],\"sort_by\":null}}]}},\"left\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1080\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1081\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1082\"},\"axis_label\":\"UMAP 2\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1083\"}}}],\"below\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1075\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1076\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1077\"},\"axis_label\":\"UMAP 1\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1078\"}}}],\"center\":[{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1079\",\"attributes\":{\"axis\":{\"id\":\"p1075\"}}},{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1084\",\"attributes\":{\"dimension\":1,\"axis\":{\"id\":\"p1080\"}}}]}}]}};\n  const render_items = [{\"docid\":\"70e07e2a-9294-4b50-965a-8c518bb6bb58\",\"roots\":{\"p1063\":\"daf57d73-39a0-40d4-a786-cdfcf713a8aa\"},\"root_ids\":[\"p1063\"]}];\n  void root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n  }\n  if (root.Bokeh !== undefined) {\n    embed_document(root);\n  } else {\n    let attempts = 0;\n    const timer = setInterval(function(root) {\n      if (root.Bokeh !== undefined) {\n        clearInterval(timer);\n        embed_document(root);\n      } else {\n        attempts++;\n        if (attempts > 100) {\n          clearInterval(timer);\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n        }\n      }\n    }, 10, root)\n  }\n})(window);\n</script>\n```\n:::\n:::\n\n\nNotice how the node2vec embeddings with $q=0.1$ (DFS-like exploration) create even more distinct community clusters compared to DeepWalk. This is because the biased walk explores the community structure more thoroughly.\n\n#### Step 4: Clustering Analysis\n\nLet's apply K-means clustering to the node2vec embeddings:\n\n::: {#ed4b3d93 .cell execution_count=21}\n``` {.python .cell-code}\n# Find optimal clusters for node2vec embeddings\ncluster_labels_n2v = find_optimal_clusters(embedding_node2vec)\n\n# Visualize the clustering results\nigraph.plot(\n    g,\n    vertex_color=[palette[label] for label in cluster_labels_n2v],\n    bbox=(500, 500),\n    vertex_size=20,\n    vertex_label=[str(i) for i in range(n_nodes)]\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nk=2: silhouette score = 0.448\nk=3: silhouette score = 0.527\nk=4: silhouette score = 0.454\nk=5: silhouette score = 0.400\nk=6: silhouette score = 0.366\nk=7: silhouette score = 0.290\nk=8: silhouette score = 0.337\nk=9: silhouette score = 0.343\n\nOptimal number of clusters: 3\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=21}\n![](02-coding_files/figure-html/cell-22-output-2.svg){}\n:::\n:::\n\n\nBy tuning the $p$ and $q$ parameters, node2vec can adapt to different network analysis tasks - from community detection (small $q$) to role discovery (large $q$).\n\n",
    "supporting": [
      "02-coding_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}